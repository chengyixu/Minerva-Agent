from zhipuai import ZhipuAI
import streamlit as st
from pathlib import Path
import os
import numpy as np
from langchain_community.vectorstores import Qdrant
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
from pathlib import Path

api_key="e4ba1f8270cc29fef6fbf1693a4737fd.Cy9L1vRKT6WJ6rId"
base_dir = './é¢è¯•é¢˜'

system_prompt = """# Role: ä¸“ä¸šé¢è¯•å®˜

## Profile
- èµ„æ·±HRå’ŒæŠ€æœ¯é¢è¯•å®˜
- æ“…é•¿ç»“æ„åŒ–é¢è¯•å’Œäººæ‰è¯„ä¼°
- æ³¨é‡è€ƒå¯Ÿå€™é€‰äººçš„ç»¼åˆç´ è´¨

## Background
ä½ æ­£åœ¨å¯¹ä¸€ä½å€™é€‰äººè¿›è¡Œé¢è¯•ã€‚ä½ éœ€è¦åŸºäºä»¥ä¸‹ä¿¡æ¯è¿›è¡Œé¢è¯•ï¼š

å€™é€‰äººçš„ç®€å†ï¼š
<ç®€å†>
{resume_content}
</ç®€å†>

å¯å‚è€ƒçš„é¢è¯•çš„é¢˜åº“ï¼š
<é¢˜åº“>
{context}
</é¢˜åº“>

## Goals
- å…¨é¢è¯„ä¼°å€™é€‰äººçš„ä¸“ä¸šèƒ½åŠ›å’Œç»¼åˆç´ è´¨
- è¥é€ ä¸“ä¸šå‹å¥½çš„é¢è¯•æ°›å›´
- é’ˆå¯¹æ€§æå‡ºä¸å²—ä½ç›¸å…³çš„é—®é¢˜
- ç»™å‡ºå®¢è§‚å…¬æ­£çš„è¯„ä»·

## Rules
1. è‹¥æ²¡æœ‰CommandsæŒ‡ä»¤ï¼Œæ¯è½®å¯¹è¯ä»…æ ¹æ®å€™é€‰äººçš„é¢è¯•å²—ä½æå‡ºä¸€ä¸ªé¢è¯•é—®é¢˜ï¼Œé—®é¢˜è¦å¾ªåºæ¸è¿›ï¼Œç”±æµ…å…¥æ·±ï¼›è‹¥ç”¨æˆ·æå‡ºCommandsæŒ‡ä»¤ï¼Œåˆ™æ ¹æ®æŒ‡ä»¤æ‰§è¡Œç›¸åº”æ“ä½œ
2. æ³¨æ„å€¾å¬å€™é€‰äººçš„å›ç­”ï¼Œä¿æŒä¸“ä¸šå’Œå‹å¥½çš„æ€åº¦ï¼Œä¸æ³„éœ²é¢˜åº“ä¸­çš„æ ‡å‡†ç­”æ¡ˆ

## Skills
- ç»“æ„åŒ–é¢è¯•æŠ€å·§
- è€ƒå¯Ÿé‡ç‚¹åŒ…æ‹¬ï¼šä¸“ä¸šèƒ½åŠ›ã€å­¦ä¹ èƒ½åŠ›ã€æ²Ÿé€šèƒ½åŠ›ã€å›¢é˜Ÿåä½œç­‰
- æ ¹æ®å€™é€‰äººçš„å›ç­”çµæ´»è°ƒæ•´é¢è¯•ç­–ç•¥

## Workflow
1. åˆ†æå€™é€‰äººç®€å†å’Œæ±‚èŒæ„å‘
2. è®¾è®¡åˆé€‚çš„é¢è¯•é—®é¢˜
3. è¯„ä¼°å€™é€‰äººçš„å›ç­”
4. é€‚æ—¶ç»™å‡ºè¿½é—®æˆ–æ–°çš„è¯é¢˜
5. åœ¨é¢è¯•è¿‡ç¨‹ä¸­æŒç»­ç§¯ç´¯å¯¹å€™é€‰äººçš„è¯„ä»·

## Commands
- å¯è¢«æ‰§è¡Œçš„æŒ‡ä»¤ï¼š
/analyze_resume - åˆ†æå€™é€‰äººç®€å†è¦ç‚¹ï¼Œä»¥hrè§†è§’è¿›è¡Œç‚¹è¯„ï¼Œå¹¶æä¾›æ”¹è¿›å»ºè®®
/next_question - åˆ‡æ¢é¢è¯•çŸ¥è¯†ç‚¹ï¼Œç”Ÿæˆä»¥è¯¥çŸ¥è¯†ç‚¹ä¸ºä¸»çš„ä¸€ä¸ªé¢è¯•é—®é¢˜è®©å€™é€‰äººè¿›è¡Œå›ç­”
/technical_test - å¼€å§‹æŠ€æœ¯èƒ½åŠ›æµ‹è¯•ï¼Œæ¯æ¬¡æµ‹è¯•ä¸€ä¸ªæŠ€æœ¯é—®é¢˜ï¼Œåœ¨å€™é€‰äººç»™å‡ºå›ç­”åï¼Œç»™äºˆå…¶æœ¬æ¬¡å›ç­”çš„å®¢è§‚è¯„ä»·
/summary - ç”Ÿæˆé¢è¯•æ€»ç»“æŠ¥å‘Šï¼Œæä¾›é¢è¯•çš„æ”¹è¿›å»ºè®®
/language - åˆ‡æ¢é¢è¯•äº¤è°ˆçš„è¯­è¨€ï¼Œè‹¥å€™é€‰äººæœªæŒ‡å‡ºåˆ™é»˜è®¤åˆ‡æ¢ä¸ºè‹±æ–‡äº¤æµ

## Constraints
1. ä¸é€éœ²é¢è¯•è¯„åˆ†æ ‡å‡†
2. ä¸å¯¹å€™é€‰äººè¿›è¡Œäººèº«æ”»å‡»
3. ä¸æ¶‰åŠéšç§å’Œæ­§è§†æ€§é—®é¢˜
4. ä¿æŒé¢è¯•å®˜çš„ä¸“ä¸šå½¢è±¡

## Language Style
- ä½¿ç”¨ç¤¼è²Œä¸“ä¸šçš„æªè¾
- è¯­æ°”å¹³å’Œä½†ä¸å¤±æƒå¨
- é€‚å½“ä½¿ç”¨ä¸“ä¸šæœ¯è¯­
- è¡¨è¾¾æ¸…æ™°ç®€æ´

è¯·åŸºäºä»¥ä¸Šè®¾å®šï¼Œå¼€å§‹è¿›è¡Œé¢è¯•ã€‚"""

def Chat(system_prompt, messages):
    # åˆå§‹åŒ–æ™ºè°±AIå®¢æˆ·ç«¯
    client = ZhipuAI(
        api_key=api_key, 
        base_url="https://open.bigmodel.cn/api/paas/v4/"
    )
    completion = client.chat.completions.create(
        model="glm-4-plus",
        messages=[{"role": "system", "content": system_prompt}] + messages,
        temperature=0.7,
        top_p=0.8,
        max_tokens=2048,
        stream=True
    )
    # å°†å¯¹è¯å†…å®¹å†™å…¥æ–‡ä»¶
    with open("æµ‹è¯•.txt", "a", encoding="utf-8") as f:
        f.write(str([{"role": "system", "content": system_prompt}] + messages) + "\n")

    return completion

def load_documents(base_dir):
    documents = []
    for file in os.listdir(base_dir):
        file_path = os.path.join(base_dir, file)
        progress_text.text(f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {file}")
        if file.endswith('.pdf'):
            loader = PyPDFLoader(file_path)
            documents.extend(loader.load())
        elif file.endswith('.docx'):
            loader = Docx2txtLoader(file_path)
            documents.extend(loader.load())
        elif file.endswith('.txt'):
            loader = TextLoader(file_path)
            documents.extend(loader.load())
        progress_text.text(f"æ–‡ä»¶åŠ è½½å®Œæˆ")

    return documents

def RAG(query, vectorstore):
    progress_text.text("å¼€å§‹å‘é‡æ£€ç´¢...")
    # ä»å‘é‡æ•°æ®åº“æ£€ç´¢ç›¸å…³å†…å®¹
    search_result = vectorstore.similarity_search(query, k=10)
    results = {
        "documents": [[doc.page_content for doc in search_result]]
    }
    progress_text.text(f"æ£€ç´¢å®Œæˆï¼")
    return results

# åˆå§‹åŒ–å‘é‡æ•°æ®åº“
def init_vector_store(base_dir):
    progress_text.text("å¼€å§‹Embeddingsæ¨¡å‹åˆå§‹åŒ–...")
    # åˆå§‹åŒ–embeddings
    embeddings = OpenAIEmbeddings(
        model="embedding-3",
        openai_api_key=api_key,
        openai_api_base="https://open.bigmodel.cn/api/paas/v4"
    )
    progress_text.text("Embeddingsæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
    
    # åŠ è½½æ–‡æ¡£
    documents = load_documents(base_dir)
    
    # æ–‡æ¡£åˆ†å‰²
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=10)
    chunked_documents = text_splitter.split_documents(documents)
    progress_text.text(f"æ–‡æ¡£åˆ†å‰²å®Œæˆ,å…±{len(chunked_documents)}ä¸ªç‰‡æ®µ")
    
    progress_text.text("å¼€å§‹åˆå§‹åŒ–å‘é‡æ•°æ®åº“ï¼Œéœ€è¦ç­‰å¾…ä¸€æ®µæ—¶é—´...")
    # åˆå§‹åŒ–Qdrantå‘é‡æ•°æ®åº“
    vectorstore = Qdrant.from_documents(
        documents=chunked_documents,
        embedding=embeddings,
        location=":memory:",
        collection_name="interview_qa"
    )
    progress_text.text("å‘é‡æ•°æ®åº“åˆå§‹åŒ–å®Œæˆï¼Œè¾“å…¥ä½ çš„æ±‚èŒå²—ä½ï¼Œå¼€å§‹é¢è¯•å§ï¼")
    
    return vectorstore

def parse_resume(uploaded_file):
    """è§£æä¸Šä¼ çš„æ–‡ä»¶å†…å®¹"""
    # åˆ›å»ºä¸´æ—¶ç›®å½•æ¥å­˜å‚¨ä¸Šä¼ çš„æ–‡ä»¶
    temp_dir = Path("temp_uploads")
    temp_dir.mkdir(exist_ok=True)
    
    with open(f"./temp_uploads/{uploaded_file.name}", "wb") as f:
        f.write(uploaded_file.read())
    try:
        # ä½¿ç”¨ç°æœ‰çš„ load_documents å‡½æ•°åŠ è½½æ–‡ä»¶
        documents = load_documents(str(temp_dir))
        # åˆå¹¶æ‰€æœ‰æ–‡æ¡£å†…å®¹
        resume_content = "\n".join(doc.page_content for doc in documents)
        return resume_content.strip()
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for file in temp_dir.iterdir():
            file.unlink()
        temp_dir.rmdir()

# Streamlit UI
st.title("ğŸ¤– é¢è¯•åŠ©æ‰‹")

# æ·»åŠ ä¾§è¾¹æ è¯´æ˜
with st.sidebar:
    st.markdown("""
    ## é¢è¯•åŠ©æ‰‹å¯ç”¨å‘½ä»¤è¯´æ˜
    å¤åˆ¶ä»¥ä¸‹å‘½ä»¤åˆ°å¯¹è¯æ¡†ä½¿ç”¨:  
    ### å‘½ä»¤1ï¼š`/analyze_resume`
    åˆ†æç®€å†è¦ç‚¹ï¼Œç»™å‡ºç‚¹è¯„å’Œæ”¹è¿›å»ºè®®ï¼Œç¤ºä¾‹: `/analyze_resume`
    ### å‘½ä»¤2ï¼š`/next_question` 
    åˆ‡æ¢é¢è¯•çŸ¥è¯†ç‚¹ï¼Œç”Ÿæˆæ–°çš„é¢è¯•é—®é¢˜ï¼Œç¤ºä¾‹: `/next_question æœºå™¨å­¦ä¹ `
    ### å‘½ä»¤3ï¼š`/technical_test`
    å¼€å§‹æŠ€æœ¯èƒ½åŠ›æµ‹è¯•ï¼Œç¤ºä¾‹: `/technical_test`
    ### å‘½ä»¤4ï¼š`/summary`
    ç”Ÿæˆé¢è¯•æ€»ç»“æŠ¥å‘Šï¼Œæä¾›æ”¹è¿›å»ºè®®ï¼Œç¤ºä¾‹: `/summary`
    ### å‘½ä»¤5ï¼š`/language`
    åˆ‡æ¢é¢è¯•è¯­è¨€(é»˜è®¤åˆ‡æ¢ä¸ºè‹±æ–‡)ï¼Œç¤ºä¾‹: `/language English`
    """)

# æ–‡ä»¶ä¸Šä¼ 
uploaded_file = st.file_uploader("è¯·ä¸Šä¼ æ‚¨çš„ç®€å†", type=["pdf", "docx", "txt"])

if uploaded_file:
    progress_text = st.empty()
    progress_text.text(f"æ”¶åˆ°ä¸Šä¼ æ–‡ä»¶: {uploaded_file.name}")

    resume_content = parse_resume(uploaded_file)
    
    # åˆå§‹åŒ–èŠå¤©å†å²å’Œå‘é‡æ•°æ®åº“
    if "messages" not in st.session_state:
        st.session_state.messages = []
        st.session_state.vectorstore = init_vector_store(base_dir)
    
    # æ˜¾ç¤ºèŠå¤©å†å²
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„å›ç­”"):

        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        with st.spinner('æ­£åœ¨åˆ†ææ‚¨çš„å›ç­”...'):
            # ä½¿ç”¨RAGæ£€ç´¢ç›¸å…³é¢è¯•é¢˜
            results = RAG(prompt, st.session_state.vectorstore)
            context = "\n".join(results['documents'][0])
            
            # ç”Ÿæˆé¢è¯•å®˜å›å¤
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                full_response = ""
                
                completion = Chat(system_prompt.format(resume_content=resume_content, context=context), st.session_state.messages)
                
                for chunk in completion:
                    full_response += chunk.choices[0].delta.content or ""
                    message_placeholder.markdown(full_response + "â–Œ")
                message_placeholder.markdown(full_response)
            
            st.session_state.messages.append({"role": "assistant", "content": full_response})

else:
    st.write("è¯·ä¸Šä¼ æ‚¨çš„ç®€å†å¼€å§‹é¢è¯•")