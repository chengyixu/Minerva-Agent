{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qwen\n",
    "\n",
    "sk-1a28c3fcc7e044cbacd6faf47dc89755"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "firecrawl\n",
    "\n",
    "fc-343fd362814545f295a89dc14ec4ee09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#firecrawl\n",
    "fire_api = \"fc-343fd362814545f295a89dc14ec4ee09\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qwen\n",
    "api_key = \"sk-1a28c3fcc7e044cbacd6faf47dc89755\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chatcmpl-91733717-3dd7-9158-a828-31565744a0e1\",\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":\"It looks like you've provided a comprehensive overview of various topics related to machine learning, particularly focusing on large language models (LLMs), reinforcement learning (RL), and transformer architectures. Here's a structured summary and some key takeaways from the content:\\n\\n### 1. **Reward Hacking in Reinforcement Learning**\\n   - **Definition**: Reward hacking occurs when an RL agent exploits flaws or ambiguities in the reward function to achieve high rewards without genuinely completing the intended task.\\n   - **Cause**: Imperfections in RL environments and challenges in specifying accurate reward functions.\\n   - **Impact on LLMs**: With the rise of reinforcement learning from human feedback (RLHF) for aligning LLMs, reward hacking has become a critical challenge. Instances include models modifying unit tests to pass coding tasks or mimicking user biases, which can block real-world deployment.\\n\\n### 2. **Hallucination in Large Language Models**\\n   - **Definition**: Hallucination refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content.\\n   - **Types**:\\n     - **In-context hallucination**: Model output should be consistent with the provided context.\\n     - **Extrinsic hallucination**: Model output should be grounded by world knowledge, but it’s expensive to verify due to the vast pre-training dataset.\\n   - **Key Requirement**: To avoid hallucination, LLMs need to be factual and acknowledge when they don't know the answer.\\n\\n### 3. **Diffusion Models for Video Generation**\\n   - **Challenges**:\\n     - Temporal consistency across frames.\\n     - Difficulty in collecting large amounts of high-quality video data.\\n   - **Relation to Text/Image Generation**: Video generation is a superset of image generation, requiring more world knowledge and higher-dimensional data.\\n\\n### 4. **High-Quality Data in Model Training**\\n   - **Importance**: High-quality data is crucial for training deep learning models.\\n   - **Human Annotation**: Most task-specific labeled data comes from human annotation, which requires attention to detail and careful execution.\\n   - **Community Perception**: There's a subtle impression that \\\"everyone wants to do the model work, not the data work,\\\" highlighting the undervaluation of data collection efforts.\\n\\n### 5. **Adversarial Attacks on LLMs**\\n   - **Context**: Adversarial attacks on images operate in continuous, high-dimensional space, while text-based attacks are more challenging due to discrete data and lack of direct gradient signals.\\n   - **Relevance**: Attacking LLMs involves controlling the model to output unsafe content, similar to controllable text generation techniques.\\n\\n### 6. **LLM-Powered Autonomous Agents**\\n   - **Components**:\\n     - **Planning**: Breaking down tasks into subgoals.\\n     - **Reflection**: Self-criticism and learning from past actions.\\n     - **Memory**: Short-term (in-context learning) and long-term (external vector stores).\\n     - **Tool Use**: Calling external APIs for missing information.\\n   - **Potential**: LLMs can extend beyond generating text to act as general problem solvers.\\n\\n### 7. **Prompt Engineering**\\n   - **Definition**: Methods to communicate with LLMs to steer behavior without updating model weights.\\n   - **Focus**: Alignment and steerability of the model.\\n   - **Empirical Nature**: Requires heavy experimentation and heuristics.\\n\\n### 8. **Transformer Architecture Improvements**\\n   - **Overview**: Many new improvements have been proposed since 2020, including updates to the original Transformer architecture.\\n   - **Notations**: Detailed explanations of symbols used in Transformer models, such as hidden state dimensions, attention layers, and weight matrices.\\n\\n### 9. **Inference Challenges for Large Transformers**\\n   - **Factors**: Increasing model size and two main factors contributing to inference challenges: over-parameterization and optimization processes.\\n   - **NTK**: Neural tangent kernel explains the evolution of neural networks during training via gradient descent, providing insights into convergence.\\n\\n### 10. **Vision-Language Tasks**\\n   - **Approach**: Extending pre-trained language models to consume visual signals, moving away from traditional object detection networks.\\n\\n### 11. **Learning with Limited Data**\\n   - **Approaches**:\\n     - Data augmentation: Modifying existing data points.\\n     - Generating new data: Using pretrained models to create synthetic data.\\n\\n### 12. **Contrastive Representation Learning**\\n   - **Goal**: Learn an embedding space where similar samples stay close, and dissimilar ones are far apart.\\n   - **Applications**: Both supervised and unsupervised settings, especially powerful in self-supervised learning.\\n\\n### 13. **Safety Control in Pretrained Language Models**\\n   - **Challenge**: Acquiring toxic behavior and biases from online data.\\n   - **Need**: Strong safety controls for practical real-world applications.\\n\\n### 14. **Open-Domain Question Answering**\\n   - **Applications**: Useful for chatbots and AI assistants.\\n   - **Approaches**: Several methods for building open-domain QA systems.\\n\\n---\\n\\n### How Can I Help You Further?\\n- **Clarification on Specific Topics**: If you need more detailed information on any of the topics mentioned.\\n- **Summarizing Key Points**: If you want a concise summary of specific sections.\\n- **Expanding on Concepts**: If you'd like to dive deeper into certain areas, such as prompt engineering or diffusion models.\\n- **Writing Assistance**: If you need help drafting or revising content based on this material.\\n\\nLet me know how you'd like to proceed!\",\"refusal\":null,\"role\":\"assistant\",\"audio\":null,\"function_call\":null,\"tool_calls\":null}}],\"created\":1740377662,\"model\":\"qwen-plus\",\"object\":\"chat.completion\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":1136,\"prompt_tokens\":3403,\"total_tokens\":4539,\"completion_tokens_details\":null,\"prompt_tokens_details\":{\"audio_tokens\":null,\"cached_tokens\":0}}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "\n",
    "# Function to fetch and parse the webpage\n",
    "def fetch_website_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract text from paragraphs or other relevant elements\n",
    "    paragraphs = soup.find_all('p')\n",
    "    content = \" \".join([p.get_text() for p in paragraphs])\n",
    "    \n",
    "    return content\n",
    "\n",
    "# Fetch the content from the target URL\n",
    "url = \"https://lilianweng.github.io/\"\n",
    "website_content = fetch_website_content(url)\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "api_key = \"sk-1a28c3fcc7e044cbacd6faf47dc89755\"\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n",
    "\n",
    "# Pass the website content to the OpenAI model for further processing\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"qwen-plus\",  # Example model\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "        {'role': 'user', 'content': f\"Here is some content from the website: {website_content}. How can I help you with it?\"}\n",
    "    ],\n",
    "    extra_body={\n",
    "        \"enable_search\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print the result\n",
    "print(completion.model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 14:14:23.220 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:23.428 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-02-24 14:14:23.429 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:23.429 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:23.429 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:23.429 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:23.430 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:23.430 Session state does not function when running a script without `streamlit run`\n",
      "2025-02-24 14:14:23.430 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:23.430 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:23.431 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:23.431 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:23.431 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:23.431 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:23.431 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "\n",
    "# Function to fetch and parse the webpage\n",
    "def fetch_website_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check for request errors\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Extract text from paragraphs or other relevant elements\n",
    "        paragraphs = soup.find_all('p')\n",
    "        content = \" \".join([p.get_text() for p in paragraphs])\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching content: {e}\"\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"Website Content Analyzer\")\n",
    "\n",
    "# Input URL\n",
    "url = st.text_input(\"Enter the website URL:\", \"https://lilianweng.github.io/\")\n",
    "\n",
    "if st.button(\"Analyze Website\"):\n",
    "    # Fetch content from the provided URL\n",
    "    website_content = fetch_website_content(url)\n",
    "\n",
    "    if website_content.startswith(\"Error\"):\n",
    "        st.error(website_content)\n",
    "    else:\n",
    "        # Initialize the OpenAI client\n",
    "        api_key = \"sk-1a28c3fcc7e044cbacd6faf47dc89755\"\n",
    "        client = OpenAI(\n",
    "            api_key=api_key,\n",
    "            base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "        )\n",
    "\n",
    "        # Pass the website content to the OpenAI model for further processing\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"qwen-plus\",  # Example model\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "                {'role': 'user', 'content': f\"Here is some content from the website: {website_content}. How can I help you with it?\"}\n",
    "            ],\n",
    "            extra_body={\n",
    "                \"enable_search\": True\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Display result\n",
    "        st.subheader(\"Response from AI\")\n",
    "        st.json(completion.model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 14:14:25.173 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.173 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.173 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.174 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.174 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.174 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.175 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.175 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.175 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.175 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.176 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.176 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.176 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.176 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "import os\n",
    "import dashscope\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to fetch and parse the webpage\n",
    "def fetch_website_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check for request errors\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Find the latest topics by extracting titles and summaries\n",
    "        # For simplicity, assuming articles are in <article> or <h2> tags. Adjust as needed.\n",
    "        articles = soup.find_all(['h2', 'article'])\n",
    "\n",
    "        content = []\n",
    "        for idx, article in enumerate(articles[:10]):  # Only fetch the latest 10 articles\n",
    "            title = article.get_text().strip()\n",
    "            one_liner = article.find_next('p').get_text().strip() if article.find_next('p') else \"No summary available\"\n",
    "            content.append(f\"{idx + 1}. \\\"{title}\\\", \\\"{one_liner}\\\", {url}\")\n",
    "        \n",
    "        return content\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching content: {e}\"\n",
    "\n",
    "# Function to summarize the content using LLM\n",
    "def summarize_content(content):\n",
    "    api_key = \"sk-1a28c3fcc7e044cbacd6faf47dc89755\"  # Replace with your actual API key\n",
    "    # Prepare the messages for summarization\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "        {'role': 'user', 'content': f\"Please summarize the following content into the latest 10 key ideas in the format: '1. title, one-liner description, website name': {content}\"}\n",
    "    ]\n",
    "    \n",
    "    # Call the Dashscope API to summarize the content\n",
    "    try:\n",
    "        response = dashscope.Generation.call(\n",
    "            api_key=api_key,\n",
    "            model=\"qwen-plus\",  # Example model, replace as needed\n",
    "            messages=messages,\n",
    "            enable_search=True,\n",
    "            result_format='message'\n",
    "        )\n",
    "\n",
    "        # Access the response content correctly\n",
    "        summarized_content = response['message']\n",
    "        return summarized_content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")  # Catch any other errors\n",
    "        return f\"Error summarizing content: {e}\"\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"Website Content Analyzer and Summarizer\")\n",
    "\n",
    "# URL selection\n",
    "st.sidebar.header(\"Select Websites to Analyze\")\n",
    "website_choices = [\n",
    "    \"https://www.qbitai.com/\",\n",
    "    \"https://www.jiqizhixin.com/\",\n",
    "    \"https://lilianweng.github.io/\",\n",
    "    \"https://x.com/deepseek_ai\"\n",
    "]\n",
    "selected_websites = st.sidebar.multiselect(\n",
    "    \"Choose websites to analyze:\",\n",
    "    website_choices,\n",
    "    default=website_choices  # Default to analyzing all websites\n",
    ")\n",
    "\n",
    "if st.button(\"Analyze Websites\"):\n",
    "    # Loop through selected websites and fetch content\n",
    "    all_content = []\n",
    "    for website in selected_websites:\n",
    "        st.subheader(f\"Latest Topics from {website}\")\n",
    "        website_content = fetch_website_content(website)\n",
    "        \n",
    "        if isinstance(website_content, str) and website_content.startswith(\"Error\"):\n",
    "            st.error(website_content)\n",
    "        else:\n",
    "            for topic in website_content:\n",
    "                st.write(topic)\n",
    "            all_content.append(\"\\n\".join(website_content))\n",
    "    \n",
    "    # Combine all content from the websites\n",
    "    combined_content = \"\\n\\n\".join(all_content)\n",
    "    \n",
    "    # Summarize the content using LLM\n",
    "    st.subheader(\"Summarized Key Ideas from Each Website\")\n",
    "    summarized_content = summarize_content(combined_content)\n",
    "    st.write(summarized_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分区\n",
    "文章梗概+链接\n",
    "设置关注词“AI，\n",
    "\n",
    "进度条\n",
    "\n",
    "限制时间\n",
    "\n",
    "\n",
    "总结成1页\n",
    "各网站详情\n",
    "\n",
    "general爬虫 = firecrawl\n",
    "arxiv爬虫\n",
    "x 爬虫\n",
    "general爬虫\n",
    "csdn\n",
    "\n",
    "https://barretzoph.github.io/\n",
    "http://joschu.net/blog.html\n",
    "https://www.csdn.net/?spm=1001.2101.3001.4476\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_api = \"68Ht8QWPM68NDGEVqKw4gozGK\"\n",
    "x_api_secret = \"UWkfgwI6tryTEtKcJwasYTeYLep5DW8MzVxDEFLQGyUhPJYuRe\"\n",
    "bearer = \"AAAAAAAAAAAAAAAAAAAAAIOvzQEAAAAAIu9mJKMErOerEHtcahzfAp5rCtQ%3Do6794kUZOu6ufM1qmsDnTtFYsSIC8WYiSxR5rzOfk4Y31obfM2\"\n",
    "access_token = \"1942742353-LmPP6rMN8FhQFBGl5tKPsxaJWm9dhYfjztrV0U3\"\n",
    "access_token_secret = \"uTM36nTbShRe5dt00pMz1YJ2NRyURsYKOdorEKVkmeHk2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "bearer_token = bearer\n",
    "endpoint_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "query_parameters = {\n",
    "    \"query\": \"from:deepseek_ai\",\n",
    "    \"tweet.fields\": \"id,text,author_id,created_at\",\n",
    "    \"max_results\": 10,\n",
    "}\n",
    "def request_headers(bearer_token: str) -> dict:\n",
    "    \"\"\"\n",
    "    Sets up the request headers. \n",
    "    Returns a dictionary summarising the bearer token authentication details.\n",
    "    \"\"\"\n",
    "    return {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "\n",
    "headers = request_headers(bearer_token)\n",
    "\n",
    "def connect_to_endpoint(endpoint_url: str, headers: dict, parameters: dict) -> json:\n",
    "    \"\"\"\n",
    "    Connects to the endpoint and requests data.\n",
    "    Returns a json with Twitter data if a 200 status code is yielded.\n",
    "    Programme stops if there is a problem with the request and sleeps\n",
    "    if there is a temporary problem accessing the endpoint.\n",
    "    \"\"\"\n",
    "    response = requests.request(\n",
    "        \"GET\", url=endpoint_url, headers=headers, params=parameters\n",
    "    )\n",
    "    response_status_code = response.status_code\n",
    "    if response_status_code != 200:\n",
    "        if response_status_code >= 400 and response_status_code < 500:\n",
    "            raise Exception(\n",
    "                \"Cannot get data, the program will stop!\\nHTTP {}: {}\".format(\n",
    "                    response_status_code, response.text\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        sleep_seconds = random.randint(5, 60)\n",
    "        print(\n",
    "            \"Cannot get data, your program will sleep for {} seconds...\\nHTTP {}: {}\".format(\n",
    "                sleep_seconds, response_status_code, response.text\n",
    "            )\n",
    "        )\n",
    "        time.sleep(sleep_seconds)\n",
    "        return connect_to_endpoint(endpoint_url, headers, parameters)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "json_response = connect_to_endpoint(endpoint_url, headers, query_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "Payment Required: Failed to start crawl job. Insufficient credits to perform this request. For more credits, you can upgrade your plan at https://firecrawl.dev/pricing or try changing the request limit to a lower value. - No additional error details provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m app \u001b[38;5;241m=\u001b[39m FirecrawlApp(api_key\u001b[38;5;241m=\u001b[39m fire_api)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Crawl a website:\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m crawl_status \u001b[38;5;241m=\u001b[39m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrawl_url\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://36kr.com/information/AI/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlimit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscrapeOptions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mformats\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmarkdown\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlinks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m  \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m  \u001b[49m\u001b[43mpoll_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(crawl_status)\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/firecrawl/firecrawl.py:229\u001b[0m, in \u001b[0;36mFirecrawlApp.crawl_url\u001b[0;34m(self, url, params, poll_interval, idempotency_key)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_monitor_job_status(\u001b[38;5;28mid\u001b[39m, headers, poll_interval)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstart crawl job\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/firecrawl/firecrawl.py:941\u001b[0m, in \u001b[0;36mFirecrawlApp._handle_error\u001b[0;34m(self, response, action)\u001b[0m\n\u001b[1;32m    938\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected error during \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Status code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_details\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# Raise an HTTPError with the custom message and attach the response\u001b[39;00m\n\u001b[0;32m--> 941\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse)\n",
      "\u001b[0;31mHTTPError\u001b[0m: Payment Required: Failed to start crawl job. Insufficient credits to perform this request. For more credits, you can upgrade your plan at https://firecrawl.dev/pricing or try changing the request limit to a lower value. - No additional error details provided."
     ]
    }
   ],
   "source": [
    "from firecrawl import FirecrawlApp\n",
    "\n",
    "app = FirecrawlApp(api_key= fire_api)\n",
    "\n",
    "# Crawl a website:\n",
    "crawl_status = app.crawl_url(\n",
    "  'https://36kr.com/information/AI/', \n",
    "  params={\n",
    "    'limit': 100, \n",
    "    'scrapeOptions': {'formats': ['markdown', 'links']}\n",
    "  },\n",
    "  poll_interval=30\n",
    ")\n",
    "print(crawl_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FileFinder' object has no attribute 'find_module'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msnscrape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtwitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msntwitter\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/主业/Minerva/Minerva Agent/.venv/lib/python3.12/site-packages/snscrape/modules/__init__.py:17\u001b[0m\n\u001b[1;32m     13\u001b[0m \t\tmodule \u001b[38;5;241m=\u001b[39m importer\u001b[38;5;241m.\u001b[39mfind_module(moduleName)\u001b[38;5;241m.\u001b[39mload_module(moduleName)\n\u001b[1;32m     14\u001b[0m \t\t\u001b[38;5;28mglobals\u001b[39m()[moduleNameWithoutPrefix] \u001b[38;5;241m=\u001b[39m module\n\u001b[0;32m---> 17\u001b[0m \u001b[43m_import_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/主业/Minerva/Minerva Agent/.venv/lib/python3.12/site-packages/snscrape/modules/__init__.py:13\u001b[0m, in \u001b[0;36m_import_modules\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m moduleNameWithoutPrefix \u001b[38;5;241m=\u001b[39m moduleName[prefixLen:]\n\u001b[1;32m     12\u001b[0m __all__\u001b[38;5;241m.\u001b[39mappend(moduleNameWithoutPrefix)\n\u001b[0;32m---> 13\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimporter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_module\u001b[49m(moduleName)\u001b[38;5;241m.\u001b[39mload_module(moduleName)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mglobals\u001b[39m()[moduleNameWithoutPrefix] \u001b[38;5;241m=\u001b[39m module\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FileFinder' object has no attribute 'find_module'"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping raw HTML from lilianweng.github.io...\n",
      "Raw HTML retrieved from lilianweng.github.io. Analyzing with Qwen LLM...\n",
      "\n",
      "Analysis from lilianweng.github.io:\n",
      "Based on the provided HTML content, here are the latest 10 topics from Lilian Weng's blog, formatted as requested. Note that the Chinese translations for titles and descriptions are not available in the provided HTML, so I've used a translation service to provide them.\n",
      "\n",
      "1. **Reward Hacking in Reinforcement Learning | 强化学习中的奖励黑客**  \n",
      "   - **Description (EN):** Reward hacking occurs when a reinforcement learning (RL) agent exploits flaws or ambiguities in the reward function to achieve high rewards, without genuinely learning or completing the intended task.  \n",
      "   - **Description (CN):** 奖励黑客是指强化学习（RL）代理利用奖励函数中的缺陷或模糊性来获得高奖励，而实际上并没有真正学习或完成预期任务。  \n",
      "   - **Website:** Lil'Log\n",
      "\n",
      "2. **Extrinsic Hallucinations in LLMs | 大型语言模型中的外在幻觉**  \n",
      "   - **Description (EN):** Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content.  \n",
      "   - **Description (CN):** 大型语言模型中的幻觉通常指模型生成不忠实、虚构、不一致或无意义的内容。  \n",
      "   - **Website:** Lil'Log\n",
      "\n",
      "3. **Diffusion Models for Video Generation | 用于视频生成的扩散模型**  \n",
      "   - **Description (EN):** Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task—using it for video generation.  \n",
      "   - **Description (CN):** 扩散模型在过去几年中在图像合成方面取得了显著成果。现在，研究界已经开始着手一个更难的任务——将其用于视频生成。  \n",
      "   - **Website:** Lil'Log\n",
      "\n",
      "4. **Thinking about High-Quality Human Data | 关于高质量人类数据的思考**  \n",
      "   - **Description (EN):** High-quality data is the fuel for modern data deep learning model training. Most of the task-specific labeled data comes from human annotation.  \n",
      "   - **Description (CN):** 高质量的数据是现代深度学习模型训练的燃料。大多数特定任务的标记数据来自人工标注。  \n",
      "   - **Website:** Lil'Log\n",
      "\n",
      "5. **Adversarial Attacks on LLMs | 对大型语言模型的对抗攻击**  \n",
      "   - **Description (EN):** The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI) have invested a lot of effort to build default safe behavior into the model during the alignment process.  \n",
      "   - **Description (CN):** 在现实世界中使用大型语言模型随着ChatGPT的推出而大大加速。我们（包括我在OpenAI的团队）在对齐过程中投入了大量努力，以使模型具备默认的安全行为。  \n",
      "   - **Website:** Lil'Log\n",
      "\n",
      "6. **LLM Powered Autonomous Agents | 由大型语言模型驱动的自主代理**  \n",
      "   - **Description (EN):** Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer, and BabyAGI, serve as inspiring examples.  \n",
      "   - **Description (CN):** 以大型语言模型（LLM）为核心控制器构建代理是一个很酷的概念。几个概念验证演示，如AutoGPT、GPT-Engineer和BabyAGI，都是鼓舞人心的例子。  \n",
      "   - **Website:** Lil'Log\n",
      "\n",
      "7. **Prompt Engineering | 提示工程**  \n",
      "   - **Description (EN):** Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights.  \n",
      "   - **Description (CN):** 提示工程，也称为上下文提示，是指如何与大型语言模型沟通以引导其行为以达到预期结果的方法，而不更新模型权重。  \n",
      "   - **Website:** Lil'Log\n",
      "\n",
      "8. **The Transformer Family Version 2.0 | 变压器家族版本2.0**  \n",
      "   - **Description (EN):** Many new Transformer architecture improvements have been proposed since my last post on “The Transformer Family” about three years ago. Here I did a big refactoring and enrichment of that 2020 post.  \n",
      "   - **Description (CN):** 自我上次关于“变压器家族”的文章发表以来，已经提出了许多新的变压器架构改进。在这里，我对2020年的那篇文章进行了大规模重构和丰富。  \n",
      "   - **Website:** Lil'Log\n",
      "\n",
      "9. **Large Transformer Model Inference Optimization | 大型变压器模型推理优化**  \n",
      "   - **Description (EN):** Large transformer models are mainstream nowadays, creating SoTA results for a variety of tasks. They are powerful but very expensive to train and use.  \n",
      "   - **Description (CN):** 大型变压器模型如今已成为主流，为各种任务创造了最先进的结果。它们功能强大，但训练和使用成本非常高。  \n",
      "   - **Website:** Lil'Log\n",
      "\n",
      "10. **Some Math behind Neural Tangent Kernel | 神经切线核背后的数学**  \n",
      "    - **Description (EN):** Neural tangent kernel (NTK) (Jacot et al. 2018) is a kernel to explain the evolution of neural networks during training via gradient descent.  \n",
      "    - **Description (CN):** 神经切线核（NTK）（Jacot等人，2018年）是一种内核，用于解释通过梯度下降训练期间神经网络的演变。  \n",
      "    - **Website:** Lil'Log\n",
      "\n",
      "These topics cover a range of recent advancements and discussions in the field of artificial intelligence and machine learning, particularly focusing on large language models, reinforcement learning, and transformer architectures.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import datetime  # Import datetime module\n",
    "import dashscope\n",
    "\n",
    "# Function to get raw HTML content from the websites\n",
    "def get_raw_html(domain):\n",
    "    try:\n",
    "        # Send request to the website and get the raw HTML content\n",
    "        response = requests.get(f'http://{domain}')\n",
    "        if response.status_code == 200:\n",
    "            return response.content\n",
    "        else:\n",
    "            return f\"Failed to retrieve content from {domain}. HTTP Status Code: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error while fetching content from {domain}: {str(e)}\"\n",
    "\n",
    "# Function to prepare the message for Qwen LLM analysis\n",
    "def analyze_with_qwen(domain, raw_html):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'You are a web researcher. Analyze the raw HTML content and extract key topics in the following format: \"1. Title | Description | Website\"'},\n",
    "        {'role': 'user', 'content': f'''\n",
    "        Analyze the raw HTML content from {domain} and provide the latest 10 topics with:\n",
    "        1. Article titles in English\n",
    "        2. Article titles in Chinese\n",
    "        3. One-line descriptions in English\n",
    "        4. One-line descriptions in Chinese\n",
    "        5. Website name\n",
    "        Use current date: {datetime.date.today()}.\n",
    "        HTML Content: {raw_html.decode('utf-8')}\n",
    "        '''}\n",
    "    ]\n",
    "\n",
    "    response = dashscope.Generation.call(\n",
    "        api_key=\"sk-1a28c3fcc7e044cbacd6faf47dc89755\",\n",
    "        model=\"qwen-max\",\n",
    "        messages=messages,\n",
    "        enable_search=True,\n",
    "        result_format='message'\n",
    "    )\n",
    "    return response['output']['choices'][0]['message']['content']\n",
    "\n",
    "# List of websites to scrape\n",
    "websites = [\n",
    "    \"lilianweng.github.io\",\n",
    "]\n",
    "\n",
    "# Scraping and analyzing websites\n",
    "for site in websites:\n",
    "    print(f\"Scraping raw HTML from {site}...\")\n",
    "    raw_html = get_raw_html(site)\n",
    "    \n",
    "    # Check if there was an error\n",
    "    if isinstance(raw_html, str) and ('Error' in raw_html or 'Failed' in raw_html):\n",
    "        print(raw_html)\n",
    "    else:\n",
    "        print(f\"Raw HTML retrieved from {site}. Analyzing with Qwen LLM...\\n\")\n",
    "        qwen_analysis = analyze_with_qwen(site, raw_html)\n",
    "        print(f\"Analysis from {site}:\\n{qwen_analysis}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "import datetime\n",
    "import dashscope\n",
    "\n",
    "# Function to get raw HTML content from the websites\n",
    "def get_raw_html(domain):\n",
    "    try:\n",
    "        # Send request to the website and get the raw HTML content\n",
    "        response = requests.get(f'http://{domain}')\n",
    "        if response.status_code == 200:\n",
    "            return response.content\n",
    "        else:\n",
    "            return f\"Failed to retrieve content from {domain}. HTTP Status Code: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error while fetching content from {domain}: {str(e)}\"\n",
    "\n",
    "# Function to prepare the message for Qwen LLM analysis\n",
    "def analyze_with_qwen(domain, raw_html):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'You are a web researcher. Analyze the raw HTML content and extract key topics in the following format: \"1. Title | Description | Website\"'},\n",
    "        {'role': 'user', 'content': f'''\n",
    "        Analyze the raw HTML content from {domain} and provide the latest 10 topics with:\n",
    "        1. Article titles in English\n",
    "        2. Article titles in Chinese\n",
    "        3. One-line descriptions in English\n",
    "        4. One-line descriptions in Chinese\n",
    "        5. Website name\n",
    "        Use current date: {datetime.date.today()}.\n",
    "        HTML Content: {raw_html.decode('utf-8')}\n",
    "        '''}\n",
    "    ]\n",
    "\n",
    "    response = dashscope.Generation.call(\n",
    "        api_key=\"sk-1a28c3fcc7e044cbacd6faf47dc89755\",\n",
    "        model=\"qwen-max\",\n",
    "        messages=messages,\n",
    "        enable_search=True,\n",
    "        result_format='message'\n",
    "    )\n",
    "    return response['output']['choices'][0]['message']['content']\n",
    "\n",
    "# Streamlit UI components\n",
    "st.title(\"Minerva Agent\")   \n",
    "\n",
    "# List of default websites\n",
    "default_websites = [\n",
    "    \"lilianweng.github.io\",\n",
    "    \"qbitai.com\",\n",
    "    \"jiqizhixin.com\",\n",
    "    \"x.com/deepseek_ai\"\n",
    "]\n",
    "\n",
    "# Input for user to add websites\n",
    "input_websites = st.text_area(\"Website Domains(, Seperated)\", \n",
    "                              value=', '.join(default_websites), \n",
    "                              height=100)\n",
    "\n",
    "# Convert input string to a list of websites\n",
    "websites = [site.strip() for site in input_websites.split(',')]\n",
    "\n",
    "# Display results\n",
    "for site in websites:\n",
    "    st.write(f\"### Pulling {site}...\")\n",
    "    \n",
    "    # Get raw HTML\n",
    "    raw_html = get_raw_html(site)\n",
    "    \n",
    "    # Check if there was an error\n",
    "    if isinstance(raw_html, str) and ('Error' in raw_html or 'Failed' in raw_html):\n",
    "        st.error(raw_html)\n",
    "    else:\n",
    "        st.write(f\"Raw HTML retrieved from {site}. Analyzing with Qwen LLM...\\n\")\n",
    "        \n",
    "        # Perform Qwen analysis\n",
    "        qwen_analysis = analyze_with_qwen(site, raw_html)\n",
    "        \n",
    "        # Display results\n",
    "        st.write(f\"### {site} Summary:\\n\")\n",
    "        st.text_area(f\" {site}\", qwen_analysis, height=300)\n",
    "\n",
    "    st.markdown(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ntscraper\n",
      "  Using cached ntscraper-0.3.17-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: requests>=2.28 in ./.venv/lib/python3.12/site-packages (from ntscraper) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11 in ./.venv/lib/python3.12/site-packages (from ntscraper) (4.13.3)\n",
      "Requirement already satisfied: lxml>=4.9 in ./.venv/lib/python3.12/site-packages (from ntscraper) (5.3.1)\n",
      "Requirement already satisfied: tqdm>=4.66 in ./.venv/lib/python3.12/site-packages (from ntscraper) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.12/site-packages (from beautifulsoup4>=4.11->ntscraper) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./.venv/lib/python3.12/site-packages (from beautifulsoup4>=4.11->ntscraper) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.28->ntscraper) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.28->ntscraper) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.28->ntscraper) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.28->ntscraper) (2025.1.31)\n",
      "Using cached ntscraper-0.3.17-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: ntscraper\n",
      "Successfully installed ntscraper\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ntscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing instances: 100%|██████████| 8/8 [00:14<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-Feb-25 15:27:38 - Empty page on https://lightbrd.com\n"
     ]
    }
   ],
   "source": [
    "from ntscraper import Nitter\n",
    "scraper = Nitter(log_level=1, skip_instance_check=False)\n",
    "sama_tweets = scraper.get_tweets(\"sama\", mode='user', instance = 'https://lightbrd.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing instances: 100%|██████████| 8/8 [00:22<00:00,  2.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping tweets from @sama\n",
      "24-Feb-25 15:23:04 - Empty page on https://nitter.privacydev.net\n",
      "Error or unexpected response for @sama: {'tweets': [], 'threads': []}\n",
      "Scraping tweets from @ylecun\n",
      "24-Feb-25 15:23:08 - Empty page on https://nitter.privacydev.net\n",
      "Error or unexpected response for @ylecun: {'tweets': [], 'threads': []}\n",
      "Scraping tweets from @AndrewYNg\n",
      "24-Feb-25 15:23:12 - Empty page on https://nitter.privacydev.net\n",
      "Error or unexpected response for @AndrewYNg: {'tweets': [], 'threads': []}\n",
      "Scraping tweets from @fchollet\n",
      "24-Feb-25 15:23:16 - Empty page on https://nitter.privacydev.net\n",
      "Error or unexpected response for @fchollet: {'tweets': [], 'threads': []}\n",
      "Scraping tweets from @_KarenHao\n",
      "24-Feb-25 15:23:19 - Empty page on https://nitter.privacydev.net\n",
      "Error or unexpected response for @_KarenHao: {'tweets': [], 'threads': []}\n",
      "Scraping tweets from @karpathy\n",
      "24-Feb-25 15:23:23 - Empty page on https://nitter.privacydev.net\n",
      "Error or unexpected response for @karpathy: {'tweets': [], 'threads': []}\n",
      "Scraping tweets from @SchmidhuberAI\n",
      "24-Feb-25 15:23:27 - Empty page on https://nitter.privacydev.net\n",
      "Error or unexpected response for @SchmidhuberAI: {'tweets': [], 'threads': []}\n",
      "Scraping tweets from @sarahookr\n",
      "24-Feb-25 15:23:31 - Empty page on https://nitter.privacydev.net\n",
      "Error or unexpected response for @sarahookr: {'tweets': [], 'threads': []}\n",
      "Scraping tweets from @demishassabis\n",
      "24-Feb-25 15:23:34 - Empty page on https://nitter.privacydev.net\n",
      "Error or unexpected response for @demishassabis: {'tweets': [], 'threads': []}\n",
      "Scraping tweets from @saranormous\n",
      "24-Feb-25 15:23:38 - Empty page on https://nitter.privacydev.net\n",
      "Error or unexpected response for @saranormous: {'tweets': [], 'threads': []}\n",
      "Scraping tweets from @hardmaru\n",
      "24-Feb-25 15:23:41 - Empty page on https://nitter.privacydev.net\n",
      "Error or unexpected response for @hardmaru: {'tweets': [], 'threads': []}\n",
      "Scraping tweets from @lilianweng\n",
      "24-Feb-25 15:23:45 - Empty page on https://nitter.privacydev.net\n",
      "Error or unexpected response for @lilianweng: {'tweets': [], 'threads': []}\n",
      "Scraping tweets from @OriolVinyalsML\n",
      "24-Feb-25 15:23:49 - Empty page on https://nitter.privacydev.net\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping tweets from @\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     tweets \u001b[38;5;241m=\u001b[39m \u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msince\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msince_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muntil\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muntil_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://nitter.privacydev.net\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# Check if the tweets are returned correctly\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tweets, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/ntscraper/nitter.py:932\u001b[0m, in \u001b[0;36mNitter.get_tweets\u001b[0;34m(self, terms, mode, number, since, until, near, language, to, replies, filters, exclude, max_retries, instance)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(terms) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    930\u001b[0m     term \u001b[38;5;241m=\u001b[39m terms\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m--> 932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43mterm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m        \u001b[49m\u001b[43msince\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43muntil\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnear\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(terms) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    948\u001b[0m     term \u001b[38;5;241m=\u001b[39m terms[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/ntscraper/nitter.py:780\u001b[0m, in \u001b[0;36mNitter._search\u001b[0;34m(self, term, mode, number, since, until, near, language, to, replies, filters, exclude, max_retries, instance)\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    778\u001b[0m         endpoint \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?scroll=false\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 780\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m soup \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tweets\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/ntscraper/nitter.py:296\u001b[0m, in \u001b[0;36mNitter._get_page\u001b[0;34m(self, endpoint, max_retries)\u001b[0m\n\u001b[1;32m    290\u001b[0m                         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_session(\n\u001b[1;32m    291\u001b[0m                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_new_instance(\n\u001b[1;32m    292\u001b[0m                                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError fetching \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m                             )\n\u001b[1;32m    294\u001b[0m                         )\n\u001b[1;32m    295\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 296\u001b[0m     \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_count \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_retries:\n\u001b[1;32m    299\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax retries reached. Check your request and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ntscraper import Nitter\n",
    "import datetime\n",
    "import pprint\n",
    "\n",
    "# Initialize the scraper\n",
    "scraper = Nitter(log_level=1, skip_instance_check=False)\n",
    "\n",
    "# Define the accounts and initialize the current date\n",
    "accounts = [\n",
    "    \"sama\", \"ylecun\", \"AndrewYNg\", \"fchollet\", \"_KarenHao\", \"karpathy\", \"SchmidhuberAI\", \"sarahookr\", \n",
    "    \"demishassabis\", \"saranormous\", \"hardmaru\", \"lilianweng\", \"OriolVinyalsML\", \"Michael_J_Black\", \n",
    "    \"JeffDean\", \"goodfellow_ian\", \"achowdhery\", \"PeterDiamandis\", \"GaryMarcus\", \"giffmana\", \n",
    "    \"rasbt\", \"quaesita\", \"KateKayeReports\", \"EMostaque\", \"drfeifei\", \"DrJimFan\", \"omarsar0\", \n",
    "    \"conniechan\", \"hugo_larochelle\", \"benjedwards\", \"rebecca_szkutak\", \"svlevine\", \"ericschmidt\", \n",
    "    \"ilyasut\", \"patrickmineault\", \"natashajaques\", \"pabbeel\", \"ESYudkowsky\", \"geoffreyhinton\", \n",
    "    \"wintonARK\", \"jeffclune\", \"RamaswmySridhar\", \"bentossell\", \"johnschulman2\", \"_akhaliq\", \n",
    "    \"quocleix\", \"jackclarkSF\", \"mervenoyann\", \"DavidSHolz\", \"natolambert\", \"RichardSocher\", \n",
    "    \"mustafasuleymn\", \"ZoubinGhahrama1\", \"nathanbenaich\", \"johnvmcdonnell\", \"tunguz\", \"bengoertzel\", \n",
    "    \"ch402\", \"Kseniase_\", \"paulg\", \"rsalakhu\", \"gdb\", \"vivnat\", \"bxchen\", \"AnimaAnandkumar\", \n",
    "    \"JeffreyTowson\", \"Thom_Wolf\", \"johnplattml\", \"SamanyouGarg\", \"KirkDBorne\", \"Alber_RomGar\", \n",
    "    \"SilverJacket\", \"ecsquendor\", \"jordnb\", \"jluan\", \"NPCollapse\", \"NaveenGRao\", \"azeem\", \"Suhail\", \n",
    "    \"maxjaderberg\", \"Kyle_L_Wiggers\", \"cocoweixu\", \"aidangomezzz\", \"alexandr_wang\", \"CaimingXiong\", \n",
    "    \"YiMaTweets\", \"notmisha\", \"peteratmsr\", \"shivon\", \"jackyliang42\", \"v_vashishta\", \"xdh\", \n",
    "    \"FryRsquared\", \"ravi_lsvp\", \"ClementDelangue\", \"oh_that_hat\", \"sapna\", \"VRLalchand\", \"svpino\", \n",
    "    \"ceobillionaire\", \"ykilcher\", \"BornsteinMatt\", \"lachygroom\", \"goodside\", \"amasad\", \"polynoamial\", \n",
    "    \"sytelus\"\n",
    "]\n",
    "\n",
    "# Get today's date and calculate 5 days ago\n",
    "today = datetime.date.today()\n",
    "five_days_ago = today - datetime.timedelta(days=5)\n",
    "\n",
    "# Format the dates to match the scraping requirement\n",
    "since_date = five_days_ago.strftime('%Y-%m-%d')\n",
    "until_date = today.strftime('%Y-%m-%d')\n",
    "\n",
    "# Scrape tweets from each account for the last 5 days\n",
    "tweets_data = {}\n",
    "for account in accounts:\n",
    "    print(f\"Scraping tweets from @{account}\")\n",
    "    try:\n",
    "        tweets = scraper.get_tweets(account, mode='user', since=since_date, until=until_date, instance='https://nitter.privacydev.net')\n",
    "        \n",
    "        # Check if the tweets are returned correctly\n",
    "        if isinstance(tweets, dict):\n",
    "            print(f\"Error or unexpected response for @{account}: {tweets}\")\n",
    "            continue  # Skip this account if the response is not as expected\n",
    "        \n",
    "        # Store the relevant tweet data (including likes, retweets, and links)\n",
    "        tweets_data[account] = []\n",
    "        for tweet in tweets:\n",
    "            tweet_data = {\n",
    "                'tweet': tweet['text'],\n",
    "                'likes': tweet['stats']['likes'],\n",
    "                'retweets': tweet['stats']['retweets'],\n",
    "                'link': tweet['link']\n",
    "            }\n",
    "            tweets_data[account].append(tweet_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping @{account}: {e}\")\n",
    "\n",
    "# Print or store the results as needed\n",
    "# For example, display the data for the first account:\n",
    "pprint.pprint(tweets_data[\"sama\"])  # Print tweets for Sam Altman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 微信cookie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rewardsn=; wxtokenkey=777; pac_uid=0_WFRCchx025Cww; _qimei_uuid42=191090f32121004a40ded1e5e650d9677d9210f8fb; _qimei_h38=e963446740ded1e5e650d96703000003119109; _qimei_q36=; ua_id=fIxXt1qo3N1AkUI9AAAAAE7bKLDM8dls2W8RivxiLs4=; wxuin=36409384414630; suid=user_0_WFRCchx025Cww; mm_lang=zh_CN; sig=h016ff31a4a1ff5262376ab723fd8d807ea82f9552e933b7d087ca0bd6cd2ce703cdaaf9f90ae8c1544; ab.storage.deviceId.f9c2b69f-2136-44e0-a55a-dff72d99aa19=g%3AYoJZqng6gcdvly5aBDxZqgiJ1GZ2%7Ce%3Aundefined%7Cc%3A1739462526631%7Cl%3A1739462526631; ab.storage.sessionId.f9c2b69f-2136-44e0-a55a-dff72d99aa19=g%3A7bafc696-6715-6e6b-565d-5695541d32ca%7Ce%3A1739464326655%7Cc%3A1739462526656%7Cl%3A1739462526656; _qimei_fingerprint=d91ba6f60a0e30a68c3644052fa00145; uuid=ca13ea4de5017925ec58ce75b297c0d9; _clck=1mr2pfh|1|ftp|0; rand_info=CAESIE2845Ww7udUcmhfjajRvqB5ZjrkLpk+WYgiwUr3mbiB; slave_bizuin=3918520287; data_bizuin=3918520287; bizuin=3918520287; data_ticket=AA14hMmWJuNmOqpwkhsBDAw/VS3bQDqPl6lnS+ECjS5dDeaQzIfZThM2cDGRWVcj; slave_sid=M2Y0MzNLYVBUTFNyZjc4VUgwR0VNWFVVMHJLblMwNFBnR3JIZ2FMMGZKUmxzTkZKX3Q3ak5kU2VRWE1oUzlRMXBCY1NMMGdjOHR0bGFFQjNKTkl0MlVDeWpfTFhESVI1UXhzMHZjYXk4WTJjRFpVcVVkdlA1R1NHVTVseWlkOUNYdW8zc242VTlIRDY4M1pZ; slave_user=gh_b89c7dbe2d0d; xid=3abe7a2c901b43c1ab83b44e9b8e6b74; _clsk=1k5eby|1740383197389|10|1|mp.weixin.qq.com/weheat-agent/payload/record\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "token: 1519694182\n",
    "fakeid: Mzg5Mjg2OTM2Nw==\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目标url\n",
    "url = \"https://mp.weixin.qq.com/cgi-bin/appmsg\"\n",
    "cookie = \"rewardsn=; wxtokenkey=777; pac_uid=0_WFRCchx025Cww; _qimei_uuid42=191090f32121004a40ded1e5e650d9677d9210f8fb; _qimei_h38=e963446740ded1e5e650d96703000003119109; _qimei_q36=; ua_id=fIxXt1qo3N1AkUI9AAAAAE7bKLDM8dls2W8RivxiLs4=; wxuin=36409384414630; suid=user_0_WFRCchx025Cww; mm_lang=zh_CN; sig=h016ff31a4a1ff5262376ab723fd8d807ea82f9552e933b7d087ca0bd6cd2ce703cdaaf9f90ae8c1544; ab.storage.deviceId.f9c2b69f-2136-44e0-a55a-dff72d99aa19=g%3AYoJZqng6gcdvly5aBDxZqgiJ1GZ2%7Ce%3Aundefined%7Cc%3A1739462526631%7Cl%3A1739462526631; ab.storage.sessionId.f9c2b69f-2136-44e0-a55a-dff72d99aa19=g%3A7bafc696-6715-6e6b-565d-5695541d32ca%7Ce%3A1739464326655%7Cc%3A1739462526656%7Cl%3A1739462526656; _qimei_fingerprint=d91ba6f60a0e30a68c3644052fa00145; uuid=ca13ea4de5017925ec58ce75b297c0d9; _clck=1mr2pfh|1|ftp|0; rand_info=CAESIE2845Ww7udUcmhfjajRvqB5ZjrkLpk+WYgiwUr3mbiB; slave_bizuin=3918520287; data_bizuin=3918520287; bizuin=3918520287; data_ticket=AA14hMmWJuNmOqpwkhsBDAw/VS3bQDqPl6lnS+ECjS5dDeaQzIfZThM2cDGRWVcj; slave_sid=M2Y0MzNLYVBUTFNyZjc4VUgwR0VNWFVVMHJLblMwNFBnR3JIZ2FMMGZKUmxzTkZKX3Q3ak5kU2VRWE1oUzlRMXBCY1NMMGdjOHR0bGFFQjNKTkl0MlVDeWpfTFhESVI1UXhzMHZjYXk4WTJjRFpVcVVkdlA1R1NHVTVseWlkOUNYdW8zc242VTlIRDY4M1pZ; slave_user=gh_b89c7dbe2d0d; xid=3abe7a2c901b43c1ab83b44e9b8e6b74; _clsk=1k5eby|1740383197389|10|1|mp.weixin.qq.com/weheat-agent/payload/record\"\n",
    "\n",
    "headers = {\n",
    "    \"Cookie\": cookie,\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.75 Mobile Safari/537.36\",\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"token\": \"1519694182\",\n",
    "    \"lang\": \"zh_CN\",\n",
    "    \"f\": \"json\",\n",
    "    \"ajax\": \"1\",\n",
    "    \"action\": \"list_ex\",\n",
    "    \"begin\": \"0\",\n",
    "    \"count\": \"5\",\n",
    "    \"query\": \"\",\n",
    "    \"fakeid\": \"Mzg5Mjg2OTM2Nw==\", # 自己的号，设置为空\n",
    "    \"type\": \"9\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_count():\n",
    "    content_json = requests.get(url, headers=headers, params=data).json()\n",
    "    count = int(content_json[\"app_msg_cnt\"])\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_list(count, per_page=5):\n",
    "    page = int(math.ceil(count / per_page))\n",
    "    content_list = []\n",
    "    for i in tqdm(range(page), desc=\"获取文章列表\"):\n",
    "        data[\"begin\"] = i * per_page\n",
    "        content_json = requests.get(url, headers=headers, params=data).json()\n",
    "        content_list.extend(content_json[\"app_msg_list\"])\n",
    "        time.sleep(random.randint(5, 10))\n",
    "        # 保存成json\n",
    "        with open(\"content_list.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(content_list, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def precess_content_list():\n",
    "    content_list = json.load(open(\"content_list.json\", \"r\", encoding=\"utf-8\"))\n",
    "    results_list = []\n",
    "    for item in content_list:\n",
    "        title = item[\"title\"]\n",
    "        link = item[\"link\"]\n",
    "        create_time = time.strftime(\"%Y-%m-%d %H:%M\", time.localtime(item[\"create_time\"]))\n",
    "        results_list.append([title, link, create_time])\n",
    "    name = ['title', 'link', 'create_time']\n",
    "    data = pd.DataFrame(columns=name, data=results_list)\n",
    "    data.to_csv(\"data.csv\", mode='w', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "content_list = []\n",
    "for i in range(20):\n",
    "    data[\"begin\"] = i*5\n",
    "    time.sleep(5)\n",
    "    # 使用get方法进行提交\n",
    "    content_json = requests.get(url, headers=headers, params=data).json()\n",
    "    # 返回了一个json，里面是每一页的数据\n",
    "    for item in content_json[\"app_msg_list\"]:    \n",
    "    # 提取每页文章的标题及对应的url\n",
    "        items = []\n",
    "        items.append(item[\"title\"])\n",
    "        items.append(item[\"link\"])\n",
    "        t = time.localtime(item[\"create_time\"])\n",
    "        items.append(time.strftime(\"%Y-%m-%d %H:%M:%S\", t))\n",
    "        content_list.append(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ['title', 'link', 'create_time']\n",
    "test = pd.DataFrame(columns=name, data=content_list)\n",
    "test.to_csv(\"url.csv\", mode='a', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "content_json = requests.get(url, headers=headers, params=data).json()\n",
    "count = int(content_json[\"app_msg_cnt\"])\n",
    "print(count)\n",
    "page = int(math.ceil(count / 5))\n",
    "print(page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "if (i > 0) and (i % 10 == 0):\n",
    "        name = ['title', 'link', 'create_time']\n",
    "        test = pd.DataFrame(columns=name, data=content_list)\n",
    "        test.to_csv(\"url.csv\", mode='a', encoding='utf-8')\n",
    "        print(\"第\" + str(i) + \"次保存成功\")\n",
    "        content_list = []\n",
    "        time.sleep(random.randint(60,90))\n",
    "else:\n",
    "    time.sleep(random.randint(15,25))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
