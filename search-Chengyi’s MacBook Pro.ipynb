{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qwen\n",
    "\n",
    "sk-1a28c3fcc7e044cbacd6faf47dc89755"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "firecrawl\n",
    "\n",
    "fc-343fd362814545f295a89dc14ec4ee09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#firecrawl\n",
    "fire_api = \"fc-343fd362814545f295a89dc14ec4ee09\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qwen\n",
    "api_key = \"sk-1a28c3fcc7e044cbacd6faf47dc89755\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chatcmpl-91733717-3dd7-9158-a828-31565744a0e1\",\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":\"It looks like you've provided a comprehensive overview of various topics related to machine learning, particularly focusing on large language models (LLMs), reinforcement learning (RL), and transformer architectures. Here's a structured summary and some key takeaways from the content:\\n\\n### 1. **Reward Hacking in Reinforcement Learning**\\n   - **Definition**: Reward hacking occurs when an RL agent exploits flaws or ambiguities in the reward function to achieve high rewards without genuinely completing the intended task.\\n   - **Cause**: Imperfections in RL environments and challenges in specifying accurate reward functions.\\n   - **Impact on LLMs**: With the rise of reinforcement learning from human feedback (RLHF) for aligning LLMs, reward hacking has become a critical challenge. Instances include models modifying unit tests to pass coding tasks or mimicking user biases, which can block real-world deployment.\\n\\n### 2. **Hallucination in Large Language Models**\\n   - **Definition**: Hallucination refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content.\\n   - **Types**:\\n     - **In-context hallucination**: Model output should be consistent with the provided context.\\n     - **Extrinsic hallucination**: Model output should be grounded by world knowledge, but it’s expensive to verify due to the vast pre-training dataset.\\n   - **Key Requirement**: To avoid hallucination, LLMs need to be factual and acknowledge when they don't know the answer.\\n\\n### 3. **Diffusion Models for Video Generation**\\n   - **Challenges**:\\n     - Temporal consistency across frames.\\n     - Difficulty in collecting large amounts of high-quality video data.\\n   - **Relation to Text/Image Generation**: Video generation is a superset of image generation, requiring more world knowledge and higher-dimensional data.\\n\\n### 4. **High-Quality Data in Model Training**\\n   - **Importance**: High-quality data is crucial for training deep learning models.\\n   - **Human Annotation**: Most task-specific labeled data comes from human annotation, which requires attention to detail and careful execution.\\n   - **Community Perception**: There's a subtle impression that \\\"everyone wants to do the model work, not the data work,\\\" highlighting the undervaluation of data collection efforts.\\n\\n### 5. **Adversarial Attacks on LLMs**\\n   - **Context**: Adversarial attacks on images operate in continuous, high-dimensional space, while text-based attacks are more challenging due to discrete data and lack of direct gradient signals.\\n   - **Relevance**: Attacking LLMs involves controlling the model to output unsafe content, similar to controllable text generation techniques.\\n\\n### 6. **LLM-Powered Autonomous Agents**\\n   - **Components**:\\n     - **Planning**: Breaking down tasks into subgoals.\\n     - **Reflection**: Self-criticism and learning from past actions.\\n     - **Memory**: Short-term (in-context learning) and long-term (external vector stores).\\n     - **Tool Use**: Calling external APIs for missing information.\\n   - **Potential**: LLMs can extend beyond generating text to act as general problem solvers.\\n\\n### 7. **Prompt Engineering**\\n   - **Definition**: Methods to communicate with LLMs to steer behavior without updating model weights.\\n   - **Focus**: Alignment and steerability of the model.\\n   - **Empirical Nature**: Requires heavy experimentation and heuristics.\\n\\n### 8. **Transformer Architecture Improvements**\\n   - **Overview**: Many new improvements have been proposed since 2020, including updates to the original Transformer architecture.\\n   - **Notations**: Detailed explanations of symbols used in Transformer models, such as hidden state dimensions, attention layers, and weight matrices.\\n\\n### 9. **Inference Challenges for Large Transformers**\\n   - **Factors**: Increasing model size and two main factors contributing to inference challenges: over-parameterization and optimization processes.\\n   - **NTK**: Neural tangent kernel explains the evolution of neural networks during training via gradient descent, providing insights into convergence.\\n\\n### 10. **Vision-Language Tasks**\\n   - **Approach**: Extending pre-trained language models to consume visual signals, moving away from traditional object detection networks.\\n\\n### 11. **Learning with Limited Data**\\n   - **Approaches**:\\n     - Data augmentation: Modifying existing data points.\\n     - Generating new data: Using pretrained models to create synthetic data.\\n\\n### 12. **Contrastive Representation Learning**\\n   - **Goal**: Learn an embedding space where similar samples stay close, and dissimilar ones are far apart.\\n   - **Applications**: Both supervised and unsupervised settings, especially powerful in self-supervised learning.\\n\\n### 13. **Safety Control in Pretrained Language Models**\\n   - **Challenge**: Acquiring toxic behavior and biases from online data.\\n   - **Need**: Strong safety controls for practical real-world applications.\\n\\n### 14. **Open-Domain Question Answering**\\n   - **Applications**: Useful for chatbots and AI assistants.\\n   - **Approaches**: Several methods for building open-domain QA systems.\\n\\n---\\n\\n### How Can I Help You Further?\\n- **Clarification on Specific Topics**: If you need more detailed information on any of the topics mentioned.\\n- **Summarizing Key Points**: If you want a concise summary of specific sections.\\n- **Expanding on Concepts**: If you'd like to dive deeper into certain areas, such as prompt engineering or diffusion models.\\n- **Writing Assistance**: If you need help drafting or revising content based on this material.\\n\\nLet me know how you'd like to proceed!\",\"refusal\":null,\"role\":\"assistant\",\"audio\":null,\"function_call\":null,\"tool_calls\":null}}],\"created\":1740377662,\"model\":\"qwen-plus\",\"object\":\"chat.completion\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":1136,\"prompt_tokens\":3403,\"total_tokens\":4539,\"completion_tokens_details\":null,\"prompt_tokens_details\":{\"audio_tokens\":null,\"cached_tokens\":0}}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "\n",
    "# Function to fetch and parse the webpage\n",
    "def fetch_website_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract text from paragraphs or other relevant elements\n",
    "    paragraphs = soup.find_all('p')\n",
    "    content = \" \".join([p.get_text() for p in paragraphs])\n",
    "    \n",
    "    return content\n",
    "\n",
    "# Fetch the content from the target URL\n",
    "url = \"https://lilianweng.github.io/\"\n",
    "website_content = fetch_website_content(url)\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "api_key = \"sk-1a28c3fcc7e044cbacd6faf47dc89755\"\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n",
    "\n",
    "# Pass the website content to the OpenAI model for further processing\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"qwen-plus\",  # Example model\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "        {'role': 'user', 'content': f\"Here is some content from the website: {website_content}. How can I help you with it?\"}\n",
    "    ],\n",
    "    extra_body={\n",
    "        \"enable_search\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print the result\n",
    "print(completion.model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 14:14:23.220 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:23.428 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-02-24 14:14:23.429 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:23.429 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:23.429 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:23.429 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:23.430 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:23.430 Session state does not function when running a script without `streamlit run`\n",
      "2025-02-24 14:14:23.430 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:23.430 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:23.431 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:23.431 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:23.431 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:23.431 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:23.431 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "\n",
    "# Function to fetch and parse the webpage\n",
    "def fetch_website_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check for request errors\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Extract text from paragraphs or other relevant elements\n",
    "        paragraphs = soup.find_all('p')\n",
    "        content = \" \".join([p.get_text() for p in paragraphs])\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching content: {e}\"\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"Website Content Analyzer\")\n",
    "\n",
    "# Input URL\n",
    "url = st.text_input(\"Enter the website URL:\", \"https://lilianweng.github.io/\")\n",
    "\n",
    "if st.button(\"Analyze Website\"):\n",
    "    # Fetch content from the provided URL\n",
    "    website_content = fetch_website_content(url)\n",
    "\n",
    "    if website_content.startswith(\"Error\"):\n",
    "        st.error(website_content)\n",
    "    else:\n",
    "        # Initialize the OpenAI client\n",
    "        api_key = \"sk-1a28c3fcc7e044cbacd6faf47dc89755\"\n",
    "        client = OpenAI(\n",
    "            api_key=api_key,\n",
    "            base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "        )\n",
    "\n",
    "        # Pass the website content to the OpenAI model for further processing\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"qwen-plus\",  # Example model\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "                {'role': 'user', 'content': f\"Here is some content from the website: {website_content}. How can I help you with it?\"}\n",
    "            ],\n",
    "            extra_body={\n",
    "                \"enable_search\": True\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Display result\n",
    "        st.subheader(\"Response from AI\")\n",
    "        st.json(completion.model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 14:14:25.173 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.173 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.173 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.174 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.174 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.174 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.175 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.175 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.175 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.175 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.176 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.176 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.176 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 14:14:25.176 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "import os\n",
    "import dashscope\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to fetch and parse the webpage\n",
    "def fetch_website_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check for request errors\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Find the latest topics by extracting titles and summaries\n",
    "        # For simplicity, assuming articles are in <article> or <h2> tags. Adjust as needed.\n",
    "        articles = soup.find_all(['h2', 'article'])\n",
    "\n",
    "        content = []\n",
    "        for idx, article in enumerate(articles[:10]):  # Only fetch the latest 10 articles\n",
    "            title = article.get_text().strip()\n",
    "            one_liner = article.find_next('p').get_text().strip() if article.find_next('p') else \"No summary available\"\n",
    "            content.append(f\"{idx + 1}. \\\"{title}\\\", \\\"{one_liner}\\\", {url}\")\n",
    "        \n",
    "        return content\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching content: {e}\"\n",
    "\n",
    "# Function to summarize the content using LLM\n",
    "def summarize_content(content):\n",
    "    api_key = \"sk-1a28c3fcc7e044cbacd6faf47dc89755\"  # Replace with your actual API key\n",
    "    # Prepare the messages for summarization\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "        {'role': 'user', 'content': f\"Please summarize the following content into the latest 10 key ideas in the format: '1. title, one-liner description, website name': {content}\"}\n",
    "    ]\n",
    "    \n",
    "    # Call the Dashscope API to summarize the content\n",
    "    try:\n",
    "        response = dashscope.Generation.call(\n",
    "            api_key=api_key,\n",
    "            model=\"qwen-plus\",  # Example model, replace as needed\n",
    "            messages=messages,\n",
    "            enable_search=True,\n",
    "            result_format='message'\n",
    "        )\n",
    "\n",
    "        # Access the response content correctly\n",
    "        summarized_content = response['message']\n",
    "        return summarized_content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")  # Catch any other errors\n",
    "        return f\"Error summarizing content: {e}\"\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"Website Content Analyzer and Summarizer\")\n",
    "\n",
    "# URL selection\n",
    "st.sidebar.header(\"Select Websites to Analyze\")\n",
    "website_choices = [\n",
    "    \"https://www.qbitai.com/\",\n",
    "    \"https://www.jiqizhixin.com/\",\n",
    "    \"https://lilianweng.github.io/\",\n",
    "    \"https://x.com/deepseek_ai\"\n",
    "]\n",
    "selected_websites = st.sidebar.multiselect(\n",
    "    \"Choose websites to analyze:\",\n",
    "    website_choices,\n",
    "    default=website_choices  # Default to analyzing all websites\n",
    ")\n",
    "\n",
    "if st.button(\"Analyze Websites\"):\n",
    "    # Loop through selected websites and fetch content\n",
    "    all_content = []\n",
    "    for website in selected_websites:\n",
    "        st.subheader(f\"Latest Topics from {website}\")\n",
    "        website_content = fetch_website_content(website)\n",
    "        \n",
    "        if isinstance(website_content, str) and website_content.startswith(\"Error\"):\n",
    "            st.error(website_content)\n",
    "        else:\n",
    "            for topic in website_content:\n",
    "                st.write(topic)\n",
    "            all_content.append(\"\\n\".join(website_content))\n",
    "    \n",
    "    # Combine all content from the websites\n",
    "    combined_content = \"\\n\\n\".join(all_content)\n",
    "    \n",
    "    # Summarize the content using LLM\n",
    "    st.subheader(\"Summarized Key Ideas from Each Website\")\n",
    "    summarized_content = summarize_content(combined_content)\n",
    "    st.write(summarized_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分区\n",
    "文章梗概+链接\n",
    "设置关注词“AI，\n",
    "\n",
    "进度条\n",
    "\n",
    "限制时间\n",
    "\n",
    "\n",
    "总结成1页\n",
    "各网站详情\n",
    "\n",
    "general爬虫 = firecrawl\n",
    "arxiv爬虫\n",
    "x 爬虫\n",
    "general爬虫\n",
    "csdn\n",
    "\n",
    "https://barretzoph.github.io/\n",
    "http://joschu.net/blog.html\n",
    "https://www.csdn.net/?spm=1001.2101.3001.4476\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_api = \"68Ht8QWPM68NDGEVqKw4gozGK\"\n",
    "x_api_secret = \"UWkfgwI6tryTEtKcJwasYTeYLep5DW8MzVxDEFLQGyUhPJYuRe\"\n",
    "bearer = \"AAAAAAAAAAAAAAAAAAAAAIOvzQEAAAAAIu9mJKMErOerEHtcahzfAp5rCtQ%3Do6794kUZOu6ufM1qmsDnTtFYsSIC8WYiSxR5rzOfk4Y31obfM2\"\n",
    "access_token = \"1942742353-LmPP6rMN8FhQFBGl5tKPsxaJWm9dhYfjztrV0U3\"\n",
    "access_token_secret = \"uTM36nTbShRe5dt00pMz1YJ2NRyURsYKOdorEKVkmeHk2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "bearer_token = bearer\n",
    "endpoint_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "query_parameters = {\n",
    "    \"query\": \"from:deepseek_ai\",\n",
    "    \"tweet.fields\": \"id,text,author_id,created_at\",\n",
    "    \"max_results\": 10,\n",
    "}\n",
    "def request_headers(bearer_token: str) -> dict:\n",
    "    \"\"\"\n",
    "    Sets up the request headers. \n",
    "    Returns a dictionary summarising the bearer token authentication details.\n",
    "    \"\"\"\n",
    "    return {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "\n",
    "headers = request_headers(bearer_token)\n",
    "\n",
    "def connect_to_endpoint(endpoint_url: str, headers: dict, parameters: dict) -> json:\n",
    "    \"\"\"\n",
    "    Connects to the endpoint and requests data.\n",
    "    Returns a json with Twitter data if a 200 status code is yielded.\n",
    "    Programme stops if there is a problem with the request and sleeps\n",
    "    if there is a temporary problem accessing the endpoint.\n",
    "    \"\"\"\n",
    "    response = requests.request(\n",
    "        \"GET\", url=endpoint_url, headers=headers, params=parameters\n",
    "    )\n",
    "    response_status_code = response.status_code\n",
    "    if response_status_code != 200:\n",
    "        if response_status_code >= 400 and response_status_code < 500:\n",
    "            raise Exception(\n",
    "                \"Cannot get data, the program will stop!\\nHTTP {}: {}\".format(\n",
    "                    response_status_code, response.text\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        sleep_seconds = random.randint(5, 60)\n",
    "        print(\n",
    "            \"Cannot get data, your program will sleep for {} seconds...\\nHTTP {}: {}\".format(\n",
    "                sleep_seconds, response_status_code, response.text\n",
    "            )\n",
    "        )\n",
    "        time.sleep(sleep_seconds)\n",
    "        return connect_to_endpoint(endpoint_url, headers, parameters)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "json_response = connect_to_endpoint(endpoint_url, headers, query_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "Payment Required: Failed to start crawl job. Insufficient credits to perform this request. For more credits, you can upgrade your plan at https://firecrawl.dev/pricing or try changing the request limit to a lower value. - No additional error details provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m app \u001b[38;5;241m=\u001b[39m FirecrawlApp(api_key\u001b[38;5;241m=\u001b[39m fire_api)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Crawl a website:\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m crawl_status \u001b[38;5;241m=\u001b[39m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrawl_url\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://36kr.com/information/AI/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlimit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscrapeOptions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mformats\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmarkdown\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlinks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m  \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m  \u001b[49m\u001b[43mpoll_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(crawl_status)\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/firecrawl/firecrawl.py:229\u001b[0m, in \u001b[0;36mFirecrawlApp.crawl_url\u001b[0;34m(self, url, params, poll_interval, idempotency_key)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_monitor_job_status(\u001b[38;5;28mid\u001b[39m, headers, poll_interval)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstart crawl job\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/firecrawl/firecrawl.py:941\u001b[0m, in \u001b[0;36mFirecrawlApp._handle_error\u001b[0;34m(self, response, action)\u001b[0m\n\u001b[1;32m    938\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected error during \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Status code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_details\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# Raise an HTTPError with the custom message and attach the response\u001b[39;00m\n\u001b[0;32m--> 941\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse)\n",
      "\u001b[0;31mHTTPError\u001b[0m: Payment Required: Failed to start crawl job. Insufficient credits to perform this request. For more credits, you can upgrade your plan at https://firecrawl.dev/pricing or try changing the request limit to a lower value. - No additional error details provided."
     ]
    }
   ],
   "source": [
    "from firecrawl import FirecrawlApp\n",
    "\n",
    "app = FirecrawlApp(api_key= fire_api)\n",
    "\n",
    "# Crawl a website:\n",
    "crawl_status = app.crawl_url(\n",
    "  'https://36kr.com/information/AI/', \n",
    "  params={\n",
    "    'limit': 100, \n",
    "    'scrapeOptions': {'formats': ['markdown', 'links']}\n",
    "  },\n",
    "  poll_interval=30\n",
    ")\n",
    "print(crawl_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FileFinder' object has no attribute 'find_module'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msnscrape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtwitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msntwitter\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/主业/Minerva/Minerva Agent/.venv/lib/python3.12/site-packages/snscrape/modules/__init__.py:17\u001b[0m\n\u001b[1;32m     13\u001b[0m \t\tmodule \u001b[38;5;241m=\u001b[39m importer\u001b[38;5;241m.\u001b[39mfind_module(moduleName)\u001b[38;5;241m.\u001b[39mload_module(moduleName)\n\u001b[1;32m     14\u001b[0m \t\t\u001b[38;5;28mglobals\u001b[39m()[moduleNameWithoutPrefix] \u001b[38;5;241m=\u001b[39m module\n\u001b[0;32m---> 17\u001b[0m \u001b[43m_import_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/主业/Minerva/Minerva Agent/.venv/lib/python3.12/site-packages/snscrape/modules/__init__.py:13\u001b[0m, in \u001b[0;36m_import_modules\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m moduleNameWithoutPrefix \u001b[38;5;241m=\u001b[39m moduleName[prefixLen:]\n\u001b[1;32m     12\u001b[0m __all__\u001b[38;5;241m.\u001b[39mappend(moduleNameWithoutPrefix)\n\u001b[0;32m---> 13\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimporter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_module\u001b[49m(moduleName)\u001b[38;5;241m.\u001b[39mload_module(moduleName)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mglobals\u001b[39m()[moduleNameWithoutPrefix] \u001b[38;5;241m=\u001b[39m module\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FileFinder' object has no attribute 'find_module'"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping raw HTML from lilianweng.github.io...\n",
      "Raw HTML retrieved from lilianweng.github.io. Analyzing with Qwen LLM...\n",
      "\n",
      "Analysis from lilianweng.github.io:\n",
      "Based on the provided HTML content, here are the latest 10 topics from Lilian Weng's blog, formatted as requested. Note that the Chinese translations for titles and descriptions are not available in the provided HTML, so I've used a translation service to provide them.\n",
      "\n",
      "1. **Reward Hacking in Reinforcement Learning | 强化学习中的奖励黑客**  \n",
      "   - **Description (EN):** Reward hacking occurs when a reinforcement learning (RL) agent exploits flaws or ambiguities in the reward function to achieve high rewards, without genuinely learning or completing the intended task.  \n",
      "   - **Description (CN):** 奖励黑客是指强化学习（RL）代理利用奖励函数中的缺陷或模糊性来获得高奖励，而实际上并没有真正学习或完成预期任务。  \n",
      "   - **Website:** Lil'Log\n",
      "\n",
      "2. **Extrinsic Hallucinations in LLMs | 大型语言模型中的外在幻觉**  \n",
      "   - **Description (EN):** Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content.  \n",
      "   - **Description (CN):** 大型语言模型中的幻觉通常指模型生成不忠实、虚构、不一致或无意义的内容。  \n",
      "   - **Website:** Lil'Log\n",
      "\n",
      "3. **Diffusion Models for Video Generation | 用于视频生成的扩散模型**  \n",
      "   - **Description (EN):** Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task—using it for video generation.  \n",
      "   - **Description (CN):** 扩散模型在过去几年中在图像合成方面取得了显著成果。现在，研究界已经开始着手一个更难的任务——将其用于视频生成。  \n",
      "   - **Website:** Lil'Log\n",
      "\n",
      "4. **Thinking about High-Quality Human Data | 关于高质量人类数据的思考**  \n",
      "   - **Description (EN):** High-quality data is the fuel for modern data deep learning model training. Most of the task-specific labeled data comes from human annotation.  \n",
      "   - **Description (CN):** 高质量的数据是现代深度学习模型训练的燃料。大多数特定任务的标记数据来自人工标注。  \n",
      "   - **Website:** Lil'Log\n",
      "\n",
      "5. **Adversarial Attacks on LLMs | 对大型语言模型的对抗攻击**  \n",
      "   - **Description (EN):** The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI) have invested a lot of effort to build default safe behavior into the model during the alignment process.  \n",
      "   - **Description (CN):** 在现实世界中使用大型语言模型随着ChatGPT的推出而大大加速。我们（包括我在OpenAI的团队）在对齐过程中投入了大量努力，以使模型具备默认的安全行为。  \n",
      "   - **Website:** Lil'Log\n",
      "\n",
      "6. **LLM Powered Autonomous Agents | 由大型语言模型驱动的自主代理**  \n",
      "   - **Description (EN):** Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer, and BabyAGI, serve as inspiring examples.  \n",
      "   - **Description (CN):** 以大型语言模型（LLM）为核心控制器构建代理是一个很酷的概念。几个概念验证演示，如AutoGPT、GPT-Engineer和BabyAGI，都是鼓舞人心的例子。  \n",
      "   - **Website:** Lil'Log\n",
      "\n",
      "7. **Prompt Engineering | 提示工程**  \n",
      "   - **Description (EN):** Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights.  \n",
      "   - **Description (CN):** 提示工程，也称为上下文提示，是指如何与大型语言模型沟通以引导其行为以达到预期结果的方法，而不更新模型权重。  \n",
      "   - **Website:** Lil'Log\n",
      "\n",
      "8. **The Transformer Family Version 2.0 | 变压器家族版本2.0**  \n",
      "   - **Description (EN):** Many new Transformer architecture improvements have been proposed since my last post on “The Transformer Family” about three years ago. Here I did a big refactoring and enrichment of that 2020 post.  \n",
      "   - **Description (CN):** 自我上次关于“变压器家族”的文章发表以来，已经提出了许多新的变压器架构改进。在这里，我对2020年的那篇文章进行了大规模重构和丰富。  \n",
      "   - **Website:** Lil'Log\n",
      "\n",
      "9. **Large Transformer Model Inference Optimization | 大型变压器模型推理优化**  \n",
      "   - **Description (EN):** Large transformer models are mainstream nowadays, creating SoTA results for a variety of tasks. They are powerful but very expensive to train and use.  \n",
      "   - **Description (CN):** 大型变压器模型如今已成为主流，为各种任务创造了最先进的结果。它们功能强大，但训练和使用成本非常高。  \n",
      "   - **Website:** Lil'Log\n",
      "\n",
      "10. **Some Math behind Neural Tangent Kernel | 神经切线核背后的数学**  \n",
      "    - **Description (EN):** Neural tangent kernel (NTK) (Jacot et al. 2018) is a kernel to explain the evolution of neural networks during training via gradient descent.  \n",
      "    - **Description (CN):** 神经切线核（NTK）（Jacot等人，2018年）是一种内核，用于解释通过梯度下降训练期间神经网络的演变。  \n",
      "    - **Website:** Lil'Log\n",
      "\n",
      "These topics cover a range of recent advancements and discussions in the field of artificial intelligence and machine learning, particularly focusing on large language models, reinforcement learning, and transformer architectures.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import datetime  # Import datetime module\n",
    "import dashscope\n",
    "\n",
    "# Function to get raw HTML content from the websites\n",
    "def get_raw_html(domain):\n",
    "    try:\n",
    "        # Send request to the website and get the raw HTML content\n",
    "        response = requests.get(f'http://{domain}')\n",
    "        if response.status_code == 200:\n",
    "            return response.content\n",
    "        else:\n",
    "            return f\"Failed to retrieve content from {domain}. HTTP Status Code: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error while fetching content from {domain}: {str(e)}\"\n",
    "\n",
    "# Function to prepare the message for Qwen LLM analysis\n",
    "def analyze_with_qwen(domain, raw_html):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'You are a web researcher. Analyze the raw HTML content and extract key topics in the following format: \"1. Title | Description | Website\"'},\n",
    "        {'role': 'user', 'content': f'''\n",
    "        Analyze the raw HTML content from {domain} and provide the latest 10 topics with:\n",
    "        1. Article titles in English\n",
    "        2. Article titles in Chinese\n",
    "        3. One-line descriptions in English\n",
    "        4. One-line descriptions in Chinese\n",
    "        5. Website name\n",
    "        Use current date: {datetime.date.today()}.\n",
    "        HTML Content: {raw_html.decode('utf-8')}\n",
    "        '''}\n",
    "    ]\n",
    "\n",
    "    response = dashscope.Generation.call(\n",
    "        api_key=\"sk-1a28c3fcc7e044cbacd6faf47dc89755\",\n",
    "        model=\"qwen-max\",\n",
    "        messages=messages,\n",
    "        enable_search=True,\n",
    "        result_format='message'\n",
    "    )\n",
    "    return response['output']['choices'][0]['message']['content']\n",
    "\n",
    "# List of websites to scrape\n",
    "websites = [\n",
    "    \"lilianweng.github.io\",\n",
    "]\n",
    "\n",
    "# Scraping and analyzing websites\n",
    "for site in websites:\n",
    "    print(f\"Scraping raw HTML from {site}...\")\n",
    "    raw_html = get_raw_html(site)\n",
    "    \n",
    "    # Check if there was an error\n",
    "    if isinstance(raw_html, str) and ('Error' in raw_html or 'Failed' in raw_html):\n",
    "        print(raw_html)\n",
    "    else:\n",
    "        print(f\"Raw HTML retrieved from {site}. Analyzing with Qwen LLM...\\n\")\n",
    "        qwen_analysis = analyze_with_qwen(site, raw_html)\n",
    "        print(f\"Analysis from {site}:\\n{qwen_analysis}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "import datetime\n",
    "import dashscope\n",
    "\n",
    "# Function to get raw HTML content from the websites\n",
    "def get_raw_html(domain):\n",
    "    try:\n",
    "        # Send request to the website and get the raw HTML content\n",
    "        response = requests.get(f'http://{domain}')\n",
    "        if response.status_code == 200:\n",
    "            return response.content\n",
    "        else:\n",
    "            return f\"Failed to retrieve content from {domain}. HTTP Status Code: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error while fetching content from {domain}: {str(e)}\"\n",
    "\n",
    "# Function to prepare the message for Qwen LLM analysis\n",
    "def analyze_with_qwen(domain, raw_html):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'You are a web researcher. Analyze the raw HTML content and extract key topics in the following format: \"1. Title | Description | Website\"'},\n",
    "        {'role': 'user', 'content': f'''\n",
    "        Analyze the raw HTML content from {domain} and provide the latest 10 topics with:\n",
    "        1. Article titles in English\n",
    "        2. Article titles in Chinese\n",
    "        3. One-line descriptions in English\n",
    "        4. One-line descriptions in Chinese\n",
    "        5. Website name\n",
    "        Use current date: {datetime.date.today()}.\n",
    "        HTML Content: {raw_html.decode('utf-8')}\n",
    "        '''}\n",
    "    ]\n",
    "\n",
    "    response = dashscope.Generation.call(\n",
    "        api_key=\"sk-1a28c3fcc7e044cbacd6faf47dc89755\",\n",
    "        model=\"qwen-max\",\n",
    "        messages=messages,\n",
    "        enable_search=True,\n",
    "        result_format='message'\n",
    "    )\n",
    "    return response['output']['choices'][0]['message']['content']\n",
    "\n",
    "# Streamlit UI components\n",
    "st.title(\"Minerva Agent\")   \n",
    "\n",
    "# List of default websites\n",
    "default_websites = [\n",
    "    \"lilianweng.github.io\",\n",
    "    \"qbitai.com\",\n",
    "    \"jiqizhixin.com\",\n",
    "    \"x.com/deepseek_ai\"\n",
    "]\n",
    "\n",
    "# Input for user to add websites\n",
    "input_websites = st.text_area(\"Website Domains(, Seperated)\", \n",
    "                              value=', '.join(default_websites), \n",
    "                              height=100)\n",
    "\n",
    "# Convert input string to a list of websites\n",
    "websites = [site.strip() for site in input_websites.split(',')]\n",
    "\n",
    "# Display results\n",
    "for site in websites:\n",
    "    st.write(f\"### Pulling {site}...\")\n",
    "    \n",
    "    # Get raw HTML\n",
    "    raw_html = get_raw_html(site)\n",
    "    \n",
    "    # Check if there was an error\n",
    "    if isinstance(raw_html, str) and ('Error' in raw_html or 'Failed' in raw_html):\n",
    "        st.error(raw_html)\n",
    "    else:\n",
    "        st.write(f\"Raw HTML retrieved from {site}. Analyzing with Qwen LLM...\\n\")\n",
    "        \n",
    "        # Perform Qwen analysis\n",
    "        qwen_analysis = analyze_with_qwen(site, raw_html)\n",
    "        \n",
    "        # Display results\n",
    "        st.write(f\"### {site} Summary:\\n\")\n",
    "        st.text_area(f\" {site}\", qwen_analysis, height=300)\n",
    "\n",
    "    st.markdown(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ntscraper\n",
      "  Using cached ntscraper-0.3.17-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: requests>=2.28 in ./.venv/lib/python3.12/site-packages (from ntscraper) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11 in ./.venv/lib/python3.12/site-packages (from ntscraper) (4.13.3)\n",
      "Requirement already satisfied: lxml>=4.9 in ./.venv/lib/python3.12/site-packages (from ntscraper) (5.3.1)\n",
      "Requirement already satisfied: tqdm>=4.66 in ./.venv/lib/python3.12/site-packages (from ntscraper) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.12/site-packages (from beautifulsoup4>=4.11->ntscraper) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./.venv/lib/python3.12/site-packages (from beautifulsoup4>=4.11->ntscraper) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.28->ntscraper) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.28->ntscraper) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.28->ntscraper) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.28->ntscraper) (2025.1.31)\n",
      "Using cached ntscraper-0.3.17-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: ntscraper\n",
      "Successfully installed ntscraper\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ntscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing instances: 100%|██████████| 8/8 [00:14<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-Feb-25 15:27:38 - Empty page on https://lightbrd.com\n"
     ]
    }
   ],
   "source": [
    "from ntscraper import Nitter\n",
    "scraper = Nitter(log_level=1, skip_instance_check=False)\n",
    "sama_tweets = scraper.get_tweets(\"sama\", mode='user', instance = 'https://lightbrd.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing instances: 100%|██████████| 8/8 [00:22<00:00,  2.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping tweets from @sama\n",
      "24-Feb-25 15:23:04 - Empty page on https://nitter.privacydev.net\n",
      "Error or unexpected response for @sama: {'tweets': [], 'threads': []}\n",
      "Scraping tweets from @ylecun\n",
      "24-Feb-25 15:23:08 - Empty page on https://nitter.privacydev.net\n",
      "Error or unexpected response for @ylecun: {'tweets': [], 'threads': []}\n",
      "Scraping tweets from @AndrewYNg\n",
      "24-Feb-25 15:23:12 - Empty page on https://nitter.privacydev.net\n",
      "Error or unexpected response for @AndrewYNg: {'tweets': [], 'threads': []}\n",
      "Scraping tweets from @fchollet\n",
      "24-Feb-25 15:23:16 - Empty page on https://nitter.privacydev.net\n",
      "Error or unexpected response for @fchollet: {'tweets': [], 'threads': []}\n",
      "Scraping tweets from @_KarenHao\n",
      "24-Feb-25 15:23:19 - Empty page on https://nitter.privacydev.net\n",
      "Error or unexpected response for @_KarenHao: {'tweets': [], 'threads': []}\n",
      "Scraping tweets from @karpathy\n",
      "24-Feb-25 15:23:23 - Empty page on https://nitter.privacydev.net\n",
      "Error or unexpected response for @karpathy: {'tweets': [], 'threads': []}\n",
      "Scraping tweets from @SchmidhuberAI\n",
      "24-Feb-25 15:23:27 - Empty page on https://nitter.privacydev.net\n",
      "Error or unexpected response for @SchmidhuberAI: {'tweets': [], 'threads': []}\n",
      "Scraping tweets from @sarahookr\n",
      "24-Feb-25 15:23:31 - Empty page on https://nitter.privacydev.net\n",
      "Error or unexpected response for @sarahookr: {'tweets': [], 'threads': []}\n",
      "Scraping tweets from @demishassabis\n",
      "24-Feb-25 15:23:34 - Empty page on https://nitter.privacydev.net\n",
      "Error or unexpected response for @demishassabis: {'tweets': [], 'threads': []}\n",
      "Scraping tweets from @saranormous\n",
      "24-Feb-25 15:23:38 - Empty page on https://nitter.privacydev.net\n",
      "Error or unexpected response for @saranormous: {'tweets': [], 'threads': []}\n",
      "Scraping tweets from @hardmaru\n",
      "24-Feb-25 15:23:41 - Empty page on https://nitter.privacydev.net\n",
      "Error or unexpected response for @hardmaru: {'tweets': [], 'threads': []}\n",
      "Scraping tweets from @lilianweng\n",
      "24-Feb-25 15:23:45 - Empty page on https://nitter.privacydev.net\n",
      "Error or unexpected response for @lilianweng: {'tweets': [], 'threads': []}\n",
      "Scraping tweets from @OriolVinyalsML\n",
      "24-Feb-25 15:23:49 - Empty page on https://nitter.privacydev.net\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping tweets from @\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     tweets \u001b[38;5;241m=\u001b[39m \u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msince\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msince_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muntil\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muntil_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://nitter.privacydev.net\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# Check if the tweets are returned correctly\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tweets, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/ntscraper/nitter.py:932\u001b[0m, in \u001b[0;36mNitter.get_tweets\u001b[0;34m(self, terms, mode, number, since, until, near, language, to, replies, filters, exclude, max_retries, instance)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(terms) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    930\u001b[0m     term \u001b[38;5;241m=\u001b[39m terms\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m--> 932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43mterm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m        \u001b[49m\u001b[43msince\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43muntil\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnear\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(terms) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    948\u001b[0m     term \u001b[38;5;241m=\u001b[39m terms[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/ntscraper/nitter.py:780\u001b[0m, in \u001b[0;36mNitter._search\u001b[0;34m(self, term, mode, number, since, until, near, language, to, replies, filters, exclude, max_retries, instance)\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    778\u001b[0m         endpoint \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?scroll=false\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 780\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m soup \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tweets\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/ntscraper/nitter.py:296\u001b[0m, in \u001b[0;36mNitter._get_page\u001b[0;34m(self, endpoint, max_retries)\u001b[0m\n\u001b[1;32m    290\u001b[0m                         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_session(\n\u001b[1;32m    291\u001b[0m                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_new_instance(\n\u001b[1;32m    292\u001b[0m                                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError fetching \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m                             )\n\u001b[1;32m    294\u001b[0m                         )\n\u001b[1;32m    295\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 296\u001b[0m     \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_count \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_retries:\n\u001b[1;32m    299\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax retries reached. Check your request and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ntscraper import Nitter\n",
    "import datetime\n",
    "import pprint\n",
    "\n",
    "# Initialize the scraper\n",
    "scraper = Nitter(log_level=1, skip_instance_check=False)\n",
    "\n",
    "# Define the accounts and initialize the current date\n",
    "accounts = [\n",
    "    \"sama\", \"ylecun\", \"AndrewYNg\", \"fchollet\", \"_KarenHao\", \"karpathy\", \"SchmidhuberAI\", \"sarahookr\", \n",
    "    \"demishassabis\", \"saranormous\", \"hardmaru\", \"lilianweng\", \"OriolVinyalsML\", \"Michael_J_Black\", \n",
    "    \"JeffDean\", \"goodfellow_ian\", \"achowdhery\", \"PeterDiamandis\", \"GaryMarcus\", \"giffmana\", \n",
    "    \"rasbt\", \"quaesita\", \"KateKayeReports\", \"EMostaque\", \"drfeifei\", \"DrJimFan\", \"omarsar0\", \n",
    "    \"conniechan\", \"hugo_larochelle\", \"benjedwards\", \"rebecca_szkutak\", \"svlevine\", \"ericschmidt\", \n",
    "    \"ilyasut\", \"patrickmineault\", \"natashajaques\", \"pabbeel\", \"ESYudkowsky\", \"geoffreyhinton\", \n",
    "    \"wintonARK\", \"jeffclune\", \"RamaswmySridhar\", \"bentossell\", \"johnschulman2\", \"_akhaliq\", \n",
    "    \"quocleix\", \"jackclarkSF\", \"mervenoyann\", \"DavidSHolz\", \"natolambert\", \"RichardSocher\", \n",
    "    \"mustafasuleymn\", \"ZoubinGhahrama1\", \"nathanbenaich\", \"johnvmcdonnell\", \"tunguz\", \"bengoertzel\", \n",
    "    \"ch402\", \"Kseniase_\", \"paulg\", \"rsalakhu\", \"gdb\", \"vivnat\", \"bxchen\", \"AnimaAnandkumar\", \n",
    "    \"JeffreyTowson\", \"Thom_Wolf\", \"johnplattml\", \"SamanyouGarg\", \"KirkDBorne\", \"Alber_RomGar\", \n",
    "    \"SilverJacket\", \"ecsquendor\", \"jordnb\", \"jluan\", \"NPCollapse\", \"NaveenGRao\", \"azeem\", \"Suhail\", \n",
    "    \"maxjaderberg\", \"Kyle_L_Wiggers\", \"cocoweixu\", \"aidangomezzz\", \"alexandr_wang\", \"CaimingXiong\", \n",
    "    \"YiMaTweets\", \"notmisha\", \"peteratmsr\", \"shivon\", \"jackyliang42\", \"v_vashishta\", \"xdh\", \n",
    "    \"FryRsquared\", \"ravi_lsvp\", \"ClementDelangue\", \"oh_that_hat\", \"sapna\", \"VRLalchand\", \"svpino\", \n",
    "    \"ceobillionaire\", \"ykilcher\", \"BornsteinMatt\", \"lachygroom\", \"goodside\", \"amasad\", \"polynoamial\", \n",
    "    \"sytelus\"\n",
    "]\n",
    "\n",
    "# Get today's date and calculate 5 days ago\n",
    "today = datetime.date.today()\n",
    "five_days_ago = today - datetime.timedelta(days=5)\n",
    "\n",
    "# Format the dates to match the scraping requirement\n",
    "since_date = five_days_ago.strftime('%Y-%m-%d')\n",
    "until_date = today.strftime('%Y-%m-%d')\n",
    "\n",
    "# Scrape tweets from each account for the last 5 days\n",
    "tweets_data = {}\n",
    "for account in accounts:\n",
    "    print(f\"Scraping tweets from @{account}\")\n",
    "    try:\n",
    "        tweets = scraper.get_tweets(account, mode='user', since=since_date, until=until_date, instance='https://nitter.privacydev.net')\n",
    "        \n",
    "        # Check if the tweets are returned correctly\n",
    "        if isinstance(tweets, dict):\n",
    "            print(f\"Error or unexpected response for @{account}: {tweets}\")\n",
    "            continue  # Skip this account if the response is not as expected\n",
    "        \n",
    "        # Store the relevant tweet data (including likes, retweets, and links)\n",
    "        tweets_data[account] = []\n",
    "        for tweet in tweets:\n",
    "            tweet_data = {\n",
    "                'tweet': tweet['text'],\n",
    "                'likes': tweet['stats']['likes'],\n",
    "                'retweets': tweet['stats']['retweets'],\n",
    "                'link': tweet['link']\n",
    "            }\n",
    "            tweets_data[account].append(tweet_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping @{account}: {e}\")\n",
    "\n",
    "# Print or store the results as needed\n",
    "# For example, display the data for the first account:\n",
    "pprint.pprint(tweets_data[\"sama\"])  # Print tweets for Sam Altman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 微信cookie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rewardsn=; wxtokenkey=777; pac_uid=0_WFRCchx025Cww; _qimei_uuid42=191090f32121004a40ded1e5e650d9677d9210f8fb; _qimei_h38=e963446740ded1e5e650d96703000003119109; _qimei_q36=; ua_id=fIxXt1qo3N1AkUI9AAAAAE7bKLDM8dls2W8RivxiLs4=; wxuin=36409384414630; suid=user_0_WFRCchx025Cww; mm_lang=zh_CN; sig=h016ff31a4a1ff5262376ab723fd8d807ea82f9552e933b7d087ca0bd6cd2ce703cdaaf9f90ae8c1544; ab.storage.deviceId.f9c2b69f-2136-44e0-a55a-dff72d99aa19=g%3AYoJZqng6gcdvly5aBDxZqgiJ1GZ2%7Ce%3Aundefined%7Cc%3A1739462526631%7Cl%3A1739462526631; ab.storage.sessionId.f9c2b69f-2136-44e0-a55a-dff72d99aa19=g%3A7bafc696-6715-6e6b-565d-5695541d32ca%7Ce%3A1739464326655%7Cc%3A1739462526656%7Cl%3A1739462526656; _qimei_fingerprint=d91ba6f60a0e30a68c3644052fa00145; uuid=ca13ea4de5017925ec58ce75b297c0d9; _clck=1mr2pfh|1|ftp|0; rand_info=CAESIE2845Ww7udUcmhfjajRvqB5ZjrkLpk+WYgiwUr3mbiB; slave_bizuin=3918520287; data_bizuin=3918520287; bizuin=3918520287; data_ticket=AA14hMmWJuNmOqpwkhsBDAw/VS3bQDqPl6lnS+ECjS5dDeaQzIfZThM2cDGRWVcj; slave_sid=M2Y0MzNLYVBUTFNyZjc4VUgwR0VNWFVVMHJLblMwNFBnR3JIZ2FMMGZKUmxzTkZKX3Q3ak5kU2VRWE1oUzlRMXBCY1NMMGdjOHR0bGFFQjNKTkl0MlVDeWpfTFhESVI1UXhzMHZjYXk4WTJjRFpVcVVkdlA1R1NHVTVseWlkOUNYdW8zc242VTlIRDY4M1pZ; slave_user=gh_b89c7dbe2d0d; xid=3abe7a2c901b43c1ab83b44e9b8e6b74; _clsk=1k5eby|1740383197389|10|1|mp.weixin.qq.com/weheat-agent/payload/record\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "token: 1519694182\n",
    "fakeid: Mzg5Mjg2OTM2Nw=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目标url\n",
    "url = \"https://mp.weixin.qq.com/cgi-bin/appmsg\"\n",
    "cookie = \"rewardsn=; wxtokenkey=777; pac_uid=0_WFRCchx025Cww; _qimei_uuid42=191090f32121004a40ded1e5e650d9677d9210f8fb; _qimei_h38=e963446740ded1e5e650d96703000003119109; _qimei_q36=; ua_id=fIxXt1qo3N1AkUI9AAAAAE7bKLDM8dls2W8RivxiLs4=; wxuin=36409384414630; suid=user_0_WFRCchx025Cww; mm_lang=zh_CN; sig=h016ff31a4a1ff5262376ab723fd8d807ea82f9552e933b7d087ca0bd6cd2ce703cdaaf9f90ae8c1544; ab.storage.deviceId.f9c2b69f-2136-44e0-a55a-dff72d99aa19=g%3AYoJZqng6gcdvly5aBDxZqgiJ1GZ2%7Ce%3Aundefined%7Cc%3A1739462526631%7Cl%3A1739462526631; ab.storage.sessionId.f9c2b69f-2136-44e0-a55a-dff72d99aa19=g%3A7bafc696-6715-6e6b-565d-5695541d32ca%7Ce%3A1739464326655%7Cc%3A1739462526656%7Cl%3A1739462526656; _qimei_fingerprint=d91ba6f60a0e30a68c3644052fa00145; uuid=ca13ea4de5017925ec58ce75b297c0d9; _clck=1mr2pfh|1|ftp|0; rand_info=CAESIE2845Ww7udUcmhfjajRvqB5ZjrkLpk+WYgiwUr3mbiB; slave_bizuin=3918520287; data_bizuin=3918520287; bizuin=3918520287; data_ticket=AA14hMmWJuNmOqpwkhsBDAw/VS3bQDqPl6lnS+ECjS5dDeaQzIfZThM2cDGRWVcj; slave_sid=M2Y0MzNLYVBUTFNyZjc4VUgwR0VNWFVVMHJLblMwNFBnR3JIZ2FMMGZKUmxzTkZKX3Q3ak5kU2VRWE1oUzlRMXBCY1NMMGdjOHR0bGFFQjNKTkl0MlVDeWpfTFhESVI1UXhzMHZjYXk4WTJjRFpVcVVkdlA1R1NHVTVseWlkOUNYdW8zc242VTlIRDY4M1pZ; slave_user=gh_b89c7dbe2d0d; xid=3abe7a2c901b43c1ab83b44e9b8e6b74; _clsk=1k5eby|1740383197389|10|1|mp.weixin.qq.com/weheat-agent/payload/record\"\n",
    "\n",
    "headers = {\n",
    "    \"Cookie\": cookie,\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.75 Mobile Safari/537.36\",\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"token\": \"1519694182\",\n",
    "    \"lang\": \"zh_CN\",\n",
    "    \"f\": \"json\",\n",
    "    \"ajax\": \"1\",\n",
    "    \"action\": \"list_ex\",\n",
    "    \"begin\": \"0\",\n",
    "    \"count\": \"5\",\n",
    "    \"query\": \"\",\n",
    "    \"fakeid\": \"Mzg5Mjg2OTM2Nw==\", # 自己的号，设置为空\n",
    "    \"type\": \"9\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_count():\n",
    "    content_json = requests.get(url, headers=headers, params=data).json()\n",
    "    count = int(content_json[\"app_msg_cnt\"])\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_list(count, per_page=5):\n",
    "    page = int(math.ceil(count / per_page))\n",
    "    content_list = []\n",
    "    for i in tqdm(range(page), desc=\"获取文章列表\"):\n",
    "        data[\"begin\"] = i * per_page\n",
    "        content_json = requests.get(url, headers=headers, params=data).json()\n",
    "        content_list.extend(content_json[\"app_msg_list\"])\n",
    "        time.sleep(random.randint(5, 10))\n",
    "        # 保存成json\n",
    "        with open(\"content_list.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(content_list, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def precess_content_list():\n",
    "    content_list = json.load(open(\"content_list.json\", \"r\", encoding=\"utf-8\"))\n",
    "    results_list = []\n",
    "    for item in content_list:\n",
    "        title = item[\"title\"]\n",
    "        link = item[\"link\"]\n",
    "        create_time = time.strftime(\"%Y-%m-%d %H:%M\", time.localtime(item[\"create_time\"]))\n",
    "        results_list.append([title, link, create_time])\n",
    "    name = ['title', 'link', 'create_time']\n",
    "    data = pd.DataFrame(columns=name, data=results_list)\n",
    "    data.to_csv(\"data.csv\", mode='w', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "content_list = []\n",
    "for i in range(20):\n",
    "    data[\"begin\"] = i*5\n",
    "    time.sleep(5)\n",
    "    # 使用get方法进行提交\n",
    "    content_json = requests.get(url, headers=headers, params=data).json()\n",
    "    # 返回了一个json，里面是每一页的数据\n",
    "    for item in content_json[\"app_msg_list\"]:    \n",
    "    # 提取每页文章的标题及对应的url\n",
    "        items = []\n",
    "        items.append(item[\"title\"])\n",
    "        items.append(item[\"link\"])\n",
    "        t = time.localtime(item[\"create_time\"])\n",
    "        items.append(time.strftime(\"%Y-%m-%d %H:%M:%S\", t))\n",
    "        content_list.append(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ['title', 'link', 'create_time']\n",
    "test = pd.DataFrame(columns=name, data=content_list)\n",
    "test.to_csv(\"url.csv\", mode='a', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "content_json = requests.get(url, headers=headers, params=data).json()\n",
    "count = int(content_json[\"app_msg_cnt\"])\n",
    "print(count)\n",
    "page = int(math.ceil(count / 5))\n",
    "print(page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "if (i > 0) and (i % 10 == 0):\n",
    "        name = ['title', 'link', 'create_time']\n",
    "        test = pd.DataFrame(columns=name, data=content_list)\n",
    "        test.to_csv(\"url.csv\", mode='a', encoding='utf-8')\n",
    "        print(\"第\" + str(i) + \"次保存成功\")\n",
    "        content_list = []\n",
    "        time.sleep(random.randint(60,90))\n",
    "else:\n",
    "    time.sleep(random.randint(15,25))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "auth_token= 9369750b4dc49ef13315d9052b533d796406b325"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "TWITTER_AUTH_TOKEN = os.getenv(\"9369750b4dc49ef13315d9052b533d796406b325\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qwen-agent[code_interpreter,gui,python_executor,rag] in ./.venv/lib/python3.12/site-packages (0.0.15)\n",
      "Requirement already satisfied: dashscope>=1.11.0 in ./.venv/lib/python3.12/site-packages (from qwen-agent[code_interpreter,gui,python_executor,rag]) (1.22.1)\n",
      "Requirement already satisfied: eval-type-backport in ./.venv/lib/python3.12/site-packages (from qwen-agent[code_interpreter,gui,python_executor,rag]) (0.2.2)\n",
      "Requirement already satisfied: json5 in ./.venv/lib/python3.12/site-packages (from qwen-agent[code_interpreter,gui,python_executor,rag]) (0.10.0)\n",
      "Requirement already satisfied: jsonlines in ./.venv/lib/python3.12/site-packages (from qwen-agent[code_interpreter,gui,python_executor,rag]) (4.0.0)\n",
      "Requirement already satisfied: jsonschema in ./.venv/lib/python3.12/site-packages (from qwen-agent[code_interpreter,gui,python_executor,rag]) (4.23.0)\n",
      "Requirement already satisfied: openai in ./.venv/lib/python3.12/site-packages (from qwen-agent[code_interpreter,gui,python_executor,rag]) (1.63.2)\n",
      "Requirement already satisfied: pydantic>=2.3.0 in ./.venv/lib/python3.12/site-packages (from qwen-agent[code_interpreter,gui,python_executor,rag]) (2.9.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from qwen-agent[code_interpreter,gui,python_executor,rag]) (2.32.3)\n",
      "Requirement already satisfied: tiktoken in ./.venv/lib/python3.12/site-packages (from qwen-agent[code_interpreter,gui,python_executor,rag]) (0.9.0)\n",
      "Requirement already satisfied: anyio>=3.7.1 in ./.venv/lib/python3.12/site-packages (from qwen-agent[code_interpreter,gui,python_executor,rag]) (4.8.0)\n",
      "Requirement already satisfied: fastapi>=0.103.1 in ./.venv/lib/python3.12/site-packages (from qwen-agent[code_interpreter,gui,python_executor,rag]) (0.115.8)\n",
      "Collecting jupyter>=1.0.0 (from qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting matplotlib (from qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached matplotlib-3.10.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from qwen-agent[code_interpreter,gui,python_executor,rag]) (2.2.3)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from qwen-agent[code_interpreter,gui,python_executor,rag]) (2.2.3)\n",
      "Requirement already satisfied: pillow in ./.venv/lib/python3.12/site-packages (from qwen-agent[code_interpreter,gui,python_executor,rag]) (10.4.0)\n",
      "Collecting seaborn (from qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting sympy (from qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: uvicorn>=0.23.2 in ./.venv/lib/python3.12/site-packages (from qwen-agent[code_interpreter,gui,python_executor,rag]) (0.34.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./.venv/lib/python3.12/site-packages (from qwen-agent[code_interpreter,gui,python_executor,rag]) (2.23.4)\n",
      "Requirement already satisfied: gradio>=5.0.0 in ./.venv/lib/python3.12/site-packages (from qwen-agent[code_interpreter,gui,python_executor,rag]) (5.1.0)\n",
      "Requirement already satisfied: gradio-client==1.4.0 in ./.venv/lib/python3.12/site-packages (from qwen-agent[code_interpreter,gui,python_executor,rag]) (1.4.0)\n",
      "Requirement already satisfied: modelscope-studio==1.0.0-beta.8 in ./.venv/lib/python3.12/site-packages (from qwen-agent[code_interpreter,gui,python_executor,rag]) (1.0.0b8)\n",
      "Collecting pebble (from qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached Pebble-5.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting multiprocess (from qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached multiprocess-0.70.17-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting timeout-decorator (from qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached timeout_decorator-0.5.0-py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil in ./.venv/lib/python3.12/site-packages (from qwen-agent[code_interpreter,gui,python_executor,rag]) (2.9.0.post0)\n",
      "Collecting scipy (from qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached scipy-1.15.2-cp312-cp312-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: charset-normalizer in ./.venv/lib/python3.12/site-packages (from qwen-agent[code_interpreter,gui,python_executor,rag]) (3.4.1)\n",
      "Collecting rank-bm25 (from qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting jieba (from qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached jieba-0.42.1-py3-none-any.whl\n",
      "Collecting snowballstemmer (from qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached snowballstemmer-2.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.12/site-packages (from qwen-agent[code_interpreter,gui,python_executor,rag]) (4.13.3)\n",
      "Collecting pdfminer.six (from qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting pdfplumber (from qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting python-docx (from qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting python-pptx (from qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting tabulate (from qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from gradio-client==1.4.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (2025.2.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in ./.venv/lib/python3.12/site-packages (from gradio-client==1.4.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in ./.venv/lib/python3.12/site-packages (from gradio-client==1.4.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (0.29.1)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from gradio-client==1.4.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (24.2)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in ./.venv/lib/python3.12/site-packages (from gradio-client==1.4.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (4.12.2)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in ./.venv/lib/python3.12/site-packages (from gradio-client==1.4.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (12.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic>=2.3.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (0.7.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.12/site-packages (from anyio>=3.7.1->qwen-agent[code_interpreter,gui,python_executor,rag]) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.12/site-packages (from anyio>=3.7.1->qwen-agent[code_interpreter,gui,python_executor,rag]) (1.3.1)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.12/site-packages (from dashscope>=1.11.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (3.11.12)\n",
      "Requirement already satisfied: websocket-client in ./.venv/lib/python3.12/site-packages (from dashscope>=1.11.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (1.8.0)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in ./.venv/lib/python3.12/site-packages (from fastapi>=0.103.1->qwen-agent[code_interpreter,gui,python_executor,rag]) (0.45.3)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in ./.venv/lib/python3.12/site-packages (from gradio>=5.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (23.2.1)\n",
      "Requirement already satisfied: ffmpy in ./.venv/lib/python3.12/site-packages (from gradio>=5.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (0.5.0)\n",
      "Requirement already satisfied: jinja2<4.0 in ./.venv/lib/python3.12/site-packages (from gradio>=5.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (3.1.5)\n",
      "Requirement already satisfied: markupsafe~=2.0 in ./.venv/lib/python3.12/site-packages (from gradio>=5.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (2.1.5)\n",
      "Requirement already satisfied: orjson~=3.0 in ./.venv/lib/python3.12/site-packages (from gradio>=5.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (3.10.15)\n",
      "Requirement already satisfied: pydub in ./.venv/lib/python3.12/site-packages (from gradio>=5.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in ./.venv/lib/python3.12/site-packages (from gradio>=5.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in ./.venv/lib/python3.12/site-packages (from gradio>=5.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.2.2 in ./.venv/lib/python3.12/site-packages (from gradio>=5.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (0.9.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in ./.venv/lib/python3.12/site-packages (from gradio>=5.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in ./.venv/lib/python3.12/site-packages (from gradio>=5.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in ./.venv/lib/python3.12/site-packages (from gradio>=5.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (0.15.1)\n",
      "Collecting notebook (from jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached notebook-7.3.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jupyter-console (from jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached jupyter_console-6.6.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting nbconvert (from jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached nbconvert-7.16.6-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: ipykernel in ./.venv/lib/python3.12/site-packages (from jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (6.29.5)\n",
      "Collecting ipywidgets (from jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting jupyterlab (from jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached jupyterlab-4.3.5-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->qwen-agent[code_interpreter,gui,python_executor,rag]) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->qwen-agent[code_interpreter,gui,python_executor,rag]) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil->qwen-agent[code_interpreter,gui,python_executor,rag]) (1.17.0)\n",
      "Requirement already satisfied: click>=7.0 in ./.venv/lib/python3.12/site-packages (from uvicorn>=0.23.2->qwen-agent[code_interpreter,gui,python_executor,rag]) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in ./.venv/lib/python3.12/site-packages (from uvicorn>=0.23.2->qwen-agent[code_interpreter,gui,python_executor,rag]) (0.14.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.12/site-packages (from beautifulsoup4->qwen-agent[code_interpreter,gui,python_executor,rag]) (2.6)\n",
      "Requirement already satisfied: attrs>=19.2.0 in ./.venv/lib/python3.12/site-packages (from jsonlines->qwen-agent[code_interpreter,gui,python_executor,rag]) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.12/site-packages (from jsonschema->qwen-agent[code_interpreter,gui,python_executor,rag]) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.12/site-packages (from jsonschema->qwen-agent[code_interpreter,gui,python_executor,rag]) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.12/site-packages (from jsonschema->qwen-agent[code_interpreter,gui,python_executor,rag]) (0.22.3)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached contourpy-1.3.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached fonttools-4.56.0-cp312-cp312-macosx_10_13_universal2.whl.metadata (101 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting dill>=0.3.9 (from multiprocess->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.12/site-packages (from openai->qwen-agent[code_interpreter,gui,python_executor,rag]) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from openai->qwen-agent[code_interpreter,gui,python_executor,rag]) (0.8.2)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.12/site-packages (from openai->qwen-agent[code_interpreter,gui,python_executor,rag]) (4.67.1)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached cryptography-44.0.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
      "Collecting pdfminer.six (from qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached pypdfium2-4.30.1-py3-none-macosx_11_0_arm64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in ./.venv/lib/python3.12/site-packages (from python-docx->qwen-agent[code_interpreter,gui,python_executor,rag]) (5.3.1)\n",
      "Collecting XlsxWriter>=0.5.7 (from python-pptx->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached XlsxWriter-3.2.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->qwen-agent[code_interpreter,gui,python_executor,rag]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->qwen-agent[code_interpreter,gui,python_executor,rag]) (2025.1.31)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.12/site-packages (from tiktoken->qwen-agent[code_interpreter,gui,python_executor,rag]) (2024.11.6)\n",
      "Collecting cffi>=1.12 (from cryptography>=36.0.0->pdfminer.six->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached cffi-1.17.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx>=0.24.1->gradio-client==1.4.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (1.0.7)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->gradio-client==1.4.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (3.17.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio>=5.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio>=5.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (13.9.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.11.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.11.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.11.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.11.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.11.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.11.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (1.18.3)\n",
      "Requirement already satisfied: appnope in ./.venv/lib/python3.12/site-packages (from ipykernel->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (0.1.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in ./.venv/lib/python3.12/site-packages (from ipykernel->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./.venv/lib/python3.12/site-packages (from ipykernel->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (1.8.12)\n",
      "Requirement already satisfied: ipython>=7.23.1 in ./.venv/lib/python3.12/site-packages (from ipykernel->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (8.32.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in ./.venv/lib/python3.12/site-packages (from ipykernel->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./.venv/lib/python3.12/site-packages (from ipykernel->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./.venv/lib/python3.12/site-packages (from ipykernel->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in ./.venv/lib/python3.12/site-packages (from ipykernel->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (1.6.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from ipykernel->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in ./.venv/lib/python3.12/site-packages (from ipykernel->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (26.2.1)\n",
      "Requirement already satisfied: tornado>=6.1 in ./.venv/lib/python3.12/site-packages (from ipykernel->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (6.4.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in ./.venv/lib/python3.12/site-packages (from ipykernel->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in ./.venv/lib/python3.12/site-packages (from jupyter-console->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (3.0.50)\n",
      "Requirement already satisfied: pygments in ./.venv/lib/python3.12/site-packages (from jupyter-console->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (2.19.1)\n",
      "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached async_lru-2.0.4-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached jupyter_server-2.15.0-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting notebook-shim>=0.2 (from jupyterlab->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached notebook_shim-0.2.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting setuptools>=40.8.0 (from jupyterlab->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached setuptools-75.8.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting bleach!=5.0.0 (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached bleach-6.2.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting defusedxml (from nbconvert->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting jupyterlab-pygments (from nbconvert->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting mistune<4,>=2.0.3 (from nbconvert->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached mistune-3.1.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting nbclient>=0.5.0 (from nbconvert->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached nbclient-0.10.2-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting nbformat>=5.7 (from nbconvert->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting pandocfilters>=1.4.1 (from nbconvert->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting webencodings (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting tinycss2<1.5,>=1.1.0 (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached tinycss2-1.4.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (4.9.0)\n",
      "Requirement already satisfied: stack_data in ./.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (0.6.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./.venv/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (4.3.6)\n",
      "Collecting argon2-cffi>=21.1 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached argon2_cffi-23.1.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting prometheus-client>=0.9 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached prometheus_client-0.21.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting send2trash>=1.8.2 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached Send2Trash-1.8.3-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting terminado>=0.8.3 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached terminado-0.18.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting babel>=2.10 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting fastjsonschema>=2.15 (from nbformat>=5.7->nbconvert->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached fastjsonschema-2.21.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.12/site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (0.2.13)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=5.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (3.0.0)\n",
      "Collecting argon2-cffi-bindings (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached argon2_cffi_bindings-21.2.0-cp38-abi3-macosx_10_9_universal2.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (0.8.4)\n",
      "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached python_json_logger-3.2.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=5.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag]) (0.2.3)\n",
      "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting webcolors>=24.6.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached webcolors-24.11.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->qwen-agent[code_interpreter,gui,python_executor,rag])\n",
      "  Using cached types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
      "Using cached jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
      "Using cached matplotlib-3.10.0-cp312-cp312-macosx_11_0_arm64.whl (8.0 MB)\n",
      "Using cached multiprocess-0.70.17-py312-none-any.whl (147 kB)\n",
      "Using cached pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
      "Using cached pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "Using cached Pebble-5.1.0-py3-none-any.whl (36 kB)\n",
      "Using cached python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "Using cached python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
      "Using cached rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Using cached scipy-1.15.2-cp312-cp312-macosx_14_0_arm64.whl (22.4 MB)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
      "Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Using cached contourpy-1.3.1-cp312-cp312-macosx_11_0_arm64.whl (255 kB)\n",
      "Using cached cryptography-44.0.1-cp39-abi3-macosx_10_9_universal2.whl (6.6 MB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached dill-0.3.9-py3-none-any.whl (119 kB)\n",
      "Using cached fonttools-4.56.0-cp312-cp312-macosx_10_13_universal2.whl (2.7 MB)\n",
      "Using cached kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl (65 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Using cached pypdfium2-4.30.1-py3-none-macosx_11_0_arm64.whl (2.8 MB)\n",
      "Using cached XlsxWriter-3.2.2-py3-none-any.whl (165 kB)\n",
      "Using cached ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "Using cached jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Using cached jupyterlab-4.3.5-py3-none-any.whl (11.7 MB)\n",
      "Using cached nbconvert-7.16.6-py3-none-any.whl (258 kB)\n",
      "Using cached notebook-7.3.2-py3-none-any.whl (13.2 MB)\n",
      "Using cached async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
      "Using cached bleach-6.2.0-py3-none-any.whl (163 kB)\n",
      "Using cached cffi-1.17.1-cp312-cp312-macosx_11_0_arm64.whl (178 kB)\n",
      "Using cached jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
      "Using cached jupyter_server-2.15.0-py3-none-any.whl (385 kB)\n",
      "Using cached jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
      "Using cached jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "Using cached mistune-3.1.2-py3-none-any.whl (53 kB)\n",
      "Using cached nbclient-0.10.2-py3-none-any.whl (25 kB)\n",
      "Using cached nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "Using cached notebook_shim-0.2.4-py3-none-any.whl (13 kB)\n",
      "Using cached pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Using cached setuptools-75.8.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
      "Using cached argon2_cffi-23.1.0-py3-none-any.whl (15 kB)\n",
      "Using cached babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "Using cached fastjsonschema-2.21.1-py3-none-any.whl (23 kB)\n",
      "Using cached jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
      "Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
      "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Using cached prometheus_client-0.21.1-py3-none-any.whl (54 kB)\n",
      "Using cached Send2Trash-1.8.3-py3-none-any.whl (18 kB)\n",
      "Using cached terminado-0.18.1-py3-none-any.whl (14 kB)\n",
      "Using cached tinycss2-1.4.0-py3-none-any.whl (26 kB)\n",
      "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Using cached python_json_logger-3.2.1-py3-none-any.whl (14 kB)\n",
      "Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Using cached argon2_cffi_bindings-21.2.0-cp38-abi3-macosx_10_9_universal2.whl (53 kB)\n",
      "Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached webcolors-24.11.1-py3-none-any.whl (14 kB)\n",
      "Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Using cached uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Using cached arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "Using cached types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: webencodings, timeout-decorator, snowballstemmer, mpmath, jieba, fastjsonschema, XlsxWriter, widgetsnbextension, webcolors, uri-template, types-python-dateutil, tinycss2, terminado, tabulate, sympy, setuptools, send2trash, scipy, rfc3986-validator, rfc3339-validator, rank-bm25, python-json-logger, python-docx, pypdfium2, pyparsing, pycparser, prometheus-client, pebble, pandocfilters, overrides, mistune, kiwisolver, jupyterlab-widgets, jupyterlab-pygments, jsonpointer, fqdn, fonttools, dill, defusedxml, cycler, contourpy, bleach, babel, async-lru, python-pptx, multiprocess, matplotlib, jupyter-server-terminals, cffi, arrow, seaborn, isoduration, ipywidgets, cryptography, argon2-cffi-bindings, pdfminer.six, nbformat, jupyter-console, argon2-cffi, pdfplumber, nbclient, jupyter-events, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, jupyter\n",
      "Successfully installed XlsxWriter argon2-cffi argon2-cffi-bindings arrow async-lru babel bleach cffi contourpy cryptography cycler defusedxml dill fastjsonschema fonttools fqdn ipywidgets isoduration jieba jsonpointer jupyter jupyter-console jupyter-events jupyter-lsp jupyter-server jupyter-server-terminals jupyterlab jupyterlab-pygments jupyterlab-server jupyterlab-widgets kiwisolver matplotlib mistune mpmath multiprocess nbclient nbconvert nbformat notebook notebook-shim overrides pandocfilters pdfminer.six pdfplumber pebble prometheus-client pycparser pyparsing pypdfium2 python-docx python-json-logger python-pptx rank-bm25 rfc3339-validator rfc3986-validator scipy seaborn send2trash setuptools snowballstemmer sympy tabulate terminado timeout-decorator tinycss2 types-python-dateutil uri-template webcolors webencodings widgetsnbextension\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U \"qwen-agent[gui,rag,code_interpreter,python_executor]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 18:15:49,558 - simple_doc_parser.py - 382 - INFO - Start parsing /private/var/folders/m6/wxc9fp790875bb7z5jw_q5c80000gn/T/gradio/68c30befb0b39b4d25c667e79b168907f697238ca690f67b0fb1c396644690eb/Even - Teaser - Pre-A -绿洲.pdf...\n",
      "2025-02-24 18:15:49,823 - simple_doc_parser.py - 425 - INFO - Finished parsing /private/var/folders/m6/wxc9fp790875bb7z5jw_q5c80000gn/T/gradio/68c30befb0b39b4d25c667e79b168907f697238ca690f67b0fb1c396644690eb/Even - Teaser - Pre-A -绿洲.pdf. Time spent: 0.25983476638793945 seconds.\n",
      "2025-02-24 18:15:49,863 - doc_parser.py - 108 - INFO - Start chunking /private/var/folders/m6/wxc9fp790875bb7z5jw_q5c80000gn/T/gradio/68c30befb0b39b4d25c667e79b168907f697238ca690f67b0fb1c396644690eb/Even - Teaser - Pre-A -绿洲.pdf (Even - Teaser - Pre-A -绿洲.pdf)...\n",
      "2025-02-24 18:15:49,868 - doc_parser.py - 126 - INFO - Finished chunking /private/var/folders/m6/wxc9fp790875bb7z5jw_q5c80000gn/T/gradio/68c30befb0b39b4d25c667e79b168907f697238ca690f67b0fb1c396644690eb/Even - Teaser - Pre-A -绿洲.pdf (Even - Teaser - Pre-A -绿洲.pdf). Time spent: 0.0008149147033691406 seconds.\n",
      "2025-02-24 18:15:49,902 - base_search.py - 158 - INFO - [Get top] Remaining slots: 18970\n"
     ]
    }
   ],
   "source": [
    "from qwen_agent.agents import Assistant\n",
    "from qwen_agent.gui import WebUI\n",
    "\n",
    "# Correct API configuration\n",
    "llm_cfg = {\n",
    "    'model': 'qwen-max',\n",
    "    'model_server': 'dashscope',\n",
    "    'api_key': 'sk-1a28c3fcc7e044cbacd6faf47dc89755',  # Replace with your API key if not set in environment variable\n",
    "    'generate_cfg': {\n",
    "        'top_p': 0.8\n",
    "    }\n",
    "}\n",
    "\n",
    "def test():\n",
    "    # Using the correct llm_cfg\n",
    "    bot = Assistant(llm=llm_cfg)\n",
    "    messages = [{'role': 'user', 'content': [{'text': '介绍图一'}, {'file': 'https://arxiv.org/pdf/2501.12948'}]}]\n",
    "    for rsp in bot.run(messages):\n",
    "        print(rsp)\n",
    "\n",
    "\n",
    "def app_gui():\n",
    "    # Using the correct llm_cfg for the WebUI\n",
    "    bot = Assistant(llm=llm_cfg,\n",
    "                    name='Assistant',\n",
    "                    description='使用RAG检索并回答，支持文件类型：PDF/Word/PPT/TXT/HTML。')\n",
    "    chatbot_config = {\n",
    "        'prompt.suggestions': [\n",
    "            {\n",
    "                'text': '介绍图一'\n",
    "            },\n",
    "            {\n",
    "                'text': '第二章第一句话是什么？'\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "    WebUI(bot, chatbot_config=chatbot_config).run()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Uncomment for testing\n",
    "    # test()\n",
    "    app_gui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bot response:\n",
      "[{'content': 'Hello', 'role': 'assistant'}]\n",
      "bot response:\n",
      "[{'content': 'Hello! How can I', 'role': 'assistant'}]\n",
      "bot response:\n",
      "[{'content': 'Hello! How can I assist you today?', 'role': 'assistant'}]\n",
      "bot response:\n",
      "[ { 'content': 'Hello! How can I assist you today? If you have any specific',\n",
      "    'role': 'assistant'}]\n",
      "bot response:\n",
      "[ { 'content': 'Hello! How can I assist you today? If you have any specific '\n",
      "               'requests, such as generating an image',\n",
      "    'role': 'assistant'}]\n",
      "bot response:\n",
      "[ { 'content': 'Hello! How can I assist you today? If you have any specific '\n",
      "               'requests, such as generating an image or processing one, feel',\n",
      "    'role': 'assistant'}]\n",
      "bot response:\n",
      "[ { 'content': 'Hello! How can I assist you today? If you have any specific '\n",
      "               'requests, such as generating an image or processing one, feel '\n",
      "               'free to let me know!',\n",
      "    'role': 'assistant'}]\n",
      "bot response:\n",
      "[ { 'content': 'Hello! How can I assist you today? If you have any specific '\n",
      "               'requests, such as generating an image or processing one, feel '\n",
      "               'free to let me know!',\n",
      "    'role': 'assistant'}]\n",
      "bot response:\n",
      "[ { 'content': 'Hello! How can I assist you today? If you have any specific '\n",
      "               'requests, such as generating an image or processing one, feel '\n",
      "               'free to let me know!',\n",
      "    'role': 'assistant'}]\n",
      "bot response:\n",
      "[{'content': 'Thank', 'role': 'assistant'}]\n",
      "bot response:\n",
      "[{'content': 'Thank you', 'role': 'assistant'}]\n",
      "bot response:\n",
      "[{'content': 'Thank you so', 'role': 'assistant'}]\n",
      "bot response:\n",
      "[{'content': \"Thank you so much! I'm\", 'role': 'assistant'}]\n",
      "bot response:\n",
      "[{'content': \"Thank you so much! I'm here to help,\", 'role': 'assistant'}]\n",
      "bot response:\n",
      "[ { 'content': \"Thank you so much! I'm here to help, so if you have\",\n",
      "    'role': 'assistant'}]\n",
      "bot response:\n",
      "[ { 'content': \"Thank you so much! I'm here to help, so if you have any \"\n",
      "               'requests or need',\n",
      "    'role': 'assistant'}]\n",
      "bot response:\n",
      "[ { 'content': \"Thank you so much! I'm here to help, so if you have any \"\n",
      "               'requests or need assistance with anything,',\n",
      "    'role': 'assistant'}]\n",
      "bot response:\n",
      "[ { 'content': \"Thank you so much! I'm here to help, so if you have any \"\n",
      "               'requests or need assistance with anything, just let me know',\n",
      "    'role': 'assistant'}]\n",
      "bot response:\n",
      "[ { 'content': \"Thank you so much! I'm here to help, so if you have any \"\n",
      "               'requests or need assistance with anything, just let me know! 😊',\n",
      "    'role': 'assistant'}]\n",
      "bot response:\n",
      "[ { 'content': \"Thank you so much! I'm here to help, so if you have any \"\n",
      "               'requests or need assistance with anything, just let me know! 😊',\n",
      "    'role': 'assistant'}]\n",
      "bot response:\n",
      "[{'content': 'It', 'role': 'assistant'}]\n",
      "bot response:\n",
      "[{'content': 'It seems', 'role': 'assistant'}]\n",
      "bot response:\n",
      "[{'content': 'It seems like', 'role': 'assistant'}]\n",
      "bot response:\n",
      "[{'content': 'It seems like your message might have', 'role': 'assistant'}]\n",
      "bot response:\n",
      "[ { 'content': 'It seems like your message might have been cut off.',\n",
      "    'role': 'assistant'}]\n",
      "bot response:\n",
      "[ { 'content': 'It seems like your message might have been cut off. Could you '\n",
      "               'please provide',\n",
      "    'role': 'assistant'}]\n",
      "bot response:\n",
      "[ { 'content': 'It seems like your message might have been cut off. Could you '\n",
      "               'please provide more details so I',\n",
      "    'role': 'assistant'}]\n",
      "bot response:\n",
      "[ { 'content': 'It seems like your message might have been cut off. Could you '\n",
      "               'please provide more details so I can assist you better',\n",
      "    'role': 'assistant'}]\n",
      "bot response:\n",
      "[ { 'content': 'It seems like your message might have been cut off. Could you '\n",
      "               'please provide more details so I can assist you better? '\n",
      "               \"Whether it's\",\n",
      "    'role': 'assistant'}]\n",
      "bot response:\n",
      "[ { 'content': 'It seems like your message might have been cut off. Could you '\n",
      "               'please provide more details so I can assist you better? '\n",
      "               \"Whether it's generating an image,\",\n",
      "    'role': 'assistant'}]\n",
      "bot response:\n",
      "[ { 'content': 'It seems like your message might have been cut off. Could you '\n",
      "               'please provide more details so I can assist you better? '\n",
      "               \"Whether it's generating an image, processing one, or\",\n",
      "    'role': 'assistant'}]\n",
      "bot response:\n",
      "[ { 'content': 'It seems like your message might have been cut off. Could you '\n",
      "               'please provide more details so I can assist you better? '\n",
      "               \"Whether it's generating an image, processing one, or any other \"\n",
      "               'task,',\n",
      "    'role': 'assistant'}]\n",
      "bot response:\n",
      "[ { 'content': 'It seems like your message might have been cut off. Could you '\n",
      "               'please provide more details so I can assist you better? '\n",
      "               \"Whether it's generating an image, processing one, or any other \"\n",
      "               \"task, I'm here to\",\n",
      "    'role': 'assistant'}]\n",
      "bot response:\n",
      "[ { 'content': 'It seems like your message might have been cut off. Could you '\n",
      "               'please provide more details so I can assist you better? '\n",
      "               \"Whether it's generating an image, processing one, or any other \"\n",
      "               \"task, I'm here to help!\",\n",
      "    'role': 'assistant'}]\n",
      "bot response:\n",
      "[ { 'content': 'It seems like your message might have been cut off. Could you '\n",
      "               'please provide more details so I can assist you better? '\n",
      "               \"Whether it's generating an image, processing one, or any other \"\n",
      "               \"task, I'm here to help!\",\n",
      "    'role': 'assistant'}]\n",
      "bot response:\n",
      "[{'content': 'It', 'role': 'assistant'}]\n",
      "bot response:\n",
      "[{'content': 'It looks like your message might', 'role': 'assistant'}]\n",
      "bot response:\n",
      "[ { 'content': 'It looks like your message might have been sent without',\n",
      "    'role': 'assistant'}]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 63\u001b[0m\n\u001b[1;32m     61\u001b[0m messages\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: query})\n\u001b[1;32m     62\u001b[0m response \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 63\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Streaming output.\u001b[39;49;00m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbot response:\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpprint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/qwen_agent/agent.py:94\u001b[0m, in \u001b[0;36mAgent.run\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 94\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrsp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrsp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrsp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/qwen_agent/agents/fncall_agent.py:60\u001b[0m, in \u001b[0;36mFnCallAgent._run\u001b[0;34m(self, messages, lang, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m output_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_llm(messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m     57\u001b[0m                                functions\u001b[38;5;241m=\u001b[39m[func\u001b[38;5;241m.\u001b[39mfunction \u001b[38;5;28;01mfor\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_map\u001b[38;5;241m.\u001b[39mvalues()],\n\u001b[1;32m     58\u001b[0m                                extra_generate_cfg\u001b[38;5;241m=\u001b[39mextra_generate_cfg)\n\u001b[1;32m     59\u001b[0m output: List[Message] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 60\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/qwen_agent/llm/base.py:347\u001b[0m, in \u001b[0;36mBaseChatModel._convert_messages_iterator_to_target_type\u001b[0;34m(self, messages_iter, target_type)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_convert_messages_iterator_to_target_type\u001b[39m(\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;28mself\u001b[39m, messages_iter: Iterator[List[Message]],\n\u001b[1;32m    346\u001b[0m         target_type: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Iterator[List[Message]], Iterator[List[Dict]]]:\n\u001b[0;32m--> 347\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessages_iter\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_messages_to_target_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/qwen_agent/llm/base.py:235\u001b[0m, in \u001b[0;36mBaseChatModel.chat.<locals>._format_and_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_format_and_cache\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[List[Message]]:\n\u001b[1;32m    234\u001b[0m     o \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 235\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupport_multimodal_output\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/qwen_agent/llm/base.py:331\u001b[0m, in \u001b[0;36mBaseChatModel._postprocess_messages_iterator\u001b[0;34m(self, messages, fncall_mode, generate_cfg)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_postprocess_messages_iterator\u001b[39m(\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    326\u001b[0m     messages: Iterator[List[Message]],\n\u001b[1;32m    327\u001b[0m     fncall_mode: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    328\u001b[0m     generate_cfg: \u001b[38;5;28mdict\u001b[39m,\n\u001b[1;32m    329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[List[Message]]:\n\u001b[1;32m    330\u001b[0m     pre_msg \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 331\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpre_msg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_postprocess_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpre_msg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfncall_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfncall_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerate_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerate_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLLM Output:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpformat([_\u001b[38;5;241m.\u001b[39mmodel_dump()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39m_\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mpre_msg],\u001b[38;5;250m \u001b[39mindent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/qwen_agent/llm/base.py:498\u001b[0m, in \u001b[0;36mretry_model_service_iterator\u001b[0;34m(it_fn, max_retries)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrsp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrsp\u001b[49m\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/qwen_agent/llm/qwen_dashscope.py:107\u001b[0m, in \u001b[0;36mQwenChatAtDS._full_stream_output\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_full_stream_output\u001b[39m(response) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[List[Message]]:\n\u001b[0;32m--> 107\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mHTTPStatus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOK\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mASSISTANT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/dashscope/aigc/generation.py:149\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    146\u001b[0m is_stream \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_stream:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mGenerationResponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_api_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrsp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 149\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrsp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GenerationResponse\u001b[38;5;241m.\u001b[39mfrom_api_response(response)\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/dashscope/api_entities/http_request.py:81\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     79\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_request()\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(response)\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/dashscope/api_entities/http_request.py:302\u001b[0m, in \u001b[0;36mHttpRequest._handle_request\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    301\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(e)\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/dashscope/api_entities/http_request.py:298\u001b[0m, in \u001b[0;36mHttpRequest._handle_request\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    296\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m UnsupportedHTTPMethod(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported http method: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    297\u001b[0m                                         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod)\n\u001b[0;32m--> 298\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrsp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrsp\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/dashscope/api_entities/http_request.py:208\u001b[0m, in \u001b[0;36mHttpRequest._handle_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    204\u001b[0m request_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m HTTPStatus\u001b[38;5;241m.\u001b[39mOK \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m SSE_CONTENT_TYPE \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    207\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent-type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m--> 208\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_handle_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/dashscope/common/utils.py:212\u001b[0m, in \u001b[0;36m_handle_stream\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    210\u001b[0m event \u001b[38;5;241m=\u001b[39m SSEEvent(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    211\u001b[0m eventType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/requests/models.py:869\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;124;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \n\u001b[1;32m    864\u001b[0m \u001b[38;5;124;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    867\u001b[0m pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 869\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_unicode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_unicode\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/urllib3/response.py:1063\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m-> 1063\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/urllib3/response.py:1219\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1216\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1219\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1221\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/urllib3/response.py:1138\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1138\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1251\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1249\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1103\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "File \u001b[0;32m/Volumes/Outlier/Minerva Agent/.venv/lib/python3.12/site-packages/qwen_agent/utils/utils.py:46\u001b[0m, in \u001b[0;36mappend_signal_handler.<locals>.new_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m handler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m old_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mold_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import urllib.parse\n",
    "import json5\n",
    "import requests\n",
    "from qwen_agent.agents import Assistant\n",
    "from qwen_agent.tools.base import BaseTool, register_tool\n",
    "\n",
    "# Step 1: Add a custom tool named `my_image_gen`.\n",
    "@register_tool('my_image_gen')\n",
    "class MyImageGen(BaseTool):\n",
    "    description = 'AI painting (image generation) service, input text description, and return the image URL drawn based on text information.'\n",
    "    parameters = [{\n",
    "        'name': 'prompt',\n",
    "        'type': 'string',\n",
    "        'description': 'Detailed description of the desired image content, in English',\n",
    "        'required': True\n",
    "    }]\n",
    "\n",
    "    def call(self, params: str, **kwargs) -> str:\n",
    "        prompt = json5.loads(params)['prompt']\n",
    "        prompt = urllib.parse.quote(prompt)\n",
    "        return json5.dumps(\n",
    "            {'image_url': f'https://image.pollinations.ai/prompt/{prompt}'},\n",
    "            ensure_ascii=False)\n",
    "\n",
    "\n",
    "# Step 2: Configure the LLM you are using.\n",
    "llm_cfg = {\n",
    "    'model': 'qwen-max',\n",
    "    'model_server': 'dashscope',\n",
    "    'api_key': 'sk-1a28c3fcc7e044cbacd6faf47dc89755',  # Replace with your API key if not set in environment variable\n",
    "    'generate_cfg': {\n",
    "        'top_p': 0.8\n",
    "    }\n",
    "}\n",
    "\n",
    "# Step 3: Create an agent. This bot will first generate an image and then process it.\n",
    "system_instruction = '''You are a helpful assistant.\n",
    "After receiving the user's request, you should:\n",
    "- first draw an image and obtain the image url,\n",
    "- then run code `request.get(image_url)` to download the image,\n",
    "- and finally select an image operation from the given document to process the image.\n",
    "Please show the image using `plt.show()`.\n",
    "\n",
    "Additionally, you have access to a document which you can refer to for processing the user's query. The document can be found at:\n",
    "- \"https://arxiv.org/pdf/2501.12948\"\n",
    "'''\n",
    "\n",
    "tools = ['my_image_gen', 'code_interpreter']  # `code_interpreter` is a built-in tool for executing code.\n",
    "files = []  # No need to give the bot a file; it will retrieve the PDF from the URL.\n",
    "\n",
    "bot = Assistant(llm=llm_cfg,\n",
    "                system_message=system_instruction,\n",
    "                function_list=tools,\n",
    "                files=files)\n",
    "\n",
    "# Step 4: Run the agent as a chatbot.\n",
    "messages = []  # This stores the chat history.\n",
    "while True:\n",
    "    query = input('user query: ')\n",
    "    messages.append({'role': 'user', 'content': query})\n",
    "    response = []\n",
    "    for response in bot.run(messages=messages):\n",
    "        # Streaming output.\n",
    "        print('bot response:')\n",
    "        pprint.pprint(response, indent=2)\n",
    "    \n",
    "    # Append the bot responses to the chat history.\n",
    "    messages.extend(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0 --(open_auto_replies)--> S1\n",
      "S1 --(enable_auto_replies)--> S2\n",
      "S2 --(add_new_rule)--> S3\n",
      "S3 --(save_rule)--> S2\n",
      "S2 --(add_new_rule)--> S3\n",
      "S3 --(save_rule)--> S2\n",
      "S2 --(confirm_settings)--> S4\n",
      "\n",
      "Final state: S4\n"
     ]
    }
   ],
   "source": [
    "class AutoReplySTN:\n",
    "    \"\"\"\n",
    "    A simple state machine to illustrate the basic flow for setting multiple auto-reply rules.\n",
    "    States:\n",
    "      S0: Main Outlook Window\n",
    "      S1: Automatic Replies Window\n",
    "      S2: Auto Replies Enabled (where multiple rules can be created)\n",
    "      S3: Rule Editor (user is creating/editing a rule)\n",
    "      S4: Task Completed (final state)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.current_state = \"S0\"\n",
    "\n",
    "        # Define valid transitions from each state\n",
    "        # Key: current_state, Value: { action: next_state }\n",
    "        self.transitions = {\n",
    "            \"S0\": {\n",
    "                \"open_auto_replies\": \"S1\",\n",
    "            },\n",
    "            \"S1\": {\n",
    "                \"enable_auto_replies\": \"S2\",\n",
    "            },\n",
    "            \"S2\": {\n",
    "                \"add_new_rule\": \"S3\",\n",
    "                \"confirm_settings\": \"S4\",  # finish setup\n",
    "            },\n",
    "            \"S3\": {\n",
    "                \"save_rule\": \"S2\",\n",
    "            },\n",
    "            # S4 is final; no further transitions in the \"happy path\"\n",
    "            \"S4\": {},\n",
    "        }\n",
    "\n",
    "    def do(self, action):\n",
    "        \"\"\"\n",
    "        Attempt to perform an 'action' from the current state.\n",
    "        If valid, update the state; if not, raise an error.\n",
    "        \"\"\"\n",
    "        if action in self.transitions[self.current_state]:\n",
    "            new_state = self.transitions[self.current_state][action]\n",
    "            print(f\"{self.current_state} --({action})--> {new_state}\")\n",
    "            self.current_state = new_state\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid action '{action}' from state '{self.current_state}'\")\n",
    "\n",
    "    def run_happy_path(self):\n",
    "        \"\"\"\n",
    "        Demonstrates the successful scenario:\n",
    "        1. Open the Auto Replies window\n",
    "        2. Enable auto replies\n",
    "        3. Add the first rule, then save\n",
    "        4. Add the second rule, then save\n",
    "        5. Confirm settings (finishes at S4)\n",
    "        \"\"\"\n",
    "        # S0 -> S1\n",
    "        self.do(\"open_auto_replies\")\n",
    "\n",
    "        # S1 -> S2\n",
    "        self.do(\"enable_auto_replies\")\n",
    "\n",
    "        # S2 -> S3\n",
    "        self.do(\"add_new_rule\")\n",
    "\n",
    "        # S3 -> S2\n",
    "        self.do(\"save_rule\")\n",
    "\n",
    "        # S2 -> S3 (again, to add second rule)\n",
    "        self.do(\"add_new_rule\")\n",
    "\n",
    "        # S3 -> S2\n",
    "        self.do(\"save_rule\")\n",
    "\n",
    "        # S2 -> S4 (finalize settings)\n",
    "        self.do(\"confirm_settings\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    machine = AutoReplySTN()\n",
    "    machine.run_happy_path()\n",
    "    print(f\"\\nFinal state: {machine.current_state}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
