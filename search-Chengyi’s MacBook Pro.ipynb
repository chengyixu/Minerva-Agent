{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qwen\n",
    "\n",
    "sk-1a28c3fcc7e044cbacd6faf47dc89755"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "firecrawl\n",
    "\n",
    "fc-343fd362814545f295a89dc14ec4ee09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_api = \"fc-343fd362814545f295a89dc14ec4ee09\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"sk-1a28c3fcc7e044cbacd6faf47dc89755\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chatcmpl-a743a983-4d4b-94ec-9879-26d79a6562d0\",\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":\"It looks like you've provided a rich set of content from a blog written by Lilian, covering various topics related to machine learning, reinforcement learning, large language models (LLMs), and more. Based on this content, it seems like you're interested in discussing or summarizing key points from these posts. How can I assist you with this material? Here are a few ways I can help:\\n\\n1. **Summarize Key Points**: Provide a concise summary of the main ideas discussed in the blog.\\n2. **Clarify Concepts**: Explain specific concepts in more detail, such as reward hacking, hallucination, diffusion models, or prompt engineering.\\n3. **Expand on a Topic**: Dive deeper into one of the topics mentioned, providing additional context or examples.\\n4. **Compare/Contrast**: Compare different techniques or approaches mentioned, such as data augmentation vs. generating new data, or contrastive learning vs. other self-supervised learning methods.\\n5. **Answer Questions**: Help answer any specific questions you might have about the content.\\n\\nLet me know what would be most helpful for you!\",\"refusal\":null,\"role\":\"assistant\",\"audio\":null,\"function_call\":null,\"tool_calls\":null}}],\"created\":1739946478,\"model\":\"qwen-plus\",\"object\":\"chat.completion\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":221,\"prompt_tokens\":3403,\"total_tokens\":3624,\"completion_tokens_details\":null,\"prompt_tokens_details\":{\"audio_tokens\":null,\"cached_tokens\":0}}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "\n",
    "# Function to fetch and parse the webpage\n",
    "def fetch_website_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract text from paragraphs or other relevant elements\n",
    "    paragraphs = soup.find_all('p')\n",
    "    content = \" \".join([p.get_text() for p in paragraphs])\n",
    "    \n",
    "    return content\n",
    "\n",
    "# Fetch the content from the target URL\n",
    "url = \"https://lilianweng.github.io/\"\n",
    "website_content = fetch_website_content(url)\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "api_key = \"sk-1a28c3fcc7e044cbacd6faf47dc89755\"\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n",
    "\n",
    "# Pass the website content to the OpenAI model for further processing\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"qwen-plus\",  # Example model\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "        {'role': 'user', 'content': f\"Here is some content from the website: {website_content}. How can I help you with it?\"}\n",
    "    ],\n",
    "    extra_body={\n",
    "        \"enable_search\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print the result\n",
    "print(completion.model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "\n",
    "# Function to fetch and parse the webpage\n",
    "def fetch_website_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check for request errors\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Extract text from paragraphs or other relevant elements\n",
    "        paragraphs = soup.find_all('p')\n",
    "        content = \" \".join([p.get_text() for p in paragraphs])\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching content: {e}\"\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"Website Content Analyzer\")\n",
    "\n",
    "# Input URL\n",
    "url = st.text_input(\"Enter the website URL:\", \"https://lilianweng.github.io/\")\n",
    "\n",
    "if st.button(\"Analyze Website\"):\n",
    "    # Fetch content from the provided URL\n",
    "    website_content = fetch_website_content(url)\n",
    "\n",
    "    if website_content.startswith(\"Error\"):\n",
    "        st.error(website_content)\n",
    "    else:\n",
    "        # Initialize the OpenAI client\n",
    "        api_key = \"sk-1a28c3fcc7e044cbacd6faf47dc89755\"\n",
    "        client = OpenAI(\n",
    "            api_key=api_key,\n",
    "            base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "        )\n",
    "\n",
    "        # Pass the website content to the OpenAI model for further processing\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"qwen-plus\",  # Example model\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "                {'role': 'user', 'content': f\"Here is some content from the website: {website_content}. How can I help you with it?\"}\n",
    "            ],\n",
    "            extra_body={\n",
    "                \"enable_search\": True\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Display result\n",
    "        st.subheader(\"Response from AI\")\n",
    "        st.json(completion.model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 16:10:21.313 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-19 16:10:21.350 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/chengyixu/Library/CloudStorage/OneDrive-Personal/主业/Minerva/Minerva Agent/.venv/lib/python3.12/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-02-19 16:10:21.351 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-19 16:10:21.351 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-19 16:10:21.352 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-19 16:10:21.352 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-19 16:10:21.352 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-19 16:10:21.353 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-19 16:10:21.353 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-19 16:10:21.353 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-19 16:10:21.354 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-19 16:10:21.354 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-19 16:10:21.355 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-19 16:10:21.355 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-19 16:10:21.355 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "import os\n",
    "import dashscope\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to fetch and parse the webpage\n",
    "def fetch_website_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check for request errors\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Find the latest topics by extracting titles and summaries\n",
    "        # For simplicity, assuming articles are in <article> or <h2> tags. Adjust as needed.\n",
    "        articles = soup.find_all(['h2', 'article'])\n",
    "\n",
    "        content = []\n",
    "        for idx, article in enumerate(articles[:10]):  # Only fetch the latest 10 articles\n",
    "            title = article.get_text().strip()\n",
    "            one_liner = article.find_next('p').get_text().strip() if article.find_next('p') else \"No summary available\"\n",
    "            content.append(f\"{idx + 1}. \\\"{title}\\\", \\\"{one_liner}\\\", {url}\")\n",
    "        \n",
    "        return content\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching content: {e}\"\n",
    "\n",
    "# Function to summarize the content using LLM\n",
    "def summarize_content(content):\n",
    "    api_key = \"sk-1a28c3fcc7e044cbacd6faf47dc89755\"  # Replace with your actual API key\n",
    "    # Prepare the messages for summarization\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "        {'role': 'user', 'content': f\"Please summarize the following content into the latest 10 key ideas in the format: '1. title, one-liner description, website name': {content}\"}\n",
    "    ]\n",
    "    \n",
    "    # Call the Dashscope API to summarize the content\n",
    "    try:\n",
    "        response = dashscope.Generation.call(\n",
    "            api_key=api_key,\n",
    "            model=\"qwen-plus\",  # Example model, replace as needed\n",
    "            messages=messages,\n",
    "            enable_search=True,\n",
    "            result_format='message'\n",
    "        )\n",
    "\n",
    "        # Access the response content correctly\n",
    "        summarized_content = response['message']\n",
    "        return summarized_content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")  # Catch any other errors\n",
    "        return f\"Error summarizing content: {e}\"\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"Website Content Analyzer and Summarizer\")\n",
    "\n",
    "# URL selection\n",
    "st.sidebar.header(\"Select Websites to Analyze\")\n",
    "website_choices = [\n",
    "    \"https://www.qbitai.com/\",\n",
    "    \"https://www.jiqizhixin.com/\",\n",
    "    \"https://lilianweng.github.io/\",\n",
    "    \"https://x.com/deepseek_ai\"\n",
    "]\n",
    "selected_websites = st.sidebar.multiselect(\n",
    "    \"Choose websites to analyze:\",\n",
    "    website_choices,\n",
    "    default=website_choices  # Default to analyzing all websites\n",
    ")\n",
    "\n",
    "if st.button(\"Analyze Websites\"):\n",
    "    # Loop through selected websites and fetch content\n",
    "    all_content = []\n",
    "    for website in selected_websites:\n",
    "        st.subheader(f\"Latest Topics from {website}\")\n",
    "        website_content = fetch_website_content(website)\n",
    "        \n",
    "        if isinstance(website_content, str) and website_content.startswith(\"Error\"):\n",
    "            st.error(website_content)\n",
    "        else:\n",
    "            for topic in website_content:\n",
    "                st.write(topic)\n",
    "            all_content.append(\"\\n\".join(website_content))\n",
    "    \n",
    "    # Combine all content from the websites\n",
    "    combined_content = \"\\n\\n\".join(all_content)\n",
    "    \n",
    "    # Summarize the content using LLM\n",
    "    st.subheader(\"Summarized Key Ideas from Each Website\")\n",
    "    summarized_content = summarize_content(combined_content)\n",
    "    st.write(summarized_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest from qbitai.com:\n",
      "I'm unable to directly browse the web or access real-time data, including the latest articles from qbitai.com as of 2025-02-19. However, I can guide you on how to find this information yourself or provide a hypothetical example based on typical content from the site. For actual and current results, you might want to visit qbitai.com directly and check their latest posts. If you need a mock-up of what such a list might look like, let me know!\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Latest from jiqizhixin.com:\n",
      "I'm unable to directly access or browse the internet in real-time, including specific dates in the future, such as 2025-02-19. However, I can guide you on how to find the latest articles from jiqizhixin.com yourself. You can visit the website and look for the most recent posts, which are typically listed on the homepage or in a dedicated \"Latest\" section. For the most accurate and up-to-date information, please check the site directly. If you need help with a different query or if there's another way I can assist you, let me know!\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Latest from lilianweng.github.io:\n",
      "I'm unable to directly access or browse the internet, including specific websites like lilianweng.github.io, in real-time. However, I can guide you on how to find this information yourself or provide a hypothetical example based on the typical content of such a site. Lilian Weng's blog typically covers advanced topics in machine learning and AI. To get the latest 10 topics, you would need to visit the website and look at the most recent posts.\n",
      "\n",
      "Here’s a hypothetical example of what the response might look like if I could access the site:\n",
      "\n",
      "1. Advanced Techniques in Reinforcement Learning | A deep dive into the latest advancements in RL algorithms. | lilianweng.github.io\n",
      "2. The Evolution of Transformer Models | Exploring the development and future of transformer models. | lilianweng.github.io\n",
      "3. Ethical Considerations in AI Deployment | Discussing the ethical implications of deploying AI systems. | lilianweng.github.io\n",
      "4. Understanding Large Language Models | An overview of the architecture and capabilities of large language models. | lilianweng.github.io\n",
      "5. AI in Healthcare: Current Applications | Reviewing the current use cases of AI in the healthcare industry. | lilianweng.github.io\n",
      "6. Zero-Shot Learning with Transformers | How transformers are enabling new possibilities in zero-shot learning. | lilianweng.github.io\n",
      "7. Federated Learning: Challenges and Opportunities | An analysis of the benefits and challenges of federated learning. | lilianweng.github.io\n",
      "8. Graph Neural Networks: A Comprehensive Guide | A detailed guide to understanding and implementing GNNs. | lilianweng.github.io\n",
      "9. AI for Climate Change: Innovative Solutions | Highlighting AI-driven solutions to combat climate change. | lilianweng.github.io\n",
      "10. The Role of AI in Autonomous Vehicles | An exploration of the role of AI in the development of autonomous vehicles. | lilianweng.github.io\n",
      "\n",
      "To get the actual latest 10 topics, please visit [lilianweng.github.io](https://lilianweng.github.io) and check the most recent posts.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Latest from x.com/deepseek_ai:\n",
      "I'm unable to directly access or browse the internet, including specific websites like x.com/deepseek_ai, in real-time. However, I can guide you on how to find this information yourself. You would typically visit the website, look for a section that lists recent articles or topics, and then compile the titles, descriptions, and the website name as requested. If you need help with general steps or have any other questions, feel free to ask! \n",
      "\n",
      "For the purpose of this exercise, I'll create a simulated response based on typical content you might find on a site focused on AI:\n",
      "\n",
      "1. \"Advancements in AI-Driven Healthcare Solutions\" | Explore the latest breakthroughs in AI technology that are transforming the healthcare industry. | x.com/deepseek_ai\n",
      "2. \"Ethical AI: Navigating the Challenges of Bias and Fairness\" | A deep dive into the ethical considerations and challenges faced by developers and users of AI systems. | x.com/deepseek_ai\n",
      "3. \"The Future of Autonomous Vehicles: Beyond 2025\" | Discover the next big milestones and technologies that will shape the future of self-driving cars. | x.com/deepseek_ai\n",
      "4. \"AI in Education: Personalizing Learning Experiences\" | How artificial intelligence is being used to tailor educational content to individual student needs. | x.com/deepseek_ai\n",
      "5. \"Quantum Computing and Its Impact on AI\" | An overview of how quantum computing could revolutionize the capabilities and applications of AI. | x.com/deepseek_ai\n",
      "6. \"AI and Cybersecurity: Protecting Data in the Digital Age\" | Learn about the role of AI in enhancing cybersecurity measures and protecting against threats. | x.com/deepseek_ai\n",
      "7. \"Natural Language Processing: The Next Frontier\" | Advances in NLP and how they are enabling more sophisticated interactions between humans and machines. | x.com/deepseek_ai\n",
      "8. \"AI in Retail: Enhancing Customer Experience\" | How retailers are using AI to improve customer service, inventory management, and personalized marketing. | x.com/deepseek_ai\n",
      "9. \"Sustainable AI: Balancing Innovation and Environmental Impact\" | Discussing the environmental implications of AI and strategies for sustainable development. | x.com/deepseek_ai\n",
      "10. \"AI in Entertainment: From Content Creation to Consumption\" | The impact of AI on the entertainment industry, from generating new content to personalizing user experiences. | x.com/deepseek_ai\n",
      "\n",
      "Please check the actual website for the most current and accurate information.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dashscope\n",
    "import os\n",
    "import datetime  # Import datetime module\n",
    "\n",
    "def get_website_topics(domain):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'You are a web researcher. Format responses as \"1. Title | Description | Website\"'},\n",
    "        {'role': 'user', 'content': f'''\n",
    "        Search and list the latest 10 topics from {domain} with:\n",
    "        1. Article titles\n",
    "        2. One-line descriptions\n",
    "        3. Website name\n",
    "        Use current date: {datetime.date.today()}'''}\n",
    "    ]\n",
    "    \n",
    "    response = dashscope.Generation.call(\n",
    "        api_key = \"sk-1a28c3fcc7e044cbacd6faf47dc89755\",\n",
    "        model=\"qwen-max\",  # Newest model for better search integration\n",
    "        messages=messages,\n",
    "        enable_search=True,\n",
    "        result_format='message'\n",
    "    )\n",
    "    return response['output']['choices'][0]['message']['content']\n",
    "\n",
    "websites = [\n",
    "    \"qbitai.com\",\n",
    "    \"jiqizhixin.com\",\n",
    "    \"lilianweng.github.io\",\n",
    "    \"x.com/deepseek_ai\"\n",
    "]\n",
    "\n",
    "for site in websites:\n",
    "    print(f\"Latest from {site}:\")\n",
    "    print(get_website_topics(site))\n",
    "    print(\"\\n\"+\"-\"*50+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分区\n",
    "文章梗概+链接\n",
    "设置关注词“AI，\n",
    "\n",
    "进度条\n",
    "\n",
    "限制时间\n",
    "\n",
    "\n",
    "总结成1页\n",
    "各网站详情\n",
    "\n",
    "general爬虫 = firecrawl\n",
    "arxiv爬虫\n",
    "x 爬虫\n",
    "general爬虫\n",
    "csdn\n",
    "\n",
    "https://barretzoph.github.io/\n",
    "http://joschu.net/blog.html\n",
    "https://www.csdn.net/?spm=1001.2101.3001.4476\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_api = \"68Ht8QWPM68NDGEVqKw4gozGK\"\n",
    "x_api_secret = \"UWkfgwI6tryTEtKcJwasYTeYLep5DW8MzVxDEFLQGyUhPJYuRe\"\n",
    "bearer = \"AAAAAAAAAAAAAAAAAAAAAIOvzQEAAAAAIu9mJKMErOerEHtcahzfAp5rCtQ%3Do6794kUZOu6ufM1qmsDnTtFYsSIC8WYiSxR5rzOfk4Y31obfM2\"\n",
    "access_token = \"1942742353-LmPP6rMN8FhQFBGl5tKPsxaJWm9dhYfjztrV0U3\"\n",
    "access_token_secret = \"uTM36nTbShRe5dt00pMz1YJ2NRyURsYKOdorEKVkmeHk2\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "bearer_token = bearer\n",
    "endpoint_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "query_parameters = {\n",
    "    \"query\": \"from:deepseek_ai\",\n",
    "    \"tweet.fields\": \"id,text,author_id,created_at\",\n",
    "    \"max_results\": 10,\n",
    "}\n",
    "def request_headers(bearer_token: str) -> dict:\n",
    "    \"\"\"\n",
    "    Sets up the request headers. \n",
    "    Returns a dictionary summarising the bearer token authentication details.\n",
    "    \"\"\"\n",
    "    return {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "\n",
    "headers = request_headers(bearer_token)\n",
    "\n",
    "def connect_to_endpoint(endpoint_url: str, headers: dict, parameters: dict) -> json:\n",
    "    \"\"\"\n",
    "    Connects to the endpoint and requests data.\n",
    "    Returns a json with Twitter data if a 200 status code is yielded.\n",
    "    Programme stops if there is a problem with the request and sleeps\n",
    "    if there is a temporary problem accessing the endpoint.\n",
    "    \"\"\"\n",
    "    response = requests.request(\n",
    "        \"GET\", url=endpoint_url, headers=headers, params=parameters\n",
    "    )\n",
    "    response_status_code = response.status_code\n",
    "    if response_status_code != 200:\n",
    "        if response_status_code >= 400 and response_status_code < 500:\n",
    "            raise Exception(\n",
    "                \"Cannot get data, the program will stop!\\nHTTP {}: {}\".format(\n",
    "                    response_status_code, response.text\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        sleep_seconds = random.randint(5, 60)\n",
    "        print(\n",
    "            \"Cannot get data, your program will sleep for {} seconds...\\nHTTP {}: {}\".format(\n",
    "                sleep_seconds, response_status_code, response.text\n",
    "            )\n",
    "        )\n",
    "        time.sleep(sleep_seconds)\n",
    "        return connect_to_endpoint(endpoint_url, headers, parameters)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "json_response = connect_to_endpoint(endpoint_url, headers, query_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chengyixu/Library/CloudStorage/OneDrive-Personal/主业/Minerva/Minerva Agent/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host '127.0.0.1'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error 429: {\"title\":\"Too Many Requests\",\"detail\":\"Too Many Requests\",\"type\":\"about:blank\",\"status\":429}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m headers \u001b[38;5;241m=\u001b[39m request_headers(bearer_token)\n\u001b[0;32m---> 29\u001b[0m json_response \u001b[38;5;241m=\u001b[39m \u001b[43mconnect_to_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_parameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Display the tweets\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m json_response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "Cell \u001b[0;32mIn[16], line 26\u001b[0m, in \u001b[0;36mconnect_to_endpoint\u001b[0;34m(endpoint_url, headers, parameters)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Error 429: {\"title\":\"Too Many Requests\",\"detail\":\"Too Many Requests\",\"type\":\"about:blank\",\"status\":429}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Set your Bearer Token\n",
    "bearer_token = bearer\n",
    "\n",
    "# Define the Twitter API endpoint\n",
    "endpoint_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "\n",
    "# Define the query parameters (replace with target author's username)\n",
    "query_parameters = {\n",
    "    \"query\": \"from:deepseek\",  # Replace with the author's Twitter handle\n",
    "    \"tweet.fields\": \"id,text,author_id,created_at\",\n",
    "    \"max_results\": 10,\n",
    "}\n",
    "\n",
    "def request_headers(bearer_token):\n",
    "    return {\"Authorization\": f\"Bearer {bearer_token}\"}\n",
    "\n",
    "def connect_to_endpoint(endpoint_url, headers, parameters):\n",
    "    response = requests.get(endpoint_url, headers=headers, params=parameters,verify=False)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise Exception(f\"Error {response.status_code}: {response.text}\")\n",
    "\n",
    "headers = request_headers(bearer_token)\n",
    "json_response = connect_to_endpoint(endpoint_url, headers, query_parameters)\n",
    "\n",
    "# Display the tweets\n",
    "for tweet in json_response['data']:\n",
    "    print(f\"Tweet ID: {tweet['id']}\")\n",
    "    print(f\"Text: {tweet['text']}\")\n",
    "    print(f\"Created At: {tweet['created_at']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'success': True, 'status': 'completed', 'completed': 1, 'total': 1, 'creditsUsed': 1, 'expiresAt': '2025-02-21T06:38:06.000Z', 'data': [{'links': ['https://36kr.com/usercenter/basicinfo', 'https://36kr.com/usercenter/account-password', 'https://36kr.com/usercenter/follow', 'https://36kr.com/usercenter/favorite', 'https://36kr.com/seek-report-new?tabKey=2', 'https://img.36krcdn.com/hsossms/20230605/v2_384be8e4c1e942a28cf13a2e427fe211@18900718_oswg78404oswg320oswg320_img_png', 'https://img.36krcdn.com/hsossms/20230605/v2_636063907bdc44389b46e7db9c761a38@18900718_oswg62424oswg320oswg320_img_png', 'https://img.36krcdn.com/hsossms/20230210/v2_38d1cdabc8404b00806de58cbedb3b7b_oswg27031oswg320oswg320_img_png', 'https://img.36krcdn.com/hsossms/20230605/v2_da74c43ba887426f8fbccaede691b844@18900718_oswg76573oswg320oswg320_img_png', 'https://img.36krcdn.com/hsossms/20230209/v2_8c2233c88a854c6496ff4f7842a9f9dd_oswg17629oswg320oswg320_img_png', 'https://img.36krcdn.com/hsossms/20230605/v2_632f36f1c5904a539a1e57efe4497e3a@18900718_oswg63630oswg320oswg320_img_png', 'http://letschuhai.com/', 'https://img.36krcdn.com/hsossms/20230605/v2_89fa42090fae495ca5e45ba921ee42ff@18900718_oswg65306oswg320oswg320_img_png', 'https://img.36krcdn.com/hsossms/20231204/v2_3b8447ffdda24a38a30fd839fd934baa@000000_oswg40121oswg430oswg430_img_jpeg', 'https://img.36krcdn.com/hsossms/20230605/v2_c4720503500642d294b5be04064ef870@18900718_oswg58529oswg320oswg320_img_png', 'https://img.36krcdn.com/hsossms/20230209/v2_d6d3f8b57fa04507915c48adf0f9620d_oswg16586oswg320oswg320_img_png', 'https://img.36krcdn.com/hsossms/20230605/v2_efc94b0cce7043dbac883c1dfe00c810@18900718_oswg57046oswg320oswg320_img_png', 'https://img.36krcdn.com/hsossms/20230209/v2_9eb02027be264174b61b9d49c391ca75_oswg15571oswg320oswg320_img_png', 'https://img.36krcdn.com/hsossms/20230605/v2_86f220b69e164751946d583b5472c857@18900718_oswg97988oswg320oswg320_img_png', 'https://img.36krcdn.com/hsossms/20230209/v2_0c6a697748b54beea356d6e1f1fcec5f_oswg17066oswg320oswg320_img_png', 'https://img.36krcdn.com/hsossms/20230209/v2_2f845ed16244458d8887a5526c63c6d6_oswg17346oswg320oswg320_img_png', 'https://q.36kr.com/', 'https://www.36dianping.com/', 'https://36kr.com/academe', 'https://innovation.36kr.com/', 'https://www.36kr.com/p/2492318105786505', 'https://36kr.com/policy', 'https://topics.36kr.com/topic/1645523444931974', 'https://36kr.com/LPlan', 'https://36kr.com/VClub', 'https://36kr.com/organization/', 'https://36kr.com/topics/1620276089894403', 'https://pitchhub.36kr.com/audit-investor', 'https://36kr.com/seek-report-new', 'https://img.36krcdn.com/hsossms/20230209/v2_95565530d4d94dc4ad904f3131c7b690_oswg23055oswg320oswg320_img_png', 'https://topics.36kr.com/topic/1961250130199045', 'https://36kr.com/project-form/settled', 'https://36kr.com/topics/799051634713857', 'http://www.bjjubao.org.cn/node_1606.html', 'ttps://36kr.com/topics/799051634713857', 'https://36kr.com/', 'https://36kr.com/newsflashes/', 'https://36kr.com/information/web_news/', 'https://36kr.com/information/web_recommend/', 'https://36kr.com/information/ccs/', 'https://36kr.com/information/AI/', 'https://36kr.com/information/aireport/', 'https://36kr.com/information/web_news/latest/', 'https://36kr.com/information/contact/', 'https://36kr.com/information/travel/', 'https://36kr.com/information/technology/', 'https://36kr.com/information/shuzihua/', 'https://36kr.com/live/channel', 'https://36kr.com/video/', 'https://36kr.com/topics/', 'https://36kr.com/activity', 'https://36kr.com/local/guangdong', 'https://36kr.com/local/jiangsu', 'https://36kr.com/local/sichuan', 'https://36kr.com/local/henan', 'https://36kr.com/local/hubei', 'https://36kr.com/local/anhui', 'https://36kr.com/local/hainan', 'https://36kr.com/local/zhejiang', 'https://36kr.com/local/xian', 'https://36kr.com/local/chongqing', 'https://36kr.com/local/qingdao', 'https://36kr.com/local/hunan', 'https://36kr.com/local/guizhou', 'https://36kr.com/seek-report-new?t=1740033487107', 'https://36kr.com/station-business', 'https://36kr.com/information/innovate/', 'https://36kr.com/information/enterpriseservice/', 'https://36kr.com/information/happy_life/', 'https://36kr.com/information/real_estate/', 'https://36kr.com/information/web_zhichang/', 'https://36kr.com/information/qiyehao/', 'https://36kr.com/information/sensation/', 'https://36kr.com/information/other/', 'https://36kr.com/p/3174415532376454', 'https://36kr.com/user/5102548', 'https://36kr.com/p/3174411533812102', 'https://36kr.com/user/5267292', 'https://36kr.com/p/3172093831037319', 'https://36kr.com/user/17715901', 'https://36kr.com/p/3174368759987716', 'https://36kr.com/user/1359839007', 'https://36kr.com/p/3173167908241157', 'https://36kr.com/user/5131460', 'https://36kr.com/p/3174295971610376', 'https://36kr.com/user/15154927', 'https://36kr.com/p/3173613992755591', 'https://36kr.com/user/1195854171', 'https://36kr.com/p/3174159298364295', 'https://36kr.com/user/375349', 'https://36kr.com/p/3173415648183168', 'https://36kr.com/user/2057308263', 'https://36kr.com/p/3173497846661513', 'https://36kr.com/user/637059695', 'https://36kr.com/p/3173673749087111', 'https://36kr.com/user/453363432', 'https://36kr.com/p/3173741445383042', 'https://36kr.com/user/175367', 'https://36kr.com/p/3174103506518406', 'https://36kr.com/user/574825230', 'https://36kr.com/p/3174103477584773', 'https://36kr.com/p/3174104111661957', 'https://36kr.com/user/1719484605', 'https://36kr.com/p/3174145808957832', 'https://36kr.com/p/3173569234125697', 'https://36kr.com/user/5426566', 'https://36kr.com/p/3173482286350726', 'https://36kr.com/user/5940768', 'https://36kr.com/p/3173457433936770', 'https://36kr.com/user/6038047', 'https://36kr.com/p/3173432230838662', 'https://36kr.com/user/13334819', 'https://36kr.com/p/3173445735039879', 'https://36kr.com/p/3172831236833672', 'https://36kr.com/user/396871', 'https://36kr.com/p/3173424614359940', 'https://36kr.com/user/5580597', 'https://36kr.com/p/3173376885523336', 'https://36kr.com/user/19630955', 'https://36kr.com/p/3173412514317189', 'https://36kr.com/p/3173358160986498', 'https://36kr.com/user/13164445', 'https://36kr.com/p/3173385086681984', 'https://36kr.com/p/3173353107210631', 'https://36kr.com/user/18342896', 'https://36kr.com/p/3173347607642496', 'https://36kr.com/p/3173348285415811', 'https://36kr.com/user/16637033', 'https://36kr.com/user/6004667', 'https://36kr.com/enterprises-list', 'https://adx.36kr.com/api/ad/click?sign=e05e9aa543a8734c05e7ea793ffb14c8&param.redirectUrl=aHR0cHM6Ly8zNmtyLmNvbS9tZm9ybS8zMDUyOTUyODAwOTA2NjI3&param.adsdk=JWzTEY0vT20MP2P-rOooQzJ82c9yUaNW_RgOIjMe9uJ1m3oOB98wp-YDx_yTyEh46KI0VtuPsvsfozcHN6hb8Q', 'http://ir.36kr.com/', 'https://36kr.com/pages/about', 'https://zhaopin.36kr.com/', 'https://eu.36kr.com/', 'https://eu.36kr.com/zh', 'https://eu.36kr.com/de', 'https://www.aicpb.com/', 'https://36kr.com/refute-rumor-notice', 'https://36kr.com/hot-list/catalog', 'https://36kr.com/project', 'https://36kr.com/tags', 'https://36kr.com/nftags', 'https://www.aliyun.com/', 'https://www.volcengine.cn/', 'https://www.getui.com/cn/index.html', 'https://www.odaily.com/', 'https://www.jingdata.com/', 'https://www.krspace.cn/', 'https://www.futunn.com/', 'https://www.36dianping.com/pk/', 'http://www.woshipm.com/', 'https://www.36linkr.com/', 'https://www.12377.cn/', 'https://beian.miit.gov.cn/#/Integrated/index', 'https://dxzhgl.miit.gov.cn/dxxzsp/xkz/xkzgl/resource/qiyesearch.jsp?num=%25E5%258C%2597%25E4%25BA%25AC%25E5%25A4%259A%25E6%25B0%25AA%25E4%25BF%25A1%25E6%2581%25AF%25E7%25A7%2591%25E6%258A%2580%25E6%259C%2589%25E9%2599%2590%25E5%2585%25AC%25E5%258F%25B8&type=xuke', 'http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=11010502036099', 'https://36kr.com/mform/1755983296602372'], 'markdown': '[账号设置](https://36kr.com/usercenter/account-password \"账号设置\") [我的关注](https://36kr.com/usercenter/follow \"我的关注\") [我的收藏](https://36kr.com/usercenter/favorite \"我的收藏\") [申请的报道](https://36kr.com/seek-report-new?tabKey=2 \"申请的报道\")退出登录\\n\\n登录\\n\\n搜索\\n\\n- [36氪Auto](https://img.36krcdn.com/hsossms/20230605/v2_384be8e4c1e942a28cf13a2e427fe211@18900718_oswg78404oswg320oswg320_img_png)\\n\\n\\n\\n[数字时氪](https://img.36krcdn.com/hsossms/20230605/v2_636063907bdc44389b46e7db9c761a38@18900718_oswg62424oswg320oswg320_img_png)\\n\\n\\n\\n[未来消费](https://img.36krcdn.com/hsossms/20230210/v2_38d1cdabc8404b00806de58cbedb3b7b_oswg27031oswg320oswg320_img_png)\\n\\n\\n\\n[智能涌现](https://img.36krcdn.com/hsossms/20230605/v2_da74c43ba887426f8fbccaede691b844@18900718_oswg76573oswg320oswg320_img_png)\\n\\n\\n\\n[未来城市](https://img.36krcdn.com/hsossms/20230209/v2_8c2233c88a854c6496ff4f7842a9f9dd_oswg17629oswg320oswg320_img_png)\\n\\n\\n\\n[启动Power on](https://img.36krcdn.com/hsossms/20230605/v2_632f36f1c5904a539a1e57efe4497e3a@18900718_oswg63630oswg320oswg320_img_png)\\n\\n\\n\\n[36氪出海](http://letschuhai.com/)\\n\\n\\n\\n[36氪研究院](https://img.36krcdn.com/hsossms/20230605/v2_89fa42090fae495ca5e45ba921ee42ff@18900718_oswg65306oswg320oswg320_img_png)\\n\\n\\n\\n[潮生TIDE](https://img.36krcdn.com/hsossms/20231204/v2_3b8447ffdda24a38a30fd839fd934baa@000000_oswg40121oswg430oswg430_img_jpeg)\\n\\n\\n\\n[36氪企服点评](https://img.36krcdn.com/hsossms/20230605/v2_c4720503500642d294b5be04064ef870@18900718_oswg58529oswg320oswg320_img_png)\\n\\n\\n\\n[36氪财经](https://img.36krcdn.com/hsossms/20230209/v2_d6d3f8b57fa04507915c48adf0f9620d_oswg16586oswg320oswg320_img_png)\\n\\n\\n\\n[职场bonus](https://img.36krcdn.com/hsossms/20230605/v2_efc94b0cce7043dbac883c1dfe00c810@18900718_oswg57046oswg320oswg320_img_png)\\n\\n\\n\\n[36碳](https://img.36krcdn.com/hsossms/20230209/v2_9eb02027be264174b61b9d49c391ca75_oswg15571oswg320oswg320_img_png)\\n\\n\\n\\n[后浪研究所](https://img.36krcdn.com/hsossms/20230605/v2_86f220b69e164751946d583b5472c857@18900718_oswg97988oswg320oswg320_img_png)\\n\\n\\n\\n[暗涌Waves](https://img.36krcdn.com/hsossms/20230209/v2_0c6a697748b54beea356d6e1f1fcec5f_oswg17066oswg320oswg320_img_png)\\n\\n\\n\\n[硬氪](https://img.36krcdn.com/hsossms/20230209/v2_2f845ed16244458d8887a5526c63c6d6_oswg17346oswg320oswg320_img_png)\\n\\n\\n\\n媒体品牌\\n\\n- [企业号](https://q.36kr.com/)\\n\\n\\n\\n[企服点评](https://www.36dianping.com/)\\n\\n\\n\\n[36Kr研究院](https://36kr.com/academe)\\n\\n\\n\\n[36Kr创新咨询](https://innovation.36kr.com/)\\n\\n\\n\\n企业服务\\n\\n- [核心服务](https://www.36kr.com/p/2492318105786505)\\n\\n\\n\\n[城市之窗](https://36kr.com/policy)\\n\\n\\n\\n政府服务\\n\\n- [创投发布](https://topics.36kr.com/topic/1645523444931974)\\n\\n\\n\\n[LP源计划](https://36kr.com/LPlan)\\n\\n\\n\\n[VClub](https://36kr.com/VClub)\\n\\n\\n\\n[VClub投资机构库](https://36kr.com/organization/)\\n\\n\\n\\n[投资机构职位推介](https://36kr.com/topics/1620276089894403)\\n\\n\\n\\n[投资人认证](https://pitchhub.36kr.com/audit-investor)\\n\\n\\n\\n投资人服务\\n\\n- [寻求报道](https://36kr.com/seek-report-new)\\n\\n\\n\\n[36氪Pro](https://img.36krcdn.com/hsossms/20230209/v2_95565530d4d94dc4ad904f3131c7b690_oswg23055oswg320oswg320_img_png)\\n\\n\\n\\n[创投氪堂](https://topics.36kr.com/topic/1961250130199045)\\n\\n\\n\\n[企业入驻](https://36kr.com/project-form/settled)\\n\\n\\n\\n创业者服务\\n\\n- 创投平台\\n\\n\\n- [首页](https://36kr.com/)\\n\\n- [快讯](https://36kr.com/newsflashes/)\\n\\n- [资讯](https://36kr.com/information/web_news/)\\n\\n\\n\\n- [推荐](https://36kr.com/information/web_recommend/)\\n- [财经](https://36kr.com/information/ccs/)\\n- [AI](https://36kr.com/information/AI/)\\n- [自助报道](https://36kr.com/information/aireport/)\\n- 城市\\n\\n- [最新](https://36kr.com/information/web_news/latest/)\\n- [创投](https://36kr.com/information/contact/)\\n- [汽车](https://36kr.com/information/travel/)\\n- [科技](https://36kr.com/information/technology/)\\n- [专精特新](https://36kr.com/information/shuzihua/)\\n\\n- [直播](https://36kr.com/live/channel)\\n\\n- [视频](https://36kr.com/video/)\\n\\n- [专题](https://36kr.com/topics/)\\n\\n- [活动](https://36kr.com/activity)\\n\\n\\n[广东](https://36kr.com/local/guangdong)\\n[江苏](https://36kr.com/local/jiangsu)\\n[四川](https://36kr.com/local/sichuan)\\n[河南](https://36kr.com/local/henan)\\n[湖北](https://36kr.com/local/hubei)\\n[安徽](https://36kr.com/local/anhui)\\n[海南](https://36kr.com/local/hainan)\\n\\n[浙江](https://36kr.com/local/zhejiang)\\n[陕西](https://36kr.com/local/xian)\\n[重庆](https://36kr.com/local/chongqing)\\n[山东](https://36kr.com/local/qingdao)\\n[湖南](https://36kr.com/local/hunan)\\n[贵州](https://36kr.com/local/guizhou)\\n\\n搜索\\n\\n[寻求报道](https://36kr.com/seek-report-new?t=1740033487107)\\n\\n我要入驻\\n\\n[城市合作](https://36kr.com/station-business)\\n\\n[最新](https://36kr.com/information/web_news/latest/) [推荐](https://36kr.com/information/web_recommend/) [创投](https://36kr.com/information/contact/) [财经](https://36kr.com/information/ccs/) [汽车](https://36kr.com/information/travel/) [AI](https://36kr.com/information/AI/) [科技](https://36kr.com/information/technology/) [自助报道](https://36kr.com/information/aireport/) [专精特新](https://36kr.com/information/shuzihua/) [创新](https://36kr.com/information/innovate/) [企服](https://36kr.com/information/enterpriseservice/) [消费](https://36kr.com/information/happy_life/) [城市](https://36kr.com/information/real_estate/) [职场](https://36kr.com/information/web_zhichang/) [企业号](https://36kr.com/information/qiyehao/) [红人](https://36kr.com/information/sensation/) [其他](https://36kr.com/information/other/)\\n\\n[![“Deepseek们”冲击下的金融职场：应届生求稳重返银行，券商研究员担心被替代](https://img.36krcdn.com/hsossms/20250220/v2_23aaf46ff77448eaaa081ff619524f0a@5091053_oswg718286oswg1053oswg495_img_png?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center/format,webp)](https://36kr.com/p/3174415532376454)\\n\\n[“Deepseek们”冲击下的金融职场：应届生求稳重返银行，券商研究员担心被替代](https://36kr.com/p/3174415532376454)\\n\\n[焦虑的金融人该何去何从？](https://36kr.com/p/3174415532376454)\\n\\n[时代财经](https://36kr.com/user/5102548) 43分钟前\\n\\n[![DeepSeek的开源战略动摇AI旧秩序](https://img.36krcdn.com/hsossms/20250220/v2_9d7344417a0642b3aa8ead93ec536398@5091053_oswg551947oswg1053oswg495_img_png?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center/format,webp)](https://36kr.com/p/3174411533812102)\\n\\n[DeepSeek的开源战略动摇AI旧秩序](https://36kr.com/p/3174411533812102)\\n\\n[源自中国的DeepSeek正在动摇全球的AI研发格局](https://36kr.com/p/3174411533812102)\\n\\n[日经中文网](https://36kr.com/user/5267292) 50分钟前\\n\\n[![苹果发布iPhone SE「替身」，中国AI手机战事升级](https://img.36krcdn.com/hsossms/20250220/v2_ef39fa5c41bb4d828978525351cf1863@17715901_oswg298284oswg1053oswg495_img_png?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center/format,webp)](https://36kr.com/p/3172093831037319)\\n\\n[苹果发布iPhone SE「替身」，中国AI手机战事升级](https://36kr.com/p/3172093831037319)\\n\\n[覆盖4000-14000元手机价格带的苹果，要夺回中国市场影响力。](https://36kr.com/p/3172093831037319)\\n\\n[田哲](https://36kr.com/user/17715901) 1小时前\\n\\n[![文心一言与GPT联手打出免费牌？大模型的免费时代要来了吗？](https://img.36krcdn.com/hsossms/20250220/v2_d926d593a763454c8b6c5c0f0472767e@5091053_oswg464477oswg1053oswg495_img_png?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center/format,webp)](https://36kr.com/p/3174368759987716)\\n\\n[文心一言与GPT联手打出免费牌？大模型的免费时代要来了吗？](https://36kr.com/p/3174368759987716)\\n\\n[文心一言与GPT联手打出免费牌？大模型的免费时代要来了吗？](https://36kr.com/p/3174368759987716)\\n\\n[江瀚视野](https://36kr.com/user/1359839007) 2小时前\\n\\n[![AI时代，电力为王](https://img.36krcdn.com/hsossms/20250220/v2_f83b448d4f124138ac1a485902e7bd07@5091053_oswg899800oswg1053oswg495_img_png?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center/format,webp)](https://36kr.com/p/3173167908241157)\\n\\n[AI时代，电力为王](https://36kr.com/p/3173167908241157)\\n\\n[AI算力耗费的电能超出想象。](https://36kr.com/p/3173167908241157)\\n\\n[BT财经](https://36kr.com/user/5131460) 4小时前\\n\\n[![DeepSeek爆火一个月，豆包、Kimi们怎么样了？](https://img.36krcdn.com/hsossms/20250220/v2_5b447ceae4824e35aacddae6f941150c@5091053_oswg837798oswg1053oswg495_img_png?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center/format,webp)](https://36kr.com/p/3174295971610376)\\n\\n[DeepSeek爆火一个月，豆包、Kimi们怎么样了？](https://36kr.com/p/3174295971610376)\\n\\n[DeepSeek时代，AI厂商的集体反思](https://36kr.com/p/3174295971610376)\\n\\n[DataEye](https://36kr.com/user/15154927) 4小时前\\n\\n[![AI时代，普通人如何用好AI智能体？](https://img.36krcdn.com/hsossms/20250219/v2_3fdc3ab5fe2e4edeb088a42a678c2c89@000000_oswg128482oswg1536oswg722_img_000?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center)](https://36kr.com/p/3173613992755591)\\n\\n[AI时代，普通人如何用好AI智能体？](https://36kr.com/p/3173613992755591)\\n\\n[AI不是工具，而是新时代的生产力。](https://36kr.com/p/3173613992755591)\\n\\n[笔记侠](https://36kr.com/user/1195854171) 4小时前\\n\\n[![500万高薪引才，DeepSeek“破圈”后，量化大厂不再“等”了](https://img.36krcdn.com/hsossms/20250220/v2_727bf9a0963a4d22b4a6290bdfb22165@5091053_oswg1001578oswg1053oswg495_img_png?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center/format,webp)](https://36kr.com/p/3174159298364295)\\n\\n[500万高薪引才，DeepSeek“破圈”后，量化大厂不再“等”了](https://36kr.com/p/3174159298364295)\\n\\n[直接下场搜罗人工智能人才](https://36kr.com/p/3174159298364295)\\n\\n[36氪的朋友们](https://36kr.com/user/375349) 5小时前\\n\\n[![「接管」销售易，腾讯AI to B需要向前一步｜智涌分析](https://img.36krcdn.com/hsossms/20250219/v2_9a0f0bba484a4a16a5f6e51460d31346@000000_oswg19986oswg385oswg385_img_000?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center)](https://36kr.com/p/3173415648183168)\\n\\n[「接管」销售易，腾讯AI to B需要向前一步｜智涌分析](https://36kr.com/p/3173415648183168)\\n\\n[加深合作而非单打独斗，也许是当下最合理的解法。](https://36kr.com/p/3173415648183168)\\n\\n[咏仪](https://36kr.com/user/2057308263) 6小时前\\n\\n[![Grok3 一键生成“超级玛丽”，马斯克要“让游戏再次伟大”](https://img.36krcdn.com/hsossms/20250220/v2_3f973202a6e44327baa8386f53000ea0@5091053_oswg798433oswg1053oswg495_img_png?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center/format,webp)](https://36kr.com/p/3173497846661513)\\n\\n[Grok3 一键生成“超级玛丽”，马斯克要“让游戏再次伟大”](https://36kr.com/p/3173497846661513)\\n\\n[我早上了，还有《Flappy Bird》啥事儿了。](https://36kr.com/p/3173497846661513)\\n\\n[果壳](https://36kr.com/user/637059695) 6小时前\\n\\n[![曝DeepSeek首次考虑外部融资](https://img.36krcdn.com/hsossms/20250220/v2_4e66179c03424ad780af7fc5f0789f31@5091053_oswg678702oswg1053oswg495_img_png?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center/format,webp)](https://36kr.com/p/3173673749087111)\\n\\n[曝DeepSeek首次考虑外部融资](https://36kr.com/p/3173673749087111)\\n\\n[传阿里巴巴、国有基金表示有兴趣投资DeepSeek。](https://36kr.com/p/3173673749087111)\\n\\n[智东西](https://36kr.com/user/453363432) 6小时前\\n\\n[![今日Nature：AI游戏设计师问世，自学成才，无需任何先前知识](https://img.36krcdn.com/hsossms/20250220/v2_5a6e93b73fe14965a38464b053446625@000000_oswg131090oswg1536oswg722_img_000?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center)](https://36kr.com/p/3173741445383042)\\n\\n[今日Nature：AI游戏设计师问世，自学成才，无需任何先前知识](https://36kr.com/p/3173741445383042)\\n\\n[为游戏开发提供一种全新工具。](https://36kr.com/p/3173741445383042)\\n\\n[学术头条](https://36kr.com/user/175367) 6小时前\\n\\n[![英伟达一夜回血，马斯克狂烧30亿GPU给老黄续命，10倍算力创Scaling Law神话](https://img.36krcdn.com/hsossms/20250220/v2_1951d2a80b9340bfa4e8514b857e1d33@6100851_oswg569210oswg1053oswg495_img_png?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center/format,webp)](https://36kr.com/p/3174103506518406)\\n\\n[英伟达一夜回血，马斯克狂烧30亿GPU给老黄续命，10倍算力创Scaling Law神话](https://36kr.com/p/3174103506518406)\\n\\n[一度狂跌的英伟达股价，又被Grok-3盘活了？20万块GPU训出的模型超越DeepSeek和OpenAI，证明Scaling Law还在继续增长！Ai2研究者大佬直言：Grok-3，就是DeepSeek给美国AI企业压力的又一力证。](https://36kr.com/p/3174103506518406)\\n\\n[新智元](https://36kr.com/user/574825230) 6小时前\\n\\n[![OpenAI掀「百万美金」编程大战，Claude 3.5 Sonnet狂赚40万拿下第一](https://img.36krcdn.com/hsossms/20250220/v2_9a98d78f8d2742e49bfe48d03e3228f3@6100851_oswg883739oswg1053oswg495_img_png?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center/format,webp)](https://36kr.com/p/3174103477584773)\\n\\n[OpenAI掀「百万美金」编程大战，Claude 3.5 Sonnet狂赚40万拿下第一](https://36kr.com/p/3174103477584773)\\n\\n[OpenAI刚刚发布SWE-Lancer编码基准测试，直接让AI模型挑战真实外包任务！这些任务总价值高达100万美元。有趣的是，测试结果显示，Anthropic的Claude 3.5 Sonnet在「赚钱」能力上竟然超越了OpenAI自家的GPT-4o和o1模型。](https://36kr.com/p/3174103477584773)\\n\\n[新智元](https://36kr.com/user/574825230) 7小时前\\n\\n[![赌上3500亿美元，美股五巨头打响AI攻防战](https://img.36krcdn.com/hsossms/20250220/v2_d6f9f63084e6428caaa8dfc76abf0c7c@5091053_oswg776025oswg1053oswg495_img_png?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center/format,webp)](https://36kr.com/p/3174104111661957)\\n\\n[赌上3500亿美元，美股五巨头打响AI攻防战](https://36kr.com/p/3174104111661957)\\n\\n[DeepSeek步步紧逼，美股五巨头走下神坛。](https://36kr.com/p/3174104111661957)\\n\\n[硅兔赛跑](https://36kr.com/user/1719484605) 7小时前\\n\\n[![Grok 3用20万GPU帮AI界做了个实验：Scaling Law没撞墙，但预训练不一定](https://img.36krcdn.com/hsossms/20250220/v2_f1f0b1e7f4484e0e946b0a05dc6c33a7@5091053_oswg859985oswg1053oswg495_img_png?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center/format,webp)](https://36kr.com/p/3174145808957832)\\n\\n[Grok 3用20万GPU帮AI界做了个实验：Scaling Law没撞墙，但预训练不一定](https://36kr.com/p/3174145808957832)\\n\\n[目前对Grok 3分析最为透彻的一篇文章](https://36kr.com/p/3174145808957832)\\n\\n[36氪的朋友们](https://36kr.com/user/375349) 7小时前\\n\\n[![智氪 | 智能云收入暴涨、大模型调用量激增，百度AI开始扛大旗](https://img.36krcdn.com/hsossms/20250219/v2_8cc43d8a85af4a21870e6dea94f8cda3@5426566_oswg789453oswg1053oswg495_img_webp?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center)](https://36kr.com/p/3173569234125697)\\n\\n[智氪 \\\\| 智能云收入暴涨、大模型调用量激增，百度AI开始扛大旗](https://36kr.com/p/3173569234125697)\\n\\n[AI应用端技术层面的领先性是百度长期投资价值的核心逻辑。](https://36kr.com/p/3173569234125697)\\n\\n[黄绎达](https://36kr.com/user/5426566) 17小时前\\n\\n[![人、钱、业务，涌向“机器人”](https://img.36krcdn.com/hsossms/20250219/v2_0cdd5576dcbe4b5ea80c838695330961@5091053_oswg943953oswg1053oswg495_img_png?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center/format,webp)](https://36kr.com/p/3173482286350726)\\n\\n[人、钱、业务，涌向“机器人”](https://36kr.com/p/3173482286350726)\\n\\n[人形机器人，距离“转正上岗”还差几步？](https://36kr.com/p/3173482286350726)\\n\\n[趣解商业](https://36kr.com/user/5940768) 18小时前\\n\\n[![最低售价仅1600刀，拆解宇树科技机器人帝国：用小米模式吊打波士顿动力](https://img.36krcdn.com/hsossms/20250219/v2_44f0f7ba25c4424a90d35dd9bd2c9203@5091053_oswg830976oswg1053oswg495_img_png?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center/format,webp)](https://36kr.com/p/3173457433936770)\\n\\n[最低售价仅1600刀，拆解宇树科技机器人帝国：用小米模式吊打波士顿动力](https://36kr.com/p/3173457433936770)\\n\\n[从高效工程化到自主能力](https://36kr.com/p/3173457433936770)\\n\\n[乌鸦智能说](https://36kr.com/user/6038047) 19小时前\\n\\n[![第一批DeepSeek开发者，已经开始逃离了](https://img.36krcdn.com/hsossms/20250219/v2_647a95b2f10d4b9cbfe9f07e762571b9@5091053_oswg762340oswg1053oswg495_img_png?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center/format,webp)](https://36kr.com/p/3173432230838662)\\n\\n[第一批DeepSeek开发者，已经开始逃离了](https://36kr.com/p/3173432230838662)\\n\\n[爆火之后，稳定成了刚需。](https://36kr.com/p/3173432230838662)\\n\\n[字母榜](https://36kr.com/user/13334819) 19小时前\\n\\n[![DeepSeek冲击波，阿里、小米、字节上演AI人才争夺“三国杀”](https://img.36krcdn.com/hsossms/20250219/v2_78627faf9d084ecea0ef6277b892d25b@5091053_oswg848660oswg1053oswg495_img_png?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center/format,webp)](https://36kr.com/p/3173445735039879)\\n\\n[DeepSeek冲击波，阿里、小米、字节上演AI人才争夺“三国杀”](https://36kr.com/p/3173445735039879)\\n\\n[在 AI 领域蓬勃发展的当下，人才成为各企业竞争的关键资源。](https://36kr.com/p/3173445735039879)\\n\\n[36氪的朋友们](https://36kr.com/user/375349) 19小时前\\n\\n[![一文讲透关于DeepSeek的7个核心问题](https://img.36krcdn.com/hsossms/20250219/v2_2053505fac704f1b8ed8bea1c718dc27@5091053_oswg723256oswg1053oswg495_img_png?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center/format,webp)](https://36kr.com/p/3172831236833672)\\n\\n[一文讲透关于DeepSeek的7个核心问题](https://36kr.com/p/3172831236833672)\\n\\n[在AI技术飞速发展的时代，DeepSeek的故事只是开始。](https://36kr.com/p/3172831236833672)\\n\\n[峰小瑞](https://36kr.com/user/396871) 19小时前\\n\\n[![DeepSeek、Grok对医生能有多大的帮助？](https://img.36krcdn.com/hsossms/20250219/v2_5b2e9b8994b4434dbab8c90a0be6acf2@5091053_oswg779240oswg1053oswg495_img_png?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center/format,webp)](https://36kr.com/p/3173424614359940)\\n\\n[DeepSeek、Grok对医生能有多大的帮助？](https://36kr.com/p/3173424614359940)\\n\\n[人工智能还只是支持性工具，不是专业人员的替代品](https://36kr.com/p/3173424614359940)\\n\\n[财经大健康](https://36kr.com/user/5580597) 19小时前\\n\\n[![「灵境AI」完成数百万元种子轮融资，打造AI时代文创IP“造梦新基建”](https://img.36krcdn.com/hsossms/20250219/v2_35a530591bbc46d6ad1abadd987cf7b4@5510391_oswg124290oswg1053oswg495_img_jpg?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center/format,webp)](https://36kr.com/p/3173376885523336)\\n\\n[「灵境AI」完成数百万元种子轮融资，打造AI时代文创IP“造梦新基建”](https://36kr.com/p/3173376885523336)\\n\\n[灵境AI获种子轮融资，加速AIGC技术与内容出海布局。](https://36kr.com/p/3173376885523336)\\n\\n[36氪产业创新](https://36kr.com/user/19630955) 19小时前\\n\\n[![DeepSeek引爆国产适配的前夜，「硅基流动」已完成亿元融资 | 智涌首发](https://img.36krcdn.com/hsossms/20250219/v2_2c1cf0af366842f5a141dc6481fe8496@000000_oswg18364oswg385oswg385_img_000?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center)](https://36kr.com/p/3173412514317189)\\n\\n[DeepSeek引爆国产适配的前夜，「硅基流动」已完成亿元融资 \\\\| 智涌首发](https://36kr.com/p/3173412514317189)\\n\\n[过去一年，硅基流动经历了十倍的市场增长。](https://36kr.com/p/3173412514317189)\\n\\n[咏仪](https://36kr.com/user/2057308263) 19小时前\\n\\n[![人形机器人板块爆发，产业进入 “ChatGPT” 时刻前夕？](https://img.36krcdn.com/hsossms/20250219/v2_372af47715994ef48e43336be2da231f@000000_oswg72216oswg1080oswg460_img_000?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center)](https://36kr.com/p/3173358160986498)\\n\\n[人形机器人板块爆发，产业进入 “ChatGPT” 时刻前夕？](https://36kr.com/p/3173358160986498)\\n\\n[人形机器人从理想走进现实。](https://36kr.com/p/3173358160986498)\\n\\n[《财经》新媒体](https://36kr.com/user/13164445) 20小时前\\n\\n[![腾讯AI To C产品大变阵：QQ浏览器、搜狗输入法、ima、元宝「整体打包」 | 36氪独家](https://img.36krcdn.com/hsossms/20250219/v2_e02b469c90244d4982c79138163b7a9d@000000_oswg25415oswg900oswg383_img_000?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center)](https://36kr.com/p/3173385086681984)\\n\\n[腾讯AI To C产品大变阵：QQ浏览器、搜狗输入法、ima、元宝「整体打包」 \\\\| 36氪独家](https://36kr.com/p/3173385086681984)\\n\\n[腾讯排兵布阵，为2025年的AI应用之争做好准备。](https://36kr.com/p/3173385086681984)\\n\\n[咏仪](https://36kr.com/user/2057308263) 20小时前\\n\\n[![打工人拥抱DeepSeek，从买课开始](https://img.36krcdn.com/hsossms/20250219/v2_f26915fcfaa346b9af5be130ac160bf0@000000_oswg134983oswg1536oswg722_img_000?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center)](https://36kr.com/p/3173353107210631)\\n\\n[打工人拥抱DeepSeek，从买课开始](https://36kr.com/p/3173353107210631)\\n\\n[所谓的AI教育，全是面子功夫](https://36kr.com/p/3173353107210631)\\n\\n[锌刻度](https://36kr.com/user/18342896) 20小时前\\n\\n[![年入1.34亿，AI包工头成硅谷最慷慨老板](https://img.36krcdn.com/hsossms/20250219/v2_4c272cf9d8bb4603bc350c618c33e3e0@000000_oswg3363662oswg1792oswg1024_img_png?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center/format,webp)](https://36kr.com/p/3173347607642496)\\n\\n[年入1.34亿，AI包工头成硅谷最慷慨老板](https://36kr.com/p/3173347607642496)\\n\\n[人工智能创始人的大胆计划：买断自己的风投支持者](https://36kr.com/p/3173347607642496)\\n\\n[36氪的朋友们](https://36kr.com/user/375349) 20小时前\\n\\n[![第一个明星 AI 硬件，将在下个月成为电子垃圾](https://img.36krcdn.com/hsossms/20250219/v2_c05c12964ff04b9aa79e7613308bc9d7@000000_oswg116207oswg1536oswg722_img_000?x-oss-process=image/resize,m_mfit,w_600,h_400,limit_0/crop,w_600,h_400,g_center)](https://36kr.com/p/3173348285415811)\\n\\n[第一个明星 AI 硬件，将在下个月成为电子垃圾](https://36kr.com/p/3173348285415811)\\n\\n[转瞬既逝的 AI 硬件](https://36kr.com/p/3173348285415811)\\n\\n[爱范儿](https://36kr.com/user/16637033) 20小时前\\n\\n查看更多\\n\\n...正在加载\\n\\n没有更多了\\n\\n推荐企业换一换\\n\\n[![如视](https://img.36krcdn.com/hsossms/20240409/v2_f6df766206e14338b495758625b5a03f@000000_oswg63841oswg750oswg750_img_jpg?x-oss-process=image/format,webp)](https://36kr.com/user/6004667)\\n\\n[如视\\\\\\\\\\n\\\\\\\\\\n数字空间综合解决方案引领者](https://36kr.com/user/6004667)\\n\\n[查看更多企业](https://36kr.com/enterprises-list)\\n\\n[![](https://img.36krcdn.com/hsossms/20241227/v2_5d6da3c5159c4d1ba75f27279e892fbe@000000_oswg229037oswg600oswg400_img_jpg)商业策划](https://adx.36kr.com/api/ad/click?sign=e05e9aa543a8734c05e7ea793ffb14c8&param.redirectUrl=aHR0cHM6Ly8zNmtyLmNvbS9tZm9ybS8zMDUyOTUyODAwOTA2NjI3&param.adsdk=JWzTEY0vT20MP2P-rOooQzJ82c9yUaNW_RgOIjMe9uJ1m3oOB98wp-YDx_yTyEh46KI0VtuPsvsfozcHN6hb8Q)\\n\\n关于36氪\\n\\n[城市合作](https://36kr.com/station-business)\\n[寻求报道](https://36kr.com/seek-report-new)\\n我要入驻\\n[投资者关系](http://ir.36kr.com/)\\n\\n商务合作\\n[关于我们](https://36kr.com/pages/about)\\n联系我们\\n[加入我们](https://zhaopin.36kr.com/)\\n[36氪欧洲站](https://eu.36kr.com/) [36氪欧洲站](https://eu.36kr.com/zh) [36氪欧洲站](https://eu.36kr.com/de) [Ai产品日报](https://www.aicpb.com/)\\n\\n[网络谣言信息举报入口](https://36kr.com/refute-rumor-notice)\\n\\n热门推荐\\n\\n[热门资讯](https://36kr.com/hot-list/catalog)\\n[热门产品](https://36kr.com/project)\\n[文章标签](https://36kr.com/tags)\\n[快讯标签](https://36kr.com/nftags)\\n\\n合作伙伴\\n\\n- [![阿里云](https://36kr.com/information/AI/)](https://www.aliyun.com/)\\n- [![火山引擎](https://36kr.com/information/AI/)](https://www.volcengine.cn/)\\n- ![高德](https://36kr.com/information/AI/)\\n- [![个推](https://36kr.com/information/AI/)](https://www.getui.com/cn/index.html)\\n- [![星球日报](https://36kr.com/information/AI/)](https://www.odaily.com/)\\n- [![鲸准](https://36kr.com/information/AI/)](https://www.jingdata.com/)\\n- [![氪空间](https://36kr.com/information/AI/)](https://www.krspace.cn/)\\n- [![富途牛牛](https://36kr.com/information/AI/)](https://www.futunn.com/)\\n- [![企服点评](https://36kr.com/information/AI/)](https://www.36dianping.com/pk/)\\n- [![人人都是产品经理](https://36kr.com/information/AI/)](http://www.woshipm.com/)\\n- [![领氪](https://36kr.com/information/AI/)](https://www.36linkr.com/)\\n\\n36氪APP下载\\n\\n![](https://static.36krcdn.com/36kr-web/static/code_production.72d61993.png)\\n\\niOS & Android\\n\\n[![36氪](https://36kr.com/information/AI/)](https://36kr.com/)\\n\\n本站由\\xa0[阿里云](https://www.aliyun.com/) 提供计算与安全服务\\xa0违法和不良信息、未成年人保护举报电话：010-89650707\\xa0举报邮箱：jubao@36kr.com\\xa0[网上有害信息举报](https://www.12377.cn/)\\n\\n© 2011~2025 北京多氪信息科技有限公司 \\\\|[京ICP备12031756号-6](https://beian.miit.gov.cn/#/Integrated/index) \\\\| [京ICP证150143号](https://dxzhgl.miit.gov.cn/dxxzsp/xkz/xkzgl/resource/qiyesearch.jsp?num=%25E5%258C%2597%25E4%25BA%25AC%25E5%25A4%259A%25E6%25B0%25AA%25E4%25BF%25A1%25E6%2581%25AF%25E7%25A7%2591%25E6%258A%2580%25E6%259C%2589%25E9%2599%2590%25E5%2585%25AC%25E5%258F%25B8&type=xuke) \\\\| [京公网安备11010502036099号](http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=11010502036099)\\n\\n[意见反馈](https://36kr.com/mform/1755983296602372)\\n\\n![](https://static.36krcdn.com/36kr-web/static/code_production.72d61993.png)\\n\\n36氪APP让一部分人先看到未来\\n\\n36氪\\n\\n鲸准\\n\\n氪空间\\n\\n![](https://static.36krcdn.com/36kr-web/static/kr.ad0c1158.jpg)\\n\\n推送和解读前沿、有料的科技创投资讯\\n\\n![](https://static.36krcdn.com/36kr-web/static/jingzhun.9a251862.jpg)\\n\\n一级市场金融信息和系统服务提供商\\n\\n![](https://static.36krcdn.com/36kr-web/static/krSpace.7efbe7d3.jpg)\\n\\n聚焦全球优秀创业者，项目融资率接近97%，领跑行业', 'metadata': {'url': 'https://36kr.com/information/AI/', 'ogUrl': 'https://36kr.com/information/AI/', 'title': '36氪_让一部分人先看到未来', 'og:url': ['https://36kr.com/information/AI/', 'https://36kr.com/information/AI/'], 'favicon': {}, 'og:type': ['article', 'article'], 'ogImage': 'https://img.36krcdn.com/20191024/v2_1571894049839_img_jpg', 'ogTitle': '36氪_让一部分人先看到未来', 'keywords': '商业,财经,创投,科技,消费,职场,人物,视频', 'og:image': 'https://img.36krcdn.com/20191024/v2_1571894049839_img_jpg', 'og:title': '36氪_让一部分人先看到未来', 'renderer': 'webkit', 'scrapeId': 'fcb8668e-15bc-4bc3-a178-c26c275776c3', 'viewport': 'width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0, viewport-fit=cover', 'sourceURL': 'https://36kr.com/information/AI/', 'statusCode': 200, 'description': '36氪为您提供创业资讯、科技新闻、投融资对接、股权投资、极速融资等创业服务，致力成为创业者可以依赖的创业服务平台，为创业者提供最好的产品和服务。', 'ogDescription': '36氪为您提供创业资讯、科技新闻、投融资对接、股权投资、极速融资等创业服务，致力成为创业者可以依赖的创业服务平台，为创业者提供最好的产品和服务。', 'publishedTime': '2025-02-20T14:38:07+08:00', 'og:description': '36氪为您提供创业资讯、科技新闻、投融资对接、股权投资、极速融资等创业服务，致力成为创业者可以依赖的创业服务平台，为创业者提供最好的产品和服务。', 'MobileOptimized': '320', 'HandheldFriendly': 'True', 'format-detection': 'telephone=no', 'applicable-device': 'pc', '360-site-verification': 'b14201a8708468357bd6fab4ad556b78', 'article:published_time': '2025-02-20T14:38:07+08:00', 'baidu-site-verification': 'ET7tYDCqIv', 'sogou_site_verification': '5ZZdpkOf3U', 'shenma-site-verification': '8d43b014716644e38cc8fff2051c47a0_1623316092', 'apple-mobile-web-app-title': 'Title', 'apple-mobile-web-app-capable': 'yes', 'apple-mobile-web-app-status-bar-style': 'black'}}]}\n"
     ]
    }
   ],
   "source": [
    "from firecrawl import FirecrawlApp\n",
    "\n",
    "app = FirecrawlApp(api_key= fire_api)\n",
    "\n",
    "# Crawl a website:\n",
    "crawl_status = app.crawl_url(\n",
    "  'https://36kr.com/information/AI/', \n",
    "  params={\n",
    "    'limit': 100, \n",
    "    'scrapeOptions': {'formats': ['markdown', 'links']}\n",
    "  },\n",
    "  poll_interval=30\n",
    ")\n",
    "print(crawl_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2547846997.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    snscrape twitter-user textfiles\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "snscrape twitter-user textfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FileFinder' object has no attribute 'find_module'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msnscrape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtwitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msntwitter\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/主业/Minerva/Minerva Agent/.venv/lib/python3.12/site-packages/snscrape/modules/__init__.py:17\u001b[0m\n\u001b[1;32m     13\u001b[0m \t\tmodule \u001b[38;5;241m=\u001b[39m importer\u001b[38;5;241m.\u001b[39mfind_module(moduleName)\u001b[38;5;241m.\u001b[39mload_module(moduleName)\n\u001b[1;32m     14\u001b[0m \t\t\u001b[38;5;28mglobals\u001b[39m()[moduleNameWithoutPrefix] \u001b[38;5;241m=\u001b[39m module\n\u001b[0;32m---> 17\u001b[0m \u001b[43m_import_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/主业/Minerva/Minerva Agent/.venv/lib/python3.12/site-packages/snscrape/modules/__init__.py:13\u001b[0m, in \u001b[0;36m_import_modules\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m moduleNameWithoutPrefix \u001b[38;5;241m=\u001b[39m moduleName[prefixLen:]\n\u001b[1;32m     12\u001b[0m __all__\u001b[38;5;241m.\u001b[39mappend(moduleNameWithoutPrefix)\n\u001b[0;32m---> 13\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimporter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_module\u001b[49m(moduleName)\u001b[38;5;241m.\u001b[39mload_module(moduleName)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mglobals\u001b[39m()[moduleNameWithoutPrefix] \u001b[38;5;241m=\u001b[39m module\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FileFinder' object has no attribute 'find_module'"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sntwitter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb scraping\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m \u001b[43msntwitter\u001b[49m\u001b[38;5;241m.\u001b[39mTwitterSearchScraper(query)\u001b[38;5;241m.\u001b[39mget_items():\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mvars\u001b[39m(tweet))\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sntwitter' is not defined"
     ]
    }
   ],
   "source": [
    "query = \"web scraping\"\n",
    "for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "    print(vars(tweet))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    <!DOCTYPE html>\n",
      "    <head>\n",
      "      <title>x.com</title>\n",
      "      <meta http-equiv=\"refresh\" content=\"0; url = https://twitter.com/x/migrate?tok=7b2265223a222f646565707365656b5f6169222c2274223a313734303033383739347d902d408c5b04c91794ef7384aa79f35c\" />\n",
      "      <meta charset=\"utf-8\">\n",
      "      <meta name=\"viewport\" content=\"width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0,viewport-fit=cover\">\n",
      "\n",
      "      <link rel=\"preconnect\" href=\"//abs.twimg.com\">\n",
      "      <link rel=\"dns-prefetch\" href=\"//abs.twimg.com\">\n",
      "      <link rel=\"preconnect\" href=\"//api.twitter.com\">\n",
      "      <link rel=\"dns-prefetch\" href=\"//api.twitter.com\">\n",
      "      <link rel=\"preconnect\" href=\"//api.x.com\">\n",
      "      <link rel=\"dns-prefetch\" href=\"//api.x.com\">\n",
      "      <link rel=\"preconnect\" href=\"//pbs.twimg.com\">\n",
      "      <link rel=\"dns-prefetch\" href=\"//pbs.twimg.com\">\n",
      "      <link rel=\"preconnect\" href=\"//t.co\">\n",
      "      <link rel=\"dns-prefetch\" href=\"//t.co\">\n",
      "      <meta http-equiv=\"onion-location\" content=\"https://twitter3e4tixl4xyajtrzo62zg5vztmjuricljdp2c5kshju4avyoid.onion/\" />\n",
      "      <meta property=\"fb:app_id\" content=\"2231777543\" />\n",
      "      <meta content=\"X (formerly Twitter)\" property=\"og:site_name\" />\n",
      "      <meta name=\"google-site-verification\" content=\"600dQ0pZYsH2xOFt4hYmf5f5NpjCbWE_qk5Y04dErYM\" />\n",
      "      <meta name=\"facebook-domain-verification\" content=\"x6sdcc8b5ju3bh8nbm59eswogvg6t1\" />\n",
      "      <meta name=\"mobile-web-app-capable\" content=\"yes\" />\n",
      "      <meta name=\"apple-mobile-web-app-title\" content=\"Twitter\" />\n",
      "      <meta name=\"apple-mobile-web-app-status-bar-style\" content=\"white\" />\n",
      "      <link rel=\"search\" type=\"application/opensearchdescription+xml\" href=\"/os-x.xml\" title=\"X\"/>\n",
      "      <link rel=\"apple-touch-icon\" sizes=\"192x192\" href=\"https://abs.twimg.com/responsive-web/client-web/icon-ios.77d25eba.png\" />\n",
      "      <meta name=\"twitter-site-verification\" content=\"AUVDWo1JpbjI22xjTe5JOvTAWuW9bK41CpxYxCeCjH97mEVp7rtiHcvdOaUksJrG\" />\n",
      "      <link rel=\"manifest\" href=\"/manifest.json\" crossorigin=\"use-credentials\" />\n",
      "      <link rel=\"mask-icon\" sizes=\"any\" href=\"https://abs.twimg.com/responsive-web/client-web/icon-svg.ea5ff4aa.svg\" color=\"#1D9BF0\" />\n",
      "      <link rel=\"shortcut icon\" href=\"https://abs.twimg.com/favicons/twitter-pip.3.ico\" />\n",
      "      <meta name=\"theme-color\" content=\"#000000\" />\n",
      "      <script type=\"text/javascript\" charset=\"utf-8\" nonce=\"MWZhMjlmZjQtMGM2OS00YjQwLTkxOGUtZjVjNWI3ZjM3NzFj\">document.location = \"https://twitter.com/x/migrate?tok=7b2265223a222f646565707365656b5f6169222c2274223a313734303033383739347d902d408c5b04c91794ef7384aa79f35c\"</script>\n",
      "      </head>\n",
      "    <body style=\"background: #000\">\n",
      "\n",
      "    </body>\n",
      "    </html>\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "x_api = \"1c0f5dde67b64b004b59f85cac01e14d\"\n",
    "\n",
    "import requests\n",
    "\n",
    "payload = { 'api_key': '1c0f5dde67b64b004b59f85cac01e14d', 'url': 'https://x.com/deepseek_ai' }\n",
    "r = requests.get('https://api.scraperapi.com/', params=payload)\n",
    "print(r.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping raw HTML from qbitai.com...\n",
      "Failed to retrieve content from qbitai.com. HTTP Status Code: 403\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Scraping raw HTML from jiqizhixin.com...\n",
      "Failed to retrieve content from jiqizhixin.com. HTTP Status Code: 403\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Scraping raw HTML from lilianweng.github.io...\n",
      "Raw HTML retrieved from lilianweng.github.io. Analyzing with Qwen LLM...\n",
      "\n"
     ]
    },
    {
     "ename": "ProxyError",
     "evalue": "HTTPSConnectionPool(host='dashscope.aliyuncs.com', port=443): Max retries exceeded with url: /api/v1/services/aigc/text-generation/generation (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/主业/Minerva/Minerva Agent/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/主业/Minerva/Minerva Agent/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/主业/Minerva/Minerva Agent/.venv/lib/python3.12/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1427\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:300\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote end closed connection without\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    301\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mProxyError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;31mProxyError\u001b[0m: ('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response'))",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/主业/Minerva/Minerva Agent/.venv/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/主业/Minerva/Minerva Agent/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/主业/Minerva/Minerva Agent/.venv/lib/python3.12/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='dashscope.aliyuncs.com', port=443): Max retries exceeded with url: /api/v1/services/aigc/text-generation/generation (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProxyError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw HTML retrieved from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msite\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Analyzing with Qwen LLM...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m     qwen_analysis \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_with_qwen\u001b[49m\u001b[43m(\u001b[49m\u001b[43msite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_html\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalysis from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msite\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mqwen_analysis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 33\u001b[0m, in \u001b[0;36manalyze_with_qwen\u001b[0;34m(domain, raw_html)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21manalyze_with_qwen\u001b[39m(domain, raw_html):\n\u001b[1;32m     19\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     20\u001b[0m         {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou are a web researcher. Analyze the raw HTML content and extract key topics in the following format: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1. Title | Description | Website\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[1;32m     21\u001b[0m         {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m'''\u001b[39m}\n\u001b[1;32m     31\u001b[0m     ]\n\u001b[0;32m---> 33\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mdashscope\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msk-1a28c3fcc7e044cbacd6faf47dc89755\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqwen-max\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_search\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/主业/Minerva/Minerva Agent/.venv/lib/python3.12/site-packages/dashscope/aigc/generation.py:138\u001b[0m, in \u001b[0;36mGeneration.call\u001b[0;34m(cls, model, prompt, history, api_key, messages, plugins, workspace, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m headers\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28minput\u001b[39m, parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_build_input_parameters(\n\u001b[1;32m    137\u001b[0m     model, prompt, history, messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 138\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtask_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mworkspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m is_stream \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_stream:\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/主业/Minerva/Minerva Agent/.venv/lib/python3.12/site-packages/dashscope/client/base_api.py:145\u001b[0m, in \u001b[0;36mBaseApi.call\u001b[0;34m(cls, model, input, task_group, task, function, api_key, workspace, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m request \u001b[38;5;241m=\u001b[39m _build_api_request(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    138\u001b[0m                              \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    139\u001b[0m                              task_group\u001b[38;5;241m=\u001b[39mtask_group,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m                              api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[1;32m    143\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# call request service.\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/主业/Minerva/Minerva Agent/.venv/lib/python3.12/site-packages/dashscope/api_entities/http_request.py:83\u001b[0m, in \u001b[0;36mHttpRequest.call\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m response)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28mnext\u001b[39m(response)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/主业/Minerva/Minerva Agent/.venv/lib/python3.12/site-packages/dashscope/api_entities/http_request.py:302\u001b[0m, in \u001b[0;36mHttpRequest._handle_request\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    301\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(e)\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/主业/Minerva/Minerva Agent/.venv/lib/python3.12/site-packages/dashscope/api_entities/http_request.py:285\u001b[0m, in \u001b[0;36mHttpRequest._handle_request\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    284\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRequest body: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m obj)\n\u001b[0;32m--> 285\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m HTTPMethod\u001b[38;5;241m.\u001b[39mGET:\n\u001b[1;32m    291\u001b[0m     response \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mget(url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m    292\u001b[0m                            params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mparameters,\n\u001b[1;32m    293\u001b[0m                            headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    294\u001b[0m                            timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/主业/Minerva/Minerva Agent/.venv/lib/python3.12/site-packages/requests/sessions.py:637\u001b[0m, in \u001b[0;36mSession.post\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    627\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/主业/Minerva/Minerva Agent/.venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/主业/Minerva/Minerva Agent/.venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/主业/Minerva/Minerva Agent/.venv/lib/python3.12/site-packages/requests/adapters.py:694\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _ProxyError):\n\u001b[0;32m--> 694\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProxyError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mProxyError\u001b[0m: HTTPSConnectionPool(host='dashscope.aliyuncs.com', port=443): Max retries exceeded with url: /api/v1/services/aigc/text-generation/generation (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import datetime  # Import datetime module\n",
    "import dashscope\n",
    "\n",
    "# Function to get raw HTML content from the websites\n",
    "def get_raw_html(domain):\n",
    "    try:\n",
    "        # Send request to the website and get the raw HTML content\n",
    "        response = requests.get(f'http://{domain}')\n",
    "        if response.status_code == 200:\n",
    "            return response.content\n",
    "        else:\n",
    "            return f\"Failed to retrieve content from {domain}. HTTP Status Code: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error while fetching content from {domain}: {str(e)}\"\n",
    "\n",
    "# Function to prepare the message for Qwen LLM analysis\n",
    "def analyze_with_qwen(domain, raw_html):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'You are a web researcher. Analyze the raw HTML content and extract key topics in the following format: \"1. Title | Description | Website\"'},\n",
    "        {'role': 'user', 'content': f'''\n",
    "        Analyze the raw HTML content from {domain} and provide the latest 10 topics with:\n",
    "        1. Article titles in English\n",
    "        2. Article titles in Chinese\n",
    "        3. One-line descriptions in English\n",
    "        4. One-line descriptions in Chinese\n",
    "        5. Website name\n",
    "        Use current date: {datetime.date.today()}.\n",
    "        HTML Content: {raw_html.decode('utf-8')}\n",
    "        '''}\n",
    "    ]\n",
    "\n",
    "    response = dashscope.Generation.call(\n",
    "        api_key=\"sk-1a28c3fcc7e044cbacd6faf47dc89755\",\n",
    "        model=\"qwen-max\",\n",
    "        messages=messages,\n",
    "        enable_search=True,\n",
    "        result_format='message'\n",
    "    )\n",
    "    return response['output']['choices'][0]['message']['content']\n",
    "\n",
    "# List of websites to scrape\n",
    "websites = [\n",
    "    \"qbitai.com\",\n",
    "    \"jiqizhixin.com\",\n",
    "    \"lilianweng.github.io\",\n",
    "    \"x.com/deepseek_ai\"\n",
    "]\n",
    "\n",
    "# Scraping and analyzing websites\n",
    "for site in websites:\n",
    "    print(f\"Scraping raw HTML from {site}...\")\n",
    "    raw_html = get_raw_html(site)\n",
    "    \n",
    "    # Check if there was an error\n",
    "    if isinstance(raw_html, str) and ('Error' in raw_html or 'Failed' in raw_html):\n",
    "        print(raw_html)\n",
    "    else:\n",
    "        print(f\"Raw HTML retrieved from {site}. Analyzing with Qwen LLM...\\n\")\n",
    "        qwen_analysis = analyze_with_qwen(site, raw_html)\n",
    "        print(f\"Analysis from {site}:\\n{qwen_analysis}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "import datetime\n",
    "import dashscope\n",
    "\n",
    "# Function to get raw HTML content from the websites\n",
    "def get_raw_html(domain):\n",
    "    try:\n",
    "        # Send request to the website and get the raw HTML content\n",
    "        response = requests.get(f'http://{domain}')\n",
    "        if response.status_code == 200:\n",
    "            return response.content\n",
    "        else:\n",
    "            return f\"Failed to retrieve content from {domain}. HTTP Status Code: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error while fetching content from {domain}: {str(e)}\"\n",
    "\n",
    "# Function to prepare the message for Qwen LLM analysis\n",
    "def analyze_with_qwen(domain, raw_html):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'You are a web researcher. Analyze the raw HTML content and extract key topics in the following format: \"1. Title | Description | Website\"'},\n",
    "        {'role': 'user', 'content': f'''\n",
    "        Analyze the raw HTML content from {domain} and provide the latest 10 topics with:\n",
    "        1. Article titles in English\n",
    "        2. Article titles in Chinese\n",
    "        3. One-line descriptions in English\n",
    "        4. One-line descriptions in Chinese\n",
    "        5. Website name\n",
    "        Use current date: {datetime.date.today()}.\n",
    "        HTML Content: {raw_html.decode('utf-8')}\n",
    "        '''}\n",
    "    ]\n",
    "\n",
    "    response = dashscope.Generation.call(\n",
    "        api_key=\"sk-1a28c3fcc7e044cbacd6faf47dc89755\",\n",
    "        model=\"qwen-max\",\n",
    "        messages=messages,\n",
    "        enable_search=True,\n",
    "        result_format='message'\n",
    "    )\n",
    "    return response['output']['choices'][0]['message']['content']\n",
    "\n",
    "# Streamlit UI components\n",
    "st.title(\"Minerva Agent\")   \n",
    "\n",
    "# List of default websites\n",
    "default_websites = [\n",
    "    \"lilianweng.github.io\",\n",
    "    \"qbitai.com\",\n",
    "    \"jiqizhixin.com\",\n",
    "    \"x.com/deepseek_ai\"\n",
    "]\n",
    "\n",
    "# Input for user to add websites\n",
    "input_websites = st.text_area(\"Website Domains(, Seperated)\", \n",
    "                              value=', '.join(default_websites), \n",
    "                              height=100)\n",
    "\n",
    "# Convert input string to a list of websites\n",
    "websites = [site.strip() for site in input_websites.split(',')]\n",
    "\n",
    "# Display results\n",
    "for site in websites:\n",
    "    st.write(f\"### Pulling {site}...\")\n",
    "    \n",
    "    # Get raw HTML\n",
    "    raw_html = get_raw_html(site)\n",
    "    \n",
    "    # Check if there was an error\n",
    "    if isinstance(raw_html, str) and ('Error' in raw_html or 'Failed' in raw_html):\n",
    "        st.error(raw_html)\n",
    "    else:\n",
    "        st.write(f\"Raw HTML retrieved from {site}. Analyzing with Qwen LLM...\\n\")\n",
    "        \n",
    "        # Perform Qwen analysis\n",
    "        qwen_analysis = analyze_with_qwen(site, raw_html)\n",
    "        \n",
    "        # Display results\n",
    "        st.write(f\"### {site} Summary:\\n\")\n",
    "        st.text_area(f\" {site}\", qwen_analysis, height=300)\n",
    "\n",
    "    st.markdown(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://r.jina.ai/https://example.com'\n",
    "headers = {\n",
    "'Authorization': 'Bearer jina_26a656e516224ce28e71cc3b28fa7b07zUchXe4_MJ_935m8SpS9-TNGL--w'\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradio==4.32.0\n",
    "faiss-cpu==1.8.0.post1\n",
    "dashscope==1.20.4\n",
    "openai==1.55.3\n",
    "httpx==0.27.0\n",
    "llama-index-vector-stores-faiss==0.1.2\n",
    "llama-index-embeddings-dashscope==0.1.4\n",
    "llama-index-readers-file==0.1.33\n",
    "matplotlib==3.9.3\n",
    "docx2txt==0.8\n",
    "openpyxl==3.1.5\n",
    "llama-index-core==0.10.67\n",
    "uvicorn==0.30.6\n",
    "fastapi==0.112.0\n",
    "llama-index-postprocessor-dashscope-rerank-custom==0.1.0\n",
    "simplejson==3.19.3\n",
    "modelscope==1.18.0\n",
    "langchain_community==0.2.16\n",
    "transformers==4.44.2\n",
    "llama_index.embeddings.huggingface==0.2.3\n",
    "llama-index-embeddings-langchain==0.1.2\n",
    "datasets==2.21.0\n",
    "oss2==2.19.0\n",
    "sortedcontainers==2.4.0\n",
    "addict==2.4.0s\n",
    "config\n",
    "selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_with_qwen(domain, raw_html):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'You are a professional AI researcher. Analyze the raw HTML content and extract key topics in the following format: \"1. Description | Website\"'},\n",
    "        {'role': 'user', 'content': f'''\n",
    "        Analyze the raw HTML content from {domain} and provide the latest 10 topics with:\n",
    "        1. Article titles in Chinese\n",
    "        2. One-line descriptions in Chinese\n",
    "        3. Website name\n",
    "        Use current date: {datetime.date.today()}.\n",
    "        HTML Content: {raw_html}\n",
    "        '''}\n",
    "    ]\n",
    "    response = dashscope.Generation.call(\n",
    "        api_key=\"sk-1a28c3fcc7e044cbacd6faf47dc89755\",\n",
    "        model=\"qwen-turbo\",\n",
    "        messages=messages,\n",
    "        enable_search=True,\n",
    "        result_format='message'\n",
    "    )\n",
    "    return response['output']['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    twitter_handles = [\n",
    "        \"sama\",               # Sam Altman\n",
    "        \"ylecun\",             # Yann LeCun\n",
    "        \"AndrewYNg\",          # Andrew Ng\n",
    "        \"fchollet\",           # François Chollet\n",
    "        \"_KarenHao\",          # Karen Hao\n",
    "        \"karpathy\",           # Andrej Karpathy\n",
    "        \"SchmidhuberAI\",      # Jürgen Schmidhuber\n",
    "        \"sarahookr\",          # Sara Hooker\n",
    "        \"demishassabis\",      # Demis Hassabis\n",
    "        \"saranormous\",        # Sarah Guo\n",
    "        \"hardmaru\",           # David Hardmaru\n",
    "        \"lilianweng\",         # Lilian Weng\n",
    "        \"OriolVinyalsML\",     # Oriol Vinyals\n",
    "        \"Michael_J_Black\",    # Michael Black\n",
    "        \"JeffDean\",           # Jeff Dean\n",
    "        \"goodfellow_ian\",     # Ian Goodfellow\n",
    "        \"achowdhery\",         # Aakanksha Chowdhery\n",
    "        \"PeterDiamandis\",     # Peter H. Diamandis\n",
    "        \"GaryMarcus\",         # Gary Marcus\n",
    "        \"giffmana\",           # Lucas Beyer\n",
    "        \"rasbt\",              # Sebastian Raschka\n",
    "        \"quaesita\",           # Cassie Kozyrkov\n",
    "        \"KateKayeReports\",    # Kate Kaye\n",
    "        \"EMostaque\",          # Emad\n",
    "        \"drfeifei\",           # Fei-Fei Li\n",
    "        \"DrJimFan\",           # Jim Fan\n",
    "        \"omarsar0\",           # Elvis Saravia\n",
    "        \"conniechan\",         # Connie Chan\n",
    "        \"hugo_larochelle\",    # Hugo Larochelle\n",
    "        \"benjedwards\",        # Benj Edwards\n",
    "        \"rebecca_szkutak\",    # Becca Szkutak\n",
    "        \"svlevine\",           # Sergey Levine\n",
    "        \"ericschmidt\",        # Eric Schmidt\n",
    "        \"ilyasut\",            # Ilya Sutskever\n",
    "        \"patrickmineault\",    # Patrick Mineault\n",
    "        \"natashajaques\",      # Natasha Jaques\n",
    "        \"pabbeel\",            # Pieter Abbeel\n",
    "        \"ESYudkowsky\",        # Eliezer Yudkowsky\n",
    "        \"geoffreyhinton\",     # Geoffrey Hinton\n",
    "        \"wintonARK\",          # Brett Winton\n",
    "        \"jeffclune\",          # Jeff Clune\n",
    "        \"RamaswmySridhar\",    # Sridhar Ramaswamy\n",
    "        \"bentossell\",         # Ben Tossell\n",
    "        \"johnschulman2\",      # John Schulman\n",
    "        \"_akhaliq\",           # Ahsen Khaliq\n",
    "        \"quocleix\",           # Quoc Le\n",
    "        \"jackclarkSF\",        # Jack Clark\n",
    "        \"mervenoyann\",        # merve\n",
    "        \"DavidSHolz\",         # David\n",
    "        \"natolambert\",        # Nathan Lambert\n",
    "        \"RichardSocher\",      # Richard Socher\n",
    "        \"mustafasuleymn\",     # Mustafa Suleyman\n",
    "        \"ZoubinGhahrama1\",    # Zoubin Ghahramani\n",
    "        \"nathanbenaich\",      # Nathan Benaich\n",
    "        \"johnvmcdonnell\",     # John McDonnell\n",
    "        \"tunguz\",             # Bojan Tunguz\n",
    "        \"bengoertzel\",        # Ben Goertzel\n",
    "        \"ch402\",              # Chris Olah\n",
    "        \"Kseniase_\",          # Ksenia Se\n",
    "        \"paulg\",              # Paul Graham\n",
    "        \"rsalakhu\",           # Russ Salakhutdinov\n",
    "        \"gdb\",                # Greg Brockman\n",
    "        \"vivnat\",             # Vivek Natarajan\n",
    "        \"bxchen\",             # Brian X. Chen\n",
    "        \"AnimaAnandkumar\",    # Anima Anandkumar\n",
    "        \"JeffreyTowson\",      # Jeffrey Towson\n",
    "        \"Thom_Wolf\",          # Thomas Wolf\n",
    "        \"johnplattml\",        # John Platt\n",
    "        \"SamanyouGarg\",       # Samanyou Garg\n",
    "        \"KirkDBorne\",         # Kirk Bourne\n",
    "        \"Alber_RomGar\",       # Alberto Romero\n",
    "        \"SilverJacket\",       # Matthew Hutson\n",
    "        \"ecsquendor\",         # Tim Scarfe\n",
    "        \"jordnb\",             # Jordan Burgess\n",
    "        \"jluan\",              # David Luan\n",
    "        \"NPCollapse\",         # Connor Leahy\n",
    "        \"NaveenGRao\",         # Naveen Rao\n",
    "        \"azeem\",              # Azeem Azhar\n",
    "        \"Suhail\",             # Suhail Doshi\n",
    "        \"maxjaderberg\",       # Max Jaderberg\n",
    "        \"Kyle_L_Wiggers\",     # Kyle Wiggers\n",
    "        \"cocoweixu\",          # Wei Xu\n",
    "        \"aidangomezzz\",       # Aidan Gomez\n",
    "        \"alexandr_wang\",      # Alexandr Wang\n",
    "        \"CaimingXiong\",       # Caiming Xiong\n",
    "        \"YiMaTweets\",         # Yi Ma\n",
    "        \"notmisha\",           # Misha Denil\n",
    "        \"peteratmsr\",         # Peter Lee\n",
    "        \"shivon\",             # Shivon Zilis\n",
    "        \"jackyliang42\",       # Jacky Liang\n",
    "        \"v_vashishta\",        # Vin Vashishta\n",
    "        \"xdh\",                # Xuedong Huang\n",
    "        \"FryRsquared\",        # Hannah Fry\n",
    "        \"ravi_lsvp\",          # Ravi Mhatre\n",
    "        \"ClementDelangue\",    # clem\n",
    "        \"oh_that_hat\",        # Hattie Zhou\n",
    "        \"sapna\",              # Sapna Maheshwari\n",
    "        \"VRLalchand\",         # Vidhi Lalchand\n",
    "        \"svpino\",             # Santiago L Valdarrama\n",
    "        \"ceobillionaire\",     # Vincent Boucher\n",
    "        \"ykilcher\",           # Yannic Kilcher\n",
    "        \"BornsteinMatt\",      # Matt Bornstein\n",
    "        \"lachygroom\",         # Lachy Groom\n",
    "        \"goodside\",           # Riley Goodside\n",
    "        \"amasad\",             # Amjad Masad\n",
    "        \"polynoamial\",        # Noam Brown\n",
    "        \"sytelus\",            # Shital Shah\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeids:\n",
    "量子位: MzIzNjc1NzUzMw==\n",
    "机器之心: MzA3MzI4MjgzMw==\n",
    "海外独角兽: Mzg2OTY0MDk0NQ==\n",
    "五源资本: MzkwMDI2ODE0OQ==\n",
    "Z Lives: MzkxMzY0NzU3Mg==\n",
    "Z Potentials: MzI4NTgxMDk1NA==\n",
    "晚点Auto: MzkzMDMyNDIxNQ==\n",
    "硅兔赛跑: MzI4MDUzMTc3Mg==\n",
    "纪源资本: MjM5MTk3NTYyMA==\n",
    "Founder Park: Mzg5NTc0MjgwMw==\n",
    "甲子光年: MzU5OTI0NTc3Mg==\n",
    "新智元: MzI3MTA0MTk1MA==\n",
    "M小姐研习录: MzUzNTEyNjc0OA==\n",
    "吴说Real: MzI0ODgzMDE5MA==\n",
    "小丸子酱聊商业: Mzg4ODkwNjA3OQ==\n",
    "林坤的学习思考: Mzg3ODU2OTMzMA==\n",
    "线性资本: MzAwNTAyMDAyNQ==\n",
    "36氪pro: MzUxOTA3MzMzOQ==\n",
    "乌鸦智能说: MzkyNTY1MjE2OA==\n",
    "Redbot: Mzg5ODg0ODMyMg==\n",
    "AI寒武纪: Mzg3MTkxMjYzOA==\n",
    "数字生命卡兹克: MzIyMzA5NjEyMA==\n",
    "AI-paperdaily: MzAxMzQyMzU5Nw=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeid = \"MzA3MzI4MjgzMw==\"\n",
    "token = \"232435096\"\n",
    "cookie = \"appmsglist_action_3918520287=card; rewardsn=; wxtokenkey=777; pac_uid=0_WFRCchx025Cww; _qimei_uuid42=191090f32121004a40ded1e5e650d9677d9210f8fb; _qimei_h38=e963446740ded1e5e650d96703000003119109; _qimei_q36=; ua_id=fIxXt1qo3N1AkUI9AAAAAE7bKLDM8dls2W8RivxiLs4=; wxuin=36409384414630; suid=user_0_WFRCchx025Cww; mm_lang=zh_CN; sig=h016ff31a4a1ff5262376ab723fd8d807ea82f9552e933b7d087ca0bd6cd2ce703cdaaf9f90ae8c1544; ab.storage.deviceId.f9c2b69f-2136-44e0-a55a-dff72d99aa19=g%3AYoJZqng6gcdvly5aBDxZqgiJ1GZ2%7Ce%3Aundefined%7Cc%3A1739462526631%7Cl%3A1739462526631; ab.storage.sessionId.f9c2b69f-2136-44e0-a55a-dff72d99aa19=g%3A7bafc696-6715-6e6b-565d-5695541d32ca%7Ce%3A1739464326655%7Cc%3A1739462526656%7Cl%3A1739462526656; _qimei_fingerprint=d91ba6f60a0e30a68c3644052fa00145; uuid=fa6efb10c0815d39c692922e8b0421d3; _clck=1mr2pfh|1|ftw|0; xid=dac261de760022f125b820a3532dd7e8; slave_sid=bUdyR0VlenBWcFNaeGt6T25uWEhYUXd1VTBheWtUVWxFdWZIVHhWSE1rTnRtaEtBUGlQb0hDMmV1NzRoS29qOXVjREJoZlBfckdtNGpNTk4xS1YxeEF3WTdrZGpKU01HelFNZm94VTFBeVFYSXhPWVN1MUJPNGdwWTJPVXR1SkFwcnJGR3RqR3JFTE1UVkgz; slave_user=gh_b89c7dbe2d0d; rand_info=CAESIIrl/lpNzhAVVw0EkwiG8FU/r1+wtjeHfL3PXReBWbqM; slave_bizuin=3918520287; bizuin=3918520287; _clsk=1332kwo|1740969186856|5|1|mp.weixin.qq.com/weheat-agent/payload/record\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the latest 5 articles...\n",
      "Fetching HTML for: 特朗普发文推动国家加密货币储备：提及 BTC ETH XRP SOL ADA\n",
      "Fetching HTML for: 危急时刻：Bybit CEO Ben 揭秘 15 亿美元被盗时的关键决策\n",
      "Fetching HTML for: 吴说周精选：SEC 终止对多家加密公司调查、比特币跌破八万、OKX 与美国和解与新闻 Top10\n",
      "Saved HTML content for 3 articles\n",
      "Data extraction completed successfully!\n",
      "\n",
      "Results summary:\n",
      "Retrieved 3 articles\n",
      "                                           title      create_time                                   html_file\n",
      "          特朗普发文推动国家加密货币储备：提及 BTC ETH XRP SOL ADA 2025-03-03 07:34 article_特朗普发文推动国家加密货币储备_提及_BTC_ETH_XRP.html\n",
      "            危急时刻：Bybit CEO Ben 揭秘 15 亿美元被盗时的关键决策 2025-03-02 08:53 article_危急时刻_Bybit_CEO_Ben_揭秘_15_亿美元被盗.html\n",
      "吴说周精选：SEC 终止对多家加密公司调查、比特币跌破八万、OKX 与美国和解与新闻 Top10 2025-03-01 08:53 article_吴说周精选_SEC_终止对多家加密公司调查_比特币跌破八万_.html\n"
     ]
    }
   ],
   "source": [
    "# 目标url\n",
    "url = \"https://mp.weixin.qq.com/cgi-bin/appmsg\"\n",
    "cookie = \"appmsglist_action_3918520287=card; rewardsn=; wxtokenkey=777; pac_uid=0_WFRCchx025Cww; _qimei_uuid42=191090f32121004a40ded1e5e650d9677d9210f8fb; _qimei_h38=e963446740ded1e5e650d96703000003119109; _qimei_q36=; ua_id=fIxXt1qo3N1AkUI9AAAAAE7bKLDM8dls2W8RivxiLs4=; wxuin=36409384414630; suid=user_0_WFRCchx025Cww; mm_lang=zh_CN; sig=h016ff31a4a1ff5262376ab723fd8d807ea82f9552e933b7d087ca0bd6cd2ce703cdaaf9f90ae8c1544; ab.storage.deviceId.f9c2b69f-2136-44e0-a55a-dff72d99aa19=g%3AYoJZqng6gcdvly5aBDxZqgiJ1GZ2%7Ce%3Aundefined%7Cc%3A1739462526631%7Cl%3A1739462526631; ab.storage.sessionId.f9c2b69f-2136-44e0-a55a-dff72d99aa19=g%3A7bafc696-6715-6e6b-565d-5695541d32ca%7Ce%3A1739464326655%7Cc%3A1739462526656%7Cl%3A1739462526656; _qimei_fingerprint=d91ba6f60a0e30a68c3644052fa00145; uuid=fa6efb10c0815d39c692922e8b0421d3; _clck=1mr2pfh|1|ftw|0; xid=dac261de760022f125b820a3532dd7e8; slave_sid=bUdyR0VlenBWcFNaeGt6T25uWEhYUXd1VTBheWtUVWxFdWZIVHhWSE1rTnRtaEtBUGlQb0hDMmV1NzRoS29qOXVjREJoZlBfckdtNGpNTk4xS1YxeEF3WTdrZGpKU01HelFNZm94VTFBeVFYSXhPWVN1MUJPNGdwWTJPVXR1SkFwcnJGR3RqR3JFTE1UVkgz; slave_user=gh_b89c7dbe2d0d; rand_info=CAESIIrl/lpNzhAVVw0EkwiG8FU/r1+wtjeHfL3PXReBWbqM; slave_bizuin=3918520287; bizuin=3918520287; _clsk=1332kwo|1740969186856|5|1|mp.weixin.qq.com/weheat-agent/payload/record\"\n",
    "\n",
    "headers = {\n",
    "    \"Cookie\": cookie,\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.75 Mobile Safari/537.36\",\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"token\": \"232435096\",\n",
    "    \"lang\": \"zh_CN\",\n",
    "    \"f\": \"json\",\n",
    "    \"ajax\": \"1\",\n",
    "    \"action\": \"list_ex\",\n",
    "    \"begin\": \"0\",\n",
    "    \"count\": \"5\",\n",
    "    \"query\": \"\",\n",
    "    \"fakeid\": \"MzI0ODgzMDE5MA==\", \n",
    "    \"type\": \"9\",\n",
    "}\n",
    "\n",
    "import requests\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_total_count():\n",
    "    content_json = requests.get(url, headers=headers, params=data).json()\n",
    "    count = int(content_json[\"app_msg_cnt\"])\n",
    "    return count\n",
    "\n",
    "\n",
    "def get_content_list():\n",
    "    # Only get the latest 5 articles\n",
    "    data[\"begin\"] = 0\n",
    "    data[\"count\"] = 3\n",
    "    \n",
    "    content_json = requests.get(url, headers=headers, params=data).json()\n",
    "    content_list = content_json[\"app_msg_list\"]\n",
    "    \n",
    "    # Save to json\n",
    "    with open(\"content_list.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(content_list, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    return content_list\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def get_raw_html(full_url):\n",
    "    try:\n",
    "        # Use Jina to get the raw HTML\n",
    "        jina_url = f'https://r.jina.ai/{full_url}'\n",
    "        headers = {\n",
    "            'Authorization': 'Bearer jina_26a656e516224ce28e71cc3b28fa7b07zUchXe4_MJ_935m8SpS9-TNGL--w'\n",
    "        }\n",
    "        response = requests.get(jina_url, headers=headers)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        response.raise_for_status()  # Raises an exception for 4XX/5XX responses\n",
    "        \n",
    "        # Return the raw HTML content\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error while fetching content from {full_url}: {str(e)}\"\n",
    "\n",
    "def process_content_list(content_list):\n",
    "    results_list = []\n",
    "    \n",
    "    for item in content_list:\n",
    "        title = item[\"title\"]\n",
    "        link = item[\"link\"]\n",
    "        create_time = time.strftime(\"%Y-%m-%d %H:%M\", time.localtime(item[\"create_time\"]))\n",
    "        \n",
    "        # Get raw HTML using the full URL\n",
    "        print(f\"Fetching HTML for: {title}\")\n",
    "        html_content = get_raw_html(link)\n",
    "        \n",
    "        # Save HTML content to file\n",
    "        filename = f\"article_{re.sub(r'[^\\w]', '_', title)[:30]}.html\"\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        results_list.append([title, link, create_time, filename])\n",
    "        \n",
    "        # Add a delay between requests\n",
    "        time.sleep(random.randint(3, 5))\n",
    "    \n",
    "    name = ['title', 'link', 'create_time', 'html_file']\n",
    "    data = pd.DataFrame(columns=name, data=results_list)\n",
    "    data.to_csv(\"data.csv\", mode='w', encoding='utf-8')\n",
    "    print(f\"Saved HTML content for {len(content_list)} articles\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"Getting the latest 5 articles...\")\n",
    "        content_list = get_content_list()\n",
    "        \n",
    "        # Process and get HTML for the 5 articles\n",
    "        process_content_list(content_list)\n",
    "        print(\"Data extraction completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    print(\"\\nResults summary:\")\n",
    "    try:\n",
    "        df = pd.read_csv(\"data.csv\")\n",
    "        print(f\"Retrieved {len(df)} articles\")\n",
    "        print(df[['title', 'create_time', 'html_file']][:5].to_string(index=False))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not display results: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting WeChat Official Account Scraper\n",
      "\n",
      "==================================================\n",
      "Processing account: 量子位 (fakeid: MzIzNjc1NzUzMw==)\n",
      "==================================================\n",
      "Error getting content list, response: {'base_resp': {'ret': 200003, 'err_msg': 'invalid session'}}\n",
      "Switched to account 2\n",
      "Switched to account 1\n",
      "Waiting 11.22 seconds before processing next account...\n",
      "\n",
      "==================================================\n",
      "Processing account: 机器之心 (fakeid: MzA3MzI4MjgzMw==)\n",
      "==================================================\n",
      "Error getting content list, response: {'base_resp': {'ret': 200003, 'err_msg': 'invalid session'}}\n",
      "Switched to account 2\n",
      "Switched to account 1\n",
      "Waiting 10.18 seconds before processing next account...\n",
      "\n",
      "==================================================\n",
      "Processing account: 海外独角兽 (fakeid: Mzg2OTY0MDk0NQ==)\n",
      "==================================================\n",
      "Error getting content list, response: {'base_resp': {'ret': 200003, 'err_msg': 'invalid session'}}\n",
      "Switched to account 2\n",
      "Switched to account 1\n",
      "Waiting 13.09 seconds before processing next account...\n",
      "\n",
      "==================================================\n",
      "Processing account: 五源资本 (fakeid: MzkwMDI2ODE0OQ==)\n",
      "==================================================\n",
      "Error getting content list, response: {'base_resp': {'ret': 200003, 'err_msg': 'invalid session'}}\n",
      "Switched to account 2\n",
      "Switched to account 1\n",
      "Waiting 13.57 seconds before processing next account...\n",
      "\n",
      "==================================================\n",
      "Processing account: Z Lives (fakeid: MzkxMzY0NzU3Mg==)\n",
      "==================================================\n",
      "Error getting content list, response: {'base_resp': {'ret': 200003, 'err_msg': 'invalid session'}}\n",
      "Switched to account 2\n",
      "Switched to account 1\n",
      "Waiting 11.80 seconds before processing next account...\n",
      "\n",
      "==================================================\n",
      "Processing account: Z Potentials (fakeid: MzI4NTgxMDk1NA==)\n",
      "==================================================\n",
      "Error getting content list, response: {'base_resp': {'ret': 200003, 'err_msg': 'invalid session'}}\n",
      "Switched to account 2\n",
      "Switched to account 1\n",
      "Waiting 13.05 seconds before processing next account...\n",
      "\n",
      "Final Results summary:\n",
      "Could not display results: [Errno 2] No such file or directory: 'all_accounts_data.csv'\n",
      "\n",
      "Scraping completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "class WeChatScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://mp.weixin.qq.com/cgi-bin/appmsg\"\n",
    "        \n",
    "        # Create directory for storing HTML content\n",
    "        os.makedirs(\"html_content\", exist_ok=True)\n",
    "        os.makedirs(\"account_data\", exist_ok=True)\n",
    "        \n",
    "        # Account configurations with different cookies/user-agents\n",
    "        self.account_configs = [\n",
    "            {\n",
    "                \"cookie\": \"appmsglist_action_3918520287=card; rewardsn=; wxtokenkey=777; pac_uid=0_WFRCchx025Cww; *qimei*uuid42=191090f32121004a40ded1e5e650d9677d9210f8fb; *qimei*h38=e963446740ded1e5e650d96703000003119109; *qimei*q36=; ua_id=fIxXt1qo3N1AkUI9AAAAAE7bKLDM8dls2W8RivxiLs4=; wxuin=36409384414630; suid=user_0_WFRCchx025Cww; mm_lang=zh_CN; sig=h016ff31a4a1ff5262376ab723fd8d807ea82f9552e933b7d087ca0bd6cd2ce703cdaaf9f90ae8c1544; ab.storage.deviceId.f9c2b69f-2136-44e0-a55a-dff72d99aa19=g%3AYoJZqng6gcdvly5aBDxZqgiJ1GZ2%7Ce%3Aundefined%7Cc%3A1739462526631%7Cl%3A1739462526631; ab.storage.sessionId.f9c2b69f-2136-44e0-a55a-dff72d99aa19=g%3A7bafc696-6715-6e6b-565d-5695541d32ca%7Ce%3A1739464326655%7Cc%3A1739462526656%7Cl%3A1739462526656; *qimei*fingerprint=d91ba6f60a0e30a68c3644052fa00145; uuid=fa6efb10c0815d39c692922e8b0421d3; *clck=1mr2pfh|1|ftw|0; xid=dac261de760022f125b820a3532dd7e8; slave*sid=bUdyR0VlenBWcFNaeGt6T25uWEhYUXd1VTBheWtUVWxFdWZIVHhWSE1rTnRtaEtBUGlQb0hDMmV1NzRoS29qOXVjREJoZlBfckdtNGpNTk4xS1YxeEF3WTdrZGpKU01HelFNZm94VTFBeVFYSXhPWVN1MUJPNGdwWTJPVXR1SkFwcnJGR3RqR3JFTE1UVkgz; slave_user=gh_b89c7dbe2d0d; rand_info=CAESIIrl/lpNzhAVVw0EkwiG8FU/r1+wtjeHfL3PXReBWbqM; slave_bizuin=3918520287; bizuin=3918520287; _clsk=1332kwo|1740969186856|5|1|mp.weixin.qq.com/weheat-agent/payload/record\",\n",
    "                \"user_agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.75 Mobile Safari/537.36\",\n",
    "                \"token\": \"232435096\"\n",
    "            },\n",
    "            # Add more account configs here with different cookies and user agents\n",
    "            # This is an example of a second account (you would need to replace with real credentials)\n",
    "            {\n",
    "                \"cookie\": \"appmsglist_action_3918520287=card; rewardsn=; wxtokenkey=777; pac_uid=0_WFRCchx025Cww; *qimei*uuid42=191090f32121004a40ded1e5e650d9677d9210f8fb; *qimei*h38=e963446740ded1e5e650d96703000003119109; *qimei*q36=; ua_id=fIxXt1qo3N1AkUI9AAAAAE7bKLDM8dls2W8RivxiLs4=; wxuin=36409384414630; suid=user_0_WFRCchx025Cww; mm_lang=zh_CN; sig=h016ff31a4a1ff5262376ab723fd8d807ea82f9552e933b7d087ca0bd6cd2ce703cdaaf9f90ae8c1544; ab.storage.deviceId.f9c2b69f-2136-44e0-a55a-dff72d99aa19=g%3AYoJZqng6gcdvly5aBDxZqgiJ1GZ2%7Ce%3Aundefined%7Cc%3A1739462526631%7Cl%3A1739462526631; ab.storage.sessionId.f9c2b69f-2136-44e0-a55a-dff72d99aa19=g%3A7bafc696-6715-6e6b-565d-5695541d32ca%7Ce%3A1739464326655%7Cc%3A1739462526656%7Cl%3A1739462526656; *qimei*fingerprint=d91ba6f60a0e30a68c3644052fa00145; uuid=fa6efb10c0815d39c692922e8b0421d3; *clck=1mr2pfh|1|ftw|0; xid=dac261de760022f125b820a3532dd7e8; slave*sid=bUdyR0VlenBWcFNaeGt6T25uWEhYUXd1VTBheWtUVWxFdWZIVHhWSE1rTnRtaEtBUGlQb0hDMmV1NzRoS29qOXVjREJoZlBfckdtNGpNTk4xS1YxeEF3WTdrZGpKU01HelFNZm94VTFBeVFYSXhPWVN1MUJPNGdwWTJPVXR1SkFwcnJGR3RqR3JFTE1UVkgz; slave_user=gh_b89c7dbe2d0d; rand_info=CAESIIrl/lpNzhAVVw0EkwiG8FU/r1+wtjeHfL3PXReBWbqM; slave_bizuin=3918520287; bizuin=3918520287; _clsk=1332kwo|1740969186856|5|1|mp.weixin.qq.com/weheat-agent/payload/record\",\n",
    "                \"user_agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "                \"token\": \"232435096\"  # You might need to update this token for each account\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Dictionary to map account names to their fake IDs\n",
    "        self.fakeid_pool = {\n",
    "            \"量子位\": \"MzIzNjc1NzUzMw==\",\n",
    "            \"机器之心\": \"MzA3MzI4MjgzMw==\",\n",
    "            \"海外独角兽\": \"Mzg2OTY0MDk0NQ==\"\n",
    "        }\n",
    "        \n",
    "        # Jina API key for fetching raw HTML\n",
    "        self.jina_api_key = \"jina_26a656e516224ce28e71cc3b28fa7b07zUchXe4_MJ_935m8SpS9-TNGL--w\"\n",
    "        \n",
    "        # Current account index for rotation\n",
    "        self.current_account = 0\n",
    "        \n",
    "    def rotate_account(self):\n",
    "        \"\"\"Rotate to the next account to avoid being blocked\"\"\"\n",
    "        self.current_account = (self.current_account + 1) % len(self.account_configs)\n",
    "        print(f\"Switched to account {self.current_account + 1}\")\n",
    "        # Add a delay between account switches\n",
    "        time.sleep(random.uniform(5, 10))\n",
    "        \n",
    "    def get_headers_and_data(self, fakeid):\n",
    "        \"\"\"Get headers and data for the current account\"\"\"\n",
    "        account = self.account_configs[self.current_account]\n",
    "        \n",
    "        headers = {\n",
    "            \"Cookie\": account[\"cookie\"],\n",
    "            \"User-Agent\": account[\"user_agent\"],\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"token\": account[\"token\"],\n",
    "            \"lang\": \"zh_CN\",\n",
    "            \"f\": \"json\",\n",
    "            \"ajax\": \"1\",\n",
    "            \"action\": \"list_ex\",\n",
    "            \"begin\": \"0\",\n",
    "            \"count\": \"5\",\n",
    "            \"query\": \"\",\n",
    "            \"fakeid\": fakeid,  # The fakeid for the specific account we're scraping\n",
    "            \"type\": \"9\",\n",
    "        }\n",
    "        \n",
    "        return headers, data\n",
    "    \n",
    "    def get_total_count(self, fakeid):\n",
    "        \"\"\"Get the total count of articles for a given fakeid\"\"\"\n",
    "        headers, data = self.get_headers_and_data(fakeid)\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(self.base_url, headers=headers, params=data)\n",
    "            content_json = response.json()\n",
    "            \n",
    "            if \"app_msg_cnt\" not in content_json:\n",
    "                print(f\"Error getting count, response: {content_json}\")\n",
    "                return 0\n",
    "                \n",
    "            count = int(content_json[\"app_msg_cnt\"])\n",
    "            return count\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting total count: {e}\")\n",
    "            # Rotate account if there's an error\n",
    "            self.rotate_account()\n",
    "            return 0\n",
    "    \n",
    "    def get_content_list(self, account_name, fakeid):\n",
    "        \"\"\"Get the latest 5 articles for a given fakeid\"\"\"\n",
    "        headers, data = self.get_headers_and_data(fakeid)\n",
    "        \n",
    "        # Reset to get the latest 5 articles\n",
    "        data[\"begin\"] = 0\n",
    "        data[\"count\"] = 5\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(self.base_url, headers=headers, params=data)\n",
    "            content_json = response.json()\n",
    "            \n",
    "            if \"app_msg_list\" not in content_json:\n",
    "                print(f\"Error getting content list, response: {content_json}\")\n",
    "                # Rotate account if there's an error\n",
    "                self.rotate_account()\n",
    "                return []\n",
    "            \n",
    "            content_list = content_json[\"app_msg_list\"]\n",
    "            \n",
    "            # Save to JSON with account name\n",
    "            output_file = f\"account_data/{account_name}_content_list.json\"\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(content_list, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "            print(f\"Retrieved {len(content_list)} articles for {account_name}\")\n",
    "            return content_list\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting content list for {account_name}: {e}\")\n",
    "            # Rotate account if there's an error\n",
    "            self.rotate_account()\n",
    "            return []\n",
    "    \n",
    "    def get_raw_html(self, full_url):\n",
    "        \"\"\"Get the raw HTML content using Jina API\"\"\"\n",
    "        try:\n",
    "            # Use Jina to get the raw HTML\n",
    "            jina_url = f'https://r.jina.ai/{full_url}'\n",
    "            headers = {\n",
    "                'Authorization': f'Bearer {self.jina_api_key}'\n",
    "            }\n",
    "            response = requests.get(jina_url, headers=headers)\n",
    "            \n",
    "            # Check if the request was successful\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Return the raw HTML content\n",
    "            return response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error while fetching content from {full_url}: {str(e)}\")\n",
    "            return f\"Error while fetching content from {full_url}: {str(e)}\"\n",
    "    \n",
    "    def process_content_list(self, account_name, content_list):\n",
    "        \"\"\"Process the content list and save HTML content\"\"\"\n",
    "        results_list = []\n",
    "        \n",
    "        for item in tqdm(content_list, desc=f\"Processing {account_name} articles\"):\n",
    "            title = item[\"title\"]\n",
    "            link = item[\"link\"]\n",
    "            create_time = time.strftime(\"%Y-%m-%d %H:%M\", time.localtime(item[\"create_time\"]))\n",
    "            \n",
    "            # Create a safe filename\n",
    "            safe_title = re.sub(r'[^\\w]', '_', title)[:30]\n",
    "            filename = f\"html_content/{account_name}_{safe_title}.html\"\n",
    "            \n",
    "            # Get raw HTML using the full URL\n",
    "            print(f\"Fetching HTML for: {title}\")\n",
    "            html_content = self.get_raw_html(link)\n",
    "            \n",
    "            # Save HTML content to file\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(html_content)\n",
    "            \n",
    "            results_list.append([account_name, title, link, create_time, filename])\n",
    "            \n",
    "            # Add a random delay between requests to avoid detection\n",
    "            time.sleep(random.uniform(3, 7))\n",
    "        \n",
    "        # Save to CSV with account name\n",
    "        if results_list:\n",
    "            name = ['account', 'title', 'link', 'create_time', 'html_file']\n",
    "            data = pd.DataFrame(columns=name, data=results_list)\n",
    "            data.to_csv(f\"account_data/{account_name}_data.csv\", mode='w', encoding='utf-8', index=False)\n",
    "            print(f\"Saved HTML content for {len(content_list)} articles from {account_name}\")\n",
    "        \n",
    "        return results_list\n",
    "    \n",
    "    def scrape_all_accounts(self):\n",
    "        \"\"\"Scrape all accounts in the fakeid pool\"\"\"\n",
    "        all_results = []\n",
    "        \n",
    "        for account_name, fakeid in self.fakeid_pool.items():\n",
    "            print(f\"\\n{'=' * 50}\")\n",
    "            print(f\"Processing account: {account_name} (fakeid: {fakeid})\")\n",
    "            print(f\"{'=' * 50}\")\n",
    "            \n",
    "            # Get the content list for this account\n",
    "            content_list = self.get_content_list(account_name, fakeid)\n",
    "            \n",
    "            if content_list:\n",
    "                # Process the content list\n",
    "                results = self.process_content_list(account_name, content_list)\n",
    "                all_results.extend(results)\n",
    "            \n",
    "            # Rotate account after each account is processed\n",
    "            self.rotate_account()\n",
    "            \n",
    "            # Add a longer delay between different accounts\n",
    "            delay = random.uniform(10, 15)\n",
    "            print(f\"Waiting {delay:.2f} seconds before processing next account...\")\n",
    "            time.sleep(delay)\n",
    "        \n",
    "        # Combine all results into a single CSV\n",
    "        if all_results:\n",
    "            name = ['account', 'title', 'link', 'create_time', 'html_file']\n",
    "            combined_data = pd.DataFrame(columns=name, data=all_results)\n",
    "            combined_data.to_csv(\"all_accounts_data.csv\", mode='w', encoding='utf-8', index=False)\n",
    "            print(f\"\\nTotal articles scraped: {len(all_results)}\")\n",
    "            \n",
    "            # Display a summary\n",
    "            summary = combined_data.groupby('account').size().reset_index(name='count')\n",
    "            print(\"\\nSummary by account:\")\n",
    "            print(summary.to_string(index=False))\n",
    "        \n",
    "        return all_results\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"Starting WeChat Official Account Scraper\")\n",
    "        scraper = WeChatScraper()\n",
    "        \n",
    "        # Scrape all accounts\n",
    "        results = scraper.scrape_all_accounts()\n",
    "        \n",
    "        print(\"\\nFinal Results summary:\")\n",
    "        try:\n",
    "            df = pd.read_csv(\"all_accounts_data.csv\")\n",
    "            print(f\"Total retrieved articles: {len(df)}\")\n",
    "            print(\"\\nLatest articles by account:\")\n",
    "            \n",
    "            # Display the latest article from each account\n",
    "            latest_by_account = df.groupby('account').first().reset_index()\n",
    "            print(latest_by_account[['account', 'title', 'create_time']][:10].to_string(index=False))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Could not display results: {e}\")\n",
    "        \n",
    "        print(\"\\nScraping completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3Dall%3Aelectron%26id_list%3D%26start%3D0%26max_results%3D1\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=all:electron&amp;id_list=&amp;start=0&amp;max_results=1</title>\\n  <id>http://arxiv.org/api/cHxbiOdZaP56ODnBPIenZhzg5f8</id>\\n  <updated>2025-03-03T00:00:00-05:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">221168</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/cond-mat/0102536v1</id>\\n    <updated>2001-02-28T20:12:09Z</updated>\\n    <published>2001-02-28T20:12:09Z</published>\\n    <title>Impact of Electron-Electron Cusp on Configuration Interaction Energies</title>\\n    <summary>  The effect of the electron-electron cusp on the convergence of configuration\\ninteraction (CI) wave functions is examined. By analogy with the\\npseudopotential approach for electron-ion interactions, an effective\\nelectron-electron interaction is developed which closely reproduces the\\nscattering of the Coulomb interaction but is smooth and finite at zero\\nelectron-electron separation. The exact many-electron wave function for this\\nsmooth effective interaction has no cusp at zero electron-electron separation.\\nWe perform CI and quantum Monte Carlo calculations for He and Be atoms, both\\nwith the Coulomb electron-electron interaction and with the smooth effective\\nelectron-electron interaction. We find that convergence of the CI expansion of\\nthe wave function for the smooth electron-electron interaction is not\\nsignificantly improved compared with that for the divergent Coulomb interaction\\nfor energy differences on the order of 1 mHartree. This shows that, contrary to\\npopular belief, description of the electron-electron cusp is not a limiting\\nfactor, to within chemical accuracy, for CI calculations.\\n</summary>\\n    <author>\\n      <name>David Prendergast</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Department of Physics</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>M. Nolan</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NMRC, University College, Cork, Ireland</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Claudia Filippi</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Department of Physics</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Stephen Fahy</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Department of Physics</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>J. C. Greer</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NMRC, University College, Cork, Ireland</arxiv:affiliation>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1063/1.1383585</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1063/1.1383585\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">11 pages, 6 figures, 3 tables, LaTeX209, submitted to The Journal of\\n  Chemical Physics</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">J. Chem. Phys. 115, 1626 (2001)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/cond-mat/0102536v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cond-mat/0102536v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cond-mat.str-el\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.str-el\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = 'http://export.arxiv.org/api/query?search_query=all:electron&start=0&max_results=1'\n",
    "dataarxiv = urllib.request.urlopen(url).read()\n",
    "print(dataarxiv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting apify_client\n",
      "  Using cached apify_client-1.9.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting apify-shared>=1.1.2 (from apify_client)\n",
      "  Using cached apify_shared-1.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: httpx>=0.25 in ./.venv/lib/python3.12/site-packages (from apify_client) (0.28.1)\n",
      "Collecting more_itertools>=10.0.0 (from apify_client)\n",
      "  Using cached more_itertools-10.6.0-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.12/site-packages (from httpx>=0.25->apify_client) (4.8.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx>=0.25->apify_client) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx>=0.25->apify_client) (1.0.7)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.12/site-packages (from httpx>=0.25->apify_client) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.25->apify_client) (0.14.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.12/site-packages (from anyio->httpx>=0.25->apify_client) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in ./.venv/lib/python3.12/site-packages (from anyio->httpx>=0.25->apify_client) (4.12.2)\n",
      "Using cached apify_client-1.9.2-py3-none-any.whl (72 kB)\n",
      "Using cached apify_shared-1.2.1-py3-none-any.whl (12 kB)\n",
      "Using cached more_itertools-10.6.0-py3-none-any.whl (63 kB)\n",
      "Installing collected packages: more_itertools, apify-shared, apify_client\n",
      "Successfully installed apify-shared-1.2.1 apify_client-1.9.2 more_itertools-10.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install apify_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching tweets from 2025-02-28 to 2025-03-03 for 107 Twitter handles...\n",
      "\n",
      "Total tweets collected: 10000\n",
      "Data collected for 1 authors:\n",
      "  - @unknown: 10000 tweets\n",
      "\n",
      "Data saved to ai_influencers_tweets_2025-02-28_to_2025-03-03.json\n",
      "Engagement summary exported to ai_influencers_engagement_2025-02-28_to_2025-03-03.csv\n"
     ]
    }
   ],
   "source": [
    "from apify_client import ApifyClient\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# Initialize the ApifyClient with your API token\n",
    "client = ApifyClient(\"apify_api_kbKxE4fYwbZOMBA30gS7DkbjinZqy91SEHb9\")\n",
    "\n",
    "# List of all Twitter handles for AI influencers\n",
    "twitter_handles = [\n",
    "    \"sama\",             # Sam Altman\n",
    "    \"ylecun\",           # Yann LeCun\n",
    "    \"AndrewYNg\",        # Andrew Ng\n",
    "    \"fchollet\",         # François Chollet\n",
    "    \"_KarenHao\",        # Karen Hao 郝珂灵\n",
    "    \"karpathy\",         # Andrej Karpathy\n",
    "    \"SchmidhuberAI\",    # Jürgen Schmidhuber\n",
    "    \"sarahookr\",        # Sara Hooker\n",
    "    \"demishassabis\",    # Demis Hassabis\n",
    "    \"saranormous\",      # Sarah Guo\n",
    "    \"hardmaru\",         # David Hardmaru\n",
    "    \"lilianweng\",       # Lilian Weng\n",
    "    \"OriolVinyalsML\",   # Oriol Vinyals\n",
    "    \"Michael_J_Black\",  # Michael Black\n",
    "    \"JeffDean\",         # Jeff Dean\n",
    "    \"goodfellow_ian\",   # Ian Goodfellow\n",
    "    \"achowdhery\",       # Aakanksha Chowdhery\n",
    "    \"PeterDiamandis\",   # Peter H. Diamandis\n",
    "    \"GaryMarcus\",       # Gary Marcus\n",
    "    \"giffmana\",         # Lucas Beyer\n",
    "    \"rasbt\",            # Sebastian Raschka\n",
    "    \"quaesita\",         # Cassie Kozyrkov\n",
    "    \"KateKayeReports\",  # Kate Kaye\n",
    "    \"EMostaque\",        # Emad\n",
    "    \"drfeifei\",         # Fei-Fei Li\n",
    "    \"DrJimFan\",         # Jim Fan\n",
    "    \"omarsar0\",         # Elvis Saravia\n",
    "    \"conniechan\",       # Connie Chan\n",
    "    \"hugo_larochelle\",  # Hugo Larochelle\n",
    "    \"benjedwards\",      # Benj Edwards\n",
    "    \"rebecca_szkutak\",  # Becca Szkutak\n",
    "    \"svlevine\",         # Sergey Levine\n",
    "    \"ericschmidt\",      # Eric Schmidt\n",
    "    \"ilyasut\",          # Ilya Sutskever\n",
    "    \"patrickmineault\",  # Patrick Mineault\n",
    "    \"natashajaques\",    # Natasha Jaques\n",
    "    \"pabbeel\",          # Pieter Abbeel\n",
    "    \"ESYudkowsky\",      # Eliezer Yudkowsky\n",
    "    \"geoffreyhinton\",   # Geoffrey Hinton\n",
    "    \"wintonARK\",        # Brett Winton\n",
    "    \"jeffclune\",        # Jeff Clune\n",
    "    \"RamaswmySridhar\",  # Sridhar Ramaswamy\n",
    "    \"bentossell\",       # Ben Tossell\n",
    "    \"johnschulman2\",    # John Schulman\n",
    "    \"_akhaliq\",         # Ahsen Khaliq\n",
    "    \"quocleix\",         # Quoc Le\n",
    "    \"jackclarkSF\",      # Jack Clark\n",
    "    \"mervenoyann\",      # merve\n",
    "    \"DavidSHolz\",       # David\n",
    "    \"natolambert\",      # Nathan Lambert\n",
    "    \"RichardSocher\",    # Richard Socher\n",
    "    \"mustafasuleymn\",   # Mustafa Suleyman\n",
    "    \"ZoubinGhahrama1\",  # Zoubin Ghahramani\n",
    "    \"nathanbenaich\",    # Nathan Benaich\n",
    "    \"johnvmcdonnell\",   # John McDonnell\n",
    "    \"tunguz\",           # Bojan Tunguz\n",
    "    \"bengoertzel\",      # Ben Goertzel\n",
    "    \"ch402\",            # Chris Olah\n",
    "    \"Kseniase_\",        # Ksenia Se\n",
    "    \"paulg\",            # Paul Graham\n",
    "    \"rsalakhu\",         # Russ Salakhutdinov\n",
    "    \"gdb\",              # Greg Brockman\n",
    "    \"vivnat\",           # Vivek Natarajan\n",
    "    \"bxchen\",           # Brian X. Chen\n",
    "    \"AnimaAnandkumar\",  # Anima Anandkumar\n",
    "    \"JeffreyTowson\",    # Jeffrey Towson 陶迅\n",
    "    \"Thom_Wolf\",        # Thomas Wolf\n",
    "    \"johnplattml\",      # John Platt\n",
    "    \"SamanyouGarg\",     # Samanyou Garg\n",
    "    \"KirkDBorne\",       # Kirk Bourne\n",
    "    \"Alber_RomGar\",     # Alberto Romero\n",
    "    \"SilverJacket\",     # Matthew Hutson\n",
    "    \"ecsquendor\",       # Tim Scarfe\n",
    "    \"jordnb\",           # Jordan Burgess\n",
    "    \"jluan\",            # David Luan\n",
    "    \"NPCollapse\",       # Connor Leahy\n",
    "    \"NaveenGRao\",       # Naveen Rao\n",
    "    \"azeem\",            # Azeem Azhar\n",
    "    \"Suhail\",           # Suhail Doshi\n",
    "    \"maxjaderberg\",     # Max Jaderberg\n",
    "    \"Kyle_L_Wiggers\",   # Kyle Wiggers\n",
    "    \"cocoweixu\",        # Wei Xu\n",
    "    \"aidangomezzz\",     # Aidan Gomez\n",
    "    \"alexandr_wang\",    # Alexandr Wang\n",
    "    \"CaimingXiong\",     # Caiming Xiong\n",
    "    \"YiMaTweets\",       # Yi Ma\n",
    "    \"notmisha\",         # Misha Denil\n",
    "    \"peteratmsr\",       # Peter Lee\n",
    "    \"shivon\",           # Shivon Zilis\n",
    "    \"jackyliang42\",     # Jacky Liang\n",
    "    \"v_vashishta\",      # Vin Vashishta\n",
    "    \"xdh\",              # Xuedong Huang\n",
    "    \"FryRsquared\",      # Hannah Fry\n",
    "    \"ravi_lsvp\",        # Ravi Mhatre\n",
    "    \"ClementDelangue\",  # clem\n",
    "    \"oh_that_hat\",      # Hattie Zhou\n",
    "    \"sapna\",            # Sapna Maheshwari\n",
    "    \"VRLalchand\",       # Vidhi Lalchand\n",
    "    \"svpino\",           # Santiago L Valdarrama\n",
    "    \"ceobillionaire\",   # Vincent Boucher\n",
    "    \"ykilcher\",         # Yannic Kilcher\n",
    "    \"BornsteinMatt\",    # Matt Bornstein\n",
    "    \"lachygroom\",       # Lachy Groom\n",
    "    \"goodside\",         # Riley Goodside\n",
    "    \"amasad\",           # Amjad Masad\n",
    "    \"polynoamial\",      # Noam Brown\n",
    "    \"sytelus\",          # Shital Shah\n",
    "]\n",
    "\n",
    "# Calculate the date 3 days ago from today\n",
    "three_days_ago = (datetime.now() - timedelta(days=3)).strftime('%Y-%m-%d')\n",
    "today = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Prepare the Actor input\n",
    "run_input = {\n",
    "    \"twitterHandles\": twitter_handles,\n",
    "    \"maxItems\": 10000,  # Set a high number to ensure we get all tweets\n",
    "    \"sort\": \"Latest\",\n",
    "    \"start\": three_days_ago,\n",
    "    \"end\": today,\n",
    "}\n",
    "\n",
    "# Run the Actor and wait for it to finish\n",
    "print(f\"Fetching tweets from {three_days_ago} to {today} for {len(twitter_handles)} Twitter handles...\")\n",
    "run = client.actor(\"61RPP7dywgiy0JPD0\").call(run_input=run_input)\n",
    "\n",
    "# Process and save the results\n",
    "tweets_by_author = {}\n",
    "total_tweets = 0\n",
    "\n",
    "for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "    author = item.get(\"authorUsername\", \"unknown\")\n",
    "    \n",
    "    if author not in tweets_by_author:\n",
    "        tweets_by_author[author] = []\n",
    "    \n",
    "    # Extract relevant information\n",
    "    tweet_info = {\n",
    "        \"content\": item.get(\"content\", \"\"),\n",
    "        \"url\": item.get(\"url\", \"\"),\n",
    "        \"date\": item.get(\"date\", \"\"),\n",
    "        \"replies\": item.get(\"replyCount\", 0),\n",
    "        \"retweets\": item.get(\"retweetCount\", 0),\n",
    "        \"likes\": item.get(\"likeCount\", 0),\n",
    "        \"views\": item.get(\"viewCount\", 0),\n",
    "        \"is_reply\": item.get(\"isReply\", False),\n",
    "        \"is_retweet\": item.get(\"isRetweet\", False),\n",
    "        \"is_quote\": item.get(\"isQuote\", False),\n",
    "        \"media\": item.get(\"media\", [])\n",
    "    }\n",
    "    \n",
    "    tweets_by_author[author].append(tweet_info)\n",
    "    total_tweets += 1\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nTotal tweets collected: {total_tweets}\")\n",
    "print(f\"Data collected for {len(tweets_by_author)} authors:\")\n",
    "\n",
    "for author, tweets in sorted(tweets_by_author.items()):\n",
    "    print(f\"  - @{author}: {len(tweets)} tweets\")\n",
    "\n",
    "# Save the data to a JSON file\n",
    "output_file = f\"ai_influencers_tweets_{three_days_ago}_to_{today}.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(tweets_by_author, f, indent=2)\n",
    "\n",
    "print(f\"\\nData saved to {output_file}\")\n",
    "\n",
    "# Optional: Export CSV summary of engagement metrics\n",
    "import csv\n",
    "with open(f\"ai_influencers_engagement_{three_days_ago}_to_{today}.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    fieldnames = [\"username\", \"total_tweets\", \"avg_likes\", \"avg_retweets\", \"avg_replies\", \"max_likes\", \"max_retweets\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for author, tweets in tweets_by_author.items():\n",
    "        if not tweets:\n",
    "            continue\n",
    "            \n",
    "        total = len(tweets)\n",
    "        avg_likes = sum(t[\"likes\"] for t in tweets) / total if total > 0 else 0\n",
    "        avg_retweets = sum(t[\"retweets\"] for t in tweets) / total if total > 0 else 0\n",
    "        avg_replies = sum(t[\"replies\"] for t in tweets) / total if total > 0 else 0\n",
    "        max_likes = max((t[\"likes\"] for t in tweets), default=0)\n",
    "        max_retweets = max((t[\"retweets\"] for t in tweets), default=0)\n",
    "        \n",
    "        writer.writerow({\n",
    "            \"username\": author,\n",
    "            \"total_tweets\": total,\n",
    "            \"avg_likes\": round(avg_likes, 1),\n",
    "            \"avg_retweets\": round(avg_retweets, 1),\n",
    "            \"avg_replies\": round(avg_replies, 1),\n",
    "            \"max_likes\": max_likes,\n",
    "            \"max_retweets\": max_retweets\n",
    "        })\n",
    "    \n",
    "print(f\"Engagement summary exported to ai_influencers_engagement_{three_days_ago}_to_{today}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping tweets from 2025-02-28 to 2025-03-03\n",
      "[1/107] Scraping tweets for @sama...\n",
      "  Scraped 3 tweets for @sama\n",
      "  Waiting 5 seconds before next author...\n",
      "[2/107] Scraping tweets for @ylecun...\n",
      "  Scraped 1 tweets for @ylecun\n",
      "  Waiting 5 seconds before next author...\n",
      "[3/107] Scraping tweets for @AndrewYNg...\n",
      "  Scraped 1 tweets for @AndrewYNg\n",
      "  Waiting 5 seconds before next author...\n",
      "[4/107] Scraping tweets for @fchollet...\n",
      "  Scraped 2 tweets for @fchollet\n",
      "  Waiting 5 seconds before next author...\n",
      "[5/107] Scraping tweets for @_KarenHao...\n",
      "  Scraped 1 tweets for @_KarenHao\n",
      "  Waiting 5 seconds before next author...\n",
      "[6/107] Scraping tweets for @karpathy...\n",
      "  Scraped 10 tweets for @karpathy\n",
      "  Waiting 5 seconds before next author...\n",
      "[7/107] Scraping tweets for @SchmidhuberAI...\n",
      "  Scraped 3 tweets for @SchmidhuberAI\n",
      "  Waiting 5 seconds before next author...\n",
      "[8/107] Scraping tweets for @sarahookr...\n",
      "  Scraped 4 tweets for @sarahookr\n",
      "  Waiting 5 seconds before next author...\n",
      "[9/107] Scraping tweets for @demishassabis...\n",
      "  Scraped 1 tweets for @demishassabis\n",
      "  Waiting 5 seconds before next author...\n",
      "[10/107] Scraping tweets for @saranormous...\n",
      "  Scraped 11 tweets for @saranormous\n",
      "  Waiting 5 seconds before next author...\n",
      "[11/107] Scraping tweets for @hardmaru...\n",
      "  Scraped 2 tweets for @hardmaru\n",
      "  Waiting 5 seconds before next author...\n",
      "[12/107] Scraping tweets for @lilianweng...\n",
      "  Scraped 1 tweets for @lilianweng\n",
      "  Waiting 5 seconds before next author...\n",
      "[13/107] Scraping tweets for @OriolVinyalsML...\n",
      "  Scraped 1 tweets for @OriolVinyalsML\n",
      "  Waiting 5 seconds before next author...\n",
      "[14/107] Scraping tweets for @Michael_J_Black...\n",
      "  Scraped 1 tweets for @Michael_J_Black\n",
      "  Waiting 5 seconds before next author...\n",
      "[15/107] Scraping tweets for @JeffDean...\n",
      "  Scraped 6 tweets for @JeffDean\n",
      "  Waiting 5 seconds before next author...\n",
      "[16/107] Scraping tweets for @goodfellow_ian...\n",
      "  Scraped 7 tweets for @goodfellow_ian\n",
      "  Waiting 5 seconds before next author...\n",
      "[17/107] Scraping tweets for @achowdhery...\n",
      "  Scraped 1 tweets for @achowdhery\n",
      "  Waiting 5 seconds before next author...\n",
      "[18/107] Scraping tweets for @PeterDiamandis...\n",
      "  Scraped 8 tweets for @PeterDiamandis\n",
      "  Waiting 5 seconds before next author...\n",
      "[19/107] Scraping tweets for @GaryMarcus...\n",
      "  Scraped 50 tweets for @GaryMarcus\n",
      "  Waiting 5 seconds before next author...\n",
      "[20/107] Scraping tweets for @giffmana...\n",
      "  Scraped 44 tweets for @giffmana\n",
      "  Waiting 5 seconds before next author...\n",
      "[21/107] Scraping tweets for @rasbt...\n",
      "  Scraped 65 tweets for @rasbt\n",
      "  Waiting 5 seconds before next author...\n",
      "[22/107] Scraping tweets for @quaesita...\n",
      "  Scraped 1 tweets for @quaesita\n",
      "  Waiting 5 seconds before next author...\n",
      "[23/107] Scraping tweets for @KateKayeReports...\n",
      "  Scraped 1 tweets for @KateKayeReports\n",
      "  Waiting 5 seconds before next author...\n",
      "[24/107] Scraping tweets for @EMostaque...\n",
      "  Scraped 1 tweets for @EMostaque\n",
      "  Waiting 5 seconds before next author...\n",
      "[25/107] Scraping tweets for @drfeifei...\n",
      "  Scraped 2 tweets for @drfeifei\n",
      "  Waiting 5 seconds before next author...\n",
      "[26/107] Scraping tweets for @DrJimFan...\n",
      "  Scraped 1 tweets for @DrJimFan\n",
      "  Waiting 5 seconds before next author...\n",
      "[27/107] Scraping tweets for @omarsar0...\n",
      "  Scraped 25 tweets for @omarsar0\n",
      "  Waiting 5 seconds before next author...\n",
      "[28/107] Scraping tweets for @conniechan...\n",
      "  Scraped 1 tweets for @conniechan\n",
      "  Waiting 5 seconds before next author...\n",
      "[29/107] Scraping tweets for @hugo_larochelle...\n",
      "  Scraped 1 tweets for @hugo_larochelle\n",
      "  Waiting 5 seconds before next author...\n",
      "[30/107] Scraping tweets for @benjedwards...\n",
      "  Scraped 1 tweets for @benjedwards\n",
      "  Waiting 5 seconds before next author...\n",
      "[31/107] Scraping tweets for @rebecca_szkutak...\n",
      "  Scraped 3 tweets for @rebecca_szkutak\n",
      "  Waiting 5 seconds before next author...\n",
      "[32/107] Scraping tweets for @svlevine...\n",
      "  Scraped 1 tweets for @svlevine\n",
      "  Waiting 5 seconds before next author...\n",
      "[33/107] Scraping tweets for @ericschmidt...\n",
      "  Scraped 1 tweets for @ericschmidt\n",
      "  Waiting 5 seconds before next author...\n",
      "[34/107] Scraping tweets for @ilyasut...\n",
      "  Scraped 1 tweets for @ilyasut\n",
      "  Waiting 5 seconds before next author...\n",
      "[35/107] Scraping tweets for @patrickmineault...\n",
      "  Scraped 1 tweets for @patrickmineault\n",
      "  Waiting 5 seconds before next author...\n",
      "[36/107] Scraping tweets for @natashajaques...\n",
      "  Scraped 1 tweets for @natashajaques\n",
      "  Waiting 5 seconds before next author...\n",
      "[37/107] Scraping tweets for @pabbeel...\n",
      "  Scraped 1 tweets for @pabbeel\n",
      "  Waiting 5 seconds before next author...\n",
      "[38/107] Scraping tweets for @ESYudkowsky...\n",
      "  Scraped 68 tweets for @ESYudkowsky\n",
      "  Waiting 5 seconds before next author...\n",
      "[39/107] Scraping tweets for @geoffreyhinton...\n",
      "  Scraped 1 tweets for @geoffreyhinton\n",
      "  Waiting 5 seconds before next author...\n",
      "[40/107] Scraping tweets for @wintonARK...\n",
      "  Scraped 4 tweets for @wintonARK\n",
      "  Waiting 5 seconds before next author...\n",
      "[41/107] Scraping tweets for @jeffclune...\n",
      "  Scraped 3 tweets for @jeffclune\n",
      "  Waiting 5 seconds before next author...\n",
      "[42/107] Scraping tweets for @RamaswmySridhar...\n",
      "  Scraped 1 tweets for @RamaswmySridhar\n",
      "  Waiting 5 seconds before next author...\n",
      "[43/107] Scraping tweets for @bentossell...\n",
      "  Scraped 10 tweets for @bentossell\n",
      "  Waiting 5 seconds before next author...\n",
      "[44/107] Scraping tweets for @johnschulman2...\n",
      "  Scraped 1 tweets for @johnschulman2\n",
      "  Waiting 5 seconds before next author...\n",
      "[45/107] Scraping tweets for @_akhaliq...\n",
      "  Scraped 29 tweets for @_akhaliq\n",
      "  Waiting 5 seconds before next author...\n",
      "[46/107] Scraping tweets for @quocleix...\n",
      "  Scraped 1 tweets for @quocleix\n",
      "  Waiting 5 seconds before next author...\n",
      "[47/107] Scraping tweets for @jackclarkSF...\n",
      "  Scraped 8 tweets for @jackclarkSF\n",
      "  Waiting 5 seconds before next author...\n",
      "[48/107] Scraping tweets for @mervenoyann...\n",
      "  Scraped 14 tweets for @mervenoyann\n",
      "  Waiting 5 seconds before next author...\n",
      "[49/107] Scraping tweets for @DavidSHolz...\n",
      "  Scraped 5 tweets for @DavidSHolz\n",
      "  Waiting 5 seconds before next author...\n",
      "[50/107] Scraping tweets for @natolambert...\n",
      "  Scraped 11 tweets for @natolambert\n",
      "  Waiting 5 seconds before next author...\n",
      "[51/107] Scraping tweets for @RichardSocher...\n",
      "  Scraped 23 tweets for @RichardSocher\n",
      "  Waiting 5 seconds before next author...\n",
      "[52/107] Scraping tweets for @mustafasuleymn...\n",
      "  Scraped 1 tweets for @mustafasuleymn\n",
      "  Waiting 5 seconds before next author...\n",
      "[53/107] Scraping tweets for @ZoubinGhahrama1...\n",
      "  Scraped 1 tweets for @ZoubinGhahrama1\n",
      "  Waiting 5 seconds before next author...\n",
      "[54/107] Scraping tweets for @nathanbenaich...\n",
      "  Scraped 45 tweets for @nathanbenaich\n",
      "  Waiting 5 seconds before next author...\n",
      "[55/107] Scraping tweets for @johnvmcdonnell...\n",
      "  Scraped 8 tweets for @johnvmcdonnell\n",
      "  Waiting 5 seconds before next author...\n",
      "[56/107] Scraping tweets for @tunguz...\n",
      "  Scraped 100 tweets for @tunguz\n",
      "  Waiting 5 seconds before next author...\n",
      "[57/107] Scraping tweets for @bengoertzel...\n",
      "  Scraped 2 tweets for @bengoertzel\n",
      "  Waiting 5 seconds before next author...\n",
      "[58/107] Scraping tweets for @ch402...\n",
      "  Scraped 1 tweets for @ch402\n",
      "  Waiting 5 seconds before next author...\n",
      "[59/107] Scraping tweets for @Kseniase_...\n",
      "  Scraped 9 tweets for @Kseniase_\n",
      "  Waiting 5 seconds before next author...\n",
      "[60/107] Scraping tweets for @paulg...\n",
      "  Scraped 18 tweets for @paulg\n",
      "  Waiting 5 seconds before next author...\n",
      "[61/107] Scraping tweets for @rsalakhu...\n",
      "  Scraped 1 tweets for @rsalakhu\n",
      "  Waiting 5 seconds before next author...\n",
      "[62/107] Scraping tweets for @gdb...\n",
      "  Scraped 1 tweets for @gdb\n",
      "  Waiting 5 seconds before next author...\n",
      "[63/107] Scraping tweets for @vivnat...\n",
      "  Scraped 1 tweets for @vivnat\n",
      "  Waiting 5 seconds before next author...\n",
      "[64/107] Scraping tweets for @bxchen...\n",
      "  Scraped 1 tweets for @bxchen\n",
      "  Waiting 5 seconds before next author...\n",
      "[65/107] Scraping tweets for @AnimaAnandkumar...\n",
      "  Scraped 1 tweets for @AnimaAnandkumar\n",
      "  Waiting 5 seconds before next author...\n",
      "[66/107] Scraping tweets for @JeffreyTowson...\n",
      "  Scraped 9 tweets for @JeffreyTowson\n",
      "  Waiting 5 seconds before next author...\n",
      "[67/107] Scraping tweets for @Thom_Wolf...\n",
      "  Scraped 3 tweets for @Thom_Wolf\n",
      "  Waiting 5 seconds before next author...\n",
      "[68/107] Scraping tweets for @johnplattml...\n",
      "  Scraped 1 tweets for @johnplattml\n",
      "  Waiting 5 seconds before next author...\n",
      "[69/107] Scraping tweets for @SamanyouGarg...\n",
      "  Scraped 1 tweets for @SamanyouGarg\n",
      "  Waiting 5 seconds before next author...\n",
      "[70/107] Scraping tweets for @KirkDBorne...\n",
      "  Scraped 39 tweets for @KirkDBorne\n",
      "  Waiting 5 seconds before next author...\n",
      "[71/107] Scraping tweets for @Alber_RomGar...\n",
      "  Scraped 2 tweets for @Alber_RomGar\n",
      "  Waiting 5 seconds before next author...\n",
      "[72/107] Scraping tweets for @SilverJacket...\n",
      "  Scraped 3 tweets for @SilverJacket\n",
      "  Waiting 5 seconds before next author...\n",
      "[73/107] Scraping tweets for @ecsquendor...\n",
      "  Scraped 1 tweets for @ecsquendor\n",
      "  Waiting 5 seconds before next author...\n",
      "[74/107] Scraping tweets for @jordnb...\n",
      "  Scraped 1 tweets for @jordnb\n",
      "  Waiting 5 seconds before next author...\n",
      "[75/107] Scraping tweets for @jluan...\n",
      "  Scraped 1 tweets for @jluan\n",
      "  Waiting 5 seconds before next author...\n",
      "[76/107] Scraping tweets for @NPCollapse...\n",
      "  Scraped 1 tweets for @NPCollapse\n",
      "  Waiting 5 seconds before next author...\n",
      "[77/107] Scraping tweets for @NaveenGRao...\n",
      "  Scraped 1 tweets for @NaveenGRao\n",
      "  Waiting 5 seconds before next author...\n",
      "[78/107] Scraping tweets for @azeem...\n",
      "  Scraped 27 tweets for @azeem\n",
      "  Waiting 5 seconds before next author...\n",
      "[79/107] Scraping tweets for @Suhail...\n",
      "  Scraped 1 tweets for @Suhail\n",
      "  Waiting 5 seconds before next author...\n",
      "[80/107] Scraping tweets for @maxjaderberg...\n",
      "  Scraped 1 tweets for @maxjaderberg\n",
      "  Waiting 5 seconds before next author...\n",
      "[81/107] Scraping tweets for @Kyle_L_Wiggers...\n",
      "  Scraped 1 tweets for @Kyle_L_Wiggers\n",
      "  Waiting 5 seconds before next author...\n",
      "[82/107] Scraping tweets for @cocoweixu...\n",
      "  Scraped 1 tweets for @cocoweixu\n",
      "  Waiting 5 seconds before next author...\n",
      "[83/107] Scraping tweets for @aidangomezzz...\n",
      "  Scraped 1 tweets for @aidangomezzz\n",
      "  Waiting 5 seconds before next author...\n",
      "[84/107] Scraping tweets for @alexandr_wang...\n",
      "  Scraped 2 tweets for @alexandr_wang\n",
      "  Waiting 5 seconds before next author...\n",
      "[85/107] Scraping tweets for @CaimingXiong...\n",
      "  Scraped 1 tweets for @CaimingXiong\n",
      "  Waiting 5 seconds before next author...\n",
      "[86/107] Scraping tweets for @YiMaTweets...\n",
      "  Scraped 6 tweets for @YiMaTweets\n",
      "  Waiting 5 seconds before next author...\n",
      "[87/107] Scraping tweets for @notmisha...\n",
      "  Scraped 1 tweets for @notmisha\n",
      "  Waiting 5 seconds before next author...\n",
      "[88/107] Scraping tweets for @peteratmsr...\n",
      "  Scraped 1 tweets for @peteratmsr\n",
      "  Waiting 5 seconds before next author...\n",
      "[89/107] Scraping tweets for @shivon...\n",
      "  Scraped 3 tweets for @shivon\n",
      "  Waiting 5 seconds before next author...\n",
      "[90/107] Scraping tweets for @jackyliang42...\n",
      "  Scraped 1 tweets for @jackyliang42\n",
      "  Waiting 5 seconds before next author...\n",
      "[91/107] Scraping tweets for @v_vashishta...\n",
      "  Scraped 1 tweets for @v_vashishta\n",
      "  Waiting 5 seconds before next author...\n",
      "[92/107] Scraping tweets for @xdh...\n",
      "  Scraped 1 tweets for @xdh\n",
      "  Waiting 5 seconds before next author...\n",
      "[93/107] Scraping tweets for @FryRsquared...\n",
      "  Scraped 1 tweets for @FryRsquared\n",
      "  Waiting 5 seconds before next author...\n",
      "[94/107] Scraping tweets for @ravi_lsvp...\n",
      "  Scraped 1 tweets for @ravi_lsvp\n",
      "  Waiting 5 seconds before next author...\n",
      "[95/107] Scraping tweets for @ClementDelangue...\n",
      "  Scraped 15 tweets for @ClementDelangue\n",
      "  Waiting 5 seconds before next author...\n",
      "[96/107] Scraping tweets for @oh_that_hat...\n",
      "  Scraped 1 tweets for @oh_that_hat\n",
      "  Waiting 5 seconds before next author...\n",
      "[97/107] Scraping tweets for @sapna...\n",
      "  Scraped 1 tweets for @sapna\n",
      "  Waiting 5 seconds before next author...\n",
      "[98/107] Scraping tweets for @VRLalchand...\n",
      "  Scraped 2 tweets for @VRLalchand\n",
      "  Waiting 5 seconds before next author...\n",
      "[99/107] Scraping tweets for @svpino...\n",
      "  Scraped 48 tweets for @svpino\n",
      "  Waiting 5 seconds before next author...\n",
      "[100/107] Scraping tweets for @ceobillionaire...\n",
      "  Scraped 12 tweets for @ceobillionaire\n",
      "  Waiting 5 seconds before next author...\n",
      "[101/107] Scraping tweets for @ykilcher...\n",
      "  Scraped 1 tweets for @ykilcher\n",
      "  Waiting 5 seconds before next author...\n",
      "[102/107] Scraping tweets for @BornsteinMatt...\n",
      "  Scraped 1 tweets for @BornsteinMatt\n",
      "  Waiting 5 seconds before next author...\n",
      "[103/107] Scraping tweets for @lachygroom...\n",
      "  Scraped 1 tweets for @lachygroom\n",
      "  Waiting 5 seconds before next author...\n",
      "[104/107] Scraping tweets for @goodside...\n",
      "  Scraped 1 tweets for @goodside\n",
      "  Waiting 5 seconds before next author...\n",
      "[105/107] Scraping tweets for @amasad...\n",
      "  Scraped 19 tweets for @amasad\n",
      "  Waiting 5 seconds before next author...\n",
      "[106/107] Scraping tweets for @polynoamial...\n",
      "  Scraped 1 tweets for @polynoamial\n",
      "  Waiting 5 seconds before next author...\n",
      "[107/107] Scraping tweets for @sytelus...\n",
      "  Scraped 5 tweets for @sytelus\n",
      "\n",
      "Scraped a total of 857 tweets from 107 AI influencers\n",
      "Results saved to:\n",
      "- CSV: results/ai_influencer_tweets.csv\n",
      "- JSON: results/all_tweets.json\n",
      "- Individual JSON files for each author in the 'results' directory\n"
     ]
    }
   ],
   "source": [
    "from apify_client import ApifyClient\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "def scrape_ai_influencer_tweets():\n",
    "    # Initialize the ApifyClient with your API token\n",
    "    client = ApifyClient(\"apify_api_kbKxE4fYwbZOMBA30gS7DkbjinZqy91SEHb9\")\n",
    "    \n",
    "    # Twitter handles of AI researchers and professionals\n",
    "    twitter_handles = [\n",
    "        \"sama\",               # Sam Altman\n",
    "        \"ylecun\",             # Yann LeCun\n",
    "        \"AndrewYNg\",          # Andrew Ng\n",
    "        \"fchollet\",           # François Chollet\n",
    "        \"_KarenHao\",          # Karen Hao\n",
    "        \"karpathy\",           # Andrej Karpathy\n",
    "        \"SchmidhuberAI\",      # Jürgen Schmidhuber\n",
    "        \"sarahookr\",          # Sara Hooker\n",
    "        \"demishassabis\",      # Demis Hassabis\n",
    "        \"saranormous\",        # Sarah Guo\n",
    "        \"hardmaru\",           # David Hardmaru\n",
    "        \"lilianweng\",         # Lilian Weng\n",
    "        \"OriolVinyalsML\",     # Oriol Vinyals\n",
    "        \"Michael_J_Black\",    # Michael Black\n",
    "        \"JeffDean\",           # Jeff Dean\n",
    "        \"goodfellow_ian\",     # Ian Goodfellow\n",
    "        \"achowdhery\",         # Aakanksha Chowdhery\n",
    "        \"PeterDiamandis\",     # Peter H. Diamandis\n",
    "        \"GaryMarcus\",         # Gary Marcus\n",
    "        \"giffmana\",           # Lucas Beyer\n",
    "        \"rasbt\",              # Sebastian Raschka\n",
    "        \"quaesita\",           # Cassie Kozyrkov\n",
    "        \"KateKayeReports\",    # Kate Kaye\n",
    "        \"EMostaque\",          # Emad\n",
    "        \"drfeifei\",           # Fei-Fei Li\n",
    "        \"DrJimFan\",           # Jim Fan\n",
    "        \"omarsar0\",           # Elvis Saravia\n",
    "        \"conniechan\",         # Connie Chan\n",
    "        \"hugo_larochelle\",    # Hugo Larochelle\n",
    "        \"benjedwards\",        # Benj Edwards\n",
    "        \"rebecca_szkutak\",    # Becca Szkutak\n",
    "        \"svlevine\",           # Sergey Levine\n",
    "        \"ericschmidt\",        # Eric Schmidt\n",
    "        \"ilyasut\",            # Ilya Sutskever\n",
    "        \"patrickmineault\",    # Patrick Mineault\n",
    "        \"natashajaques\",      # Natasha Jaques\n",
    "        \"pabbeel\",            # Pieter Abbeel\n",
    "        \"ESYudkowsky\",        # Eliezer Yudkowsky\n",
    "        \"geoffreyhinton\",     # Geoffrey Hinton\n",
    "        \"wintonARK\",          # Brett Winton\n",
    "        \"jeffclune\",          # Jeff Clune\n",
    "        \"RamaswmySridhar\",    # Sridhar Ramaswamy\n",
    "        \"bentossell\",         # Ben Tossell\n",
    "        \"johnschulman2\",      # John Schulman\n",
    "        \"_akhaliq\",           # Ahsen Khaliq\n",
    "        \"quocleix\",           # Quoc Le\n",
    "        \"jackclarkSF\",        # Jack Clark\n",
    "        \"mervenoyann\",        # merve\n",
    "        \"DavidSHolz\",         # David\n",
    "        \"natolambert\",        # Nathan Lambert\n",
    "        \"RichardSocher\",      # Richard Socher\n",
    "        \"mustafasuleymn\",     # Mustafa Suleyman\n",
    "        \"ZoubinGhahrama1\",    # Zoubin Ghahramani\n",
    "        \"nathanbenaich\",      # Nathan Benaich\n",
    "        \"johnvmcdonnell\",     # John McDonnell\n",
    "        \"tunguz\",             # Bojan Tunguz\n",
    "        \"bengoertzel\",        # Ben Goertzel\n",
    "        \"ch402\",              # Chris Olah\n",
    "        \"Kseniase_\",          # Ksenia Se\n",
    "        \"paulg\",              # Paul Graham\n",
    "        \"rsalakhu\",           # Russ Salakhutdinov\n",
    "        \"gdb\",                # Greg Brockman\n",
    "        \"vivnat\",             # Vivek Natarajan\n",
    "        \"bxchen\",             # Brian X. Chen\n",
    "        \"AnimaAnandkumar\",    # Anima Anandkumar\n",
    "        \"JeffreyTowson\",      # Jeffrey Towson\n",
    "        \"Thom_Wolf\",          # Thomas Wolf\n",
    "        \"johnplattml\",        # John Platt\n",
    "        \"SamanyouGarg\",       # Samanyou Garg\n",
    "        \"KirkDBorne\",         # Kirk Bourne\n",
    "        \"Alber_RomGar\",       # Alberto Romero\n",
    "        \"SilverJacket\",       # Matthew Hutson\n",
    "        \"ecsquendor\",         # Tim Scarfe\n",
    "        \"jordnb\",             # Jordan Burgess\n",
    "        \"jluan\",              # David Luan\n",
    "        \"NPCollapse\",         # Connor Leahy\n",
    "        \"NaveenGRao\",         # Naveen Rao\n",
    "        \"azeem\",              # Azeem Azhar\n",
    "        \"Suhail\",             # Suhail Doshi\n",
    "        \"maxjaderberg\",       # Max Jaderberg\n",
    "        \"Kyle_L_Wiggers\",     # Kyle Wiggers\n",
    "        \"cocoweixu\",          # Wei Xu\n",
    "        \"aidangomezzz\",       # Aidan Gomez\n",
    "        \"alexandr_wang\",      # Alexandr Wang\n",
    "        \"CaimingXiong\",       # Caiming Xiong\n",
    "        \"YiMaTweets\",         # Yi Ma\n",
    "        \"notmisha\",           # Misha Denil\n",
    "        \"peteratmsr\",         # Peter Lee\n",
    "        \"shivon\",             # Shivon Zilis\n",
    "        \"jackyliang42\",       # Jacky Liang\n",
    "        \"v_vashishta\",        # Vin Vashishta\n",
    "        \"xdh\",                # Xuedong Huang\n",
    "        \"FryRsquared\",        # Hannah Fry\n",
    "        \"ravi_lsvp\",          # Ravi Mhatre\n",
    "        \"ClementDelangue\",    # clem\n",
    "        \"oh_that_hat\",        # Hattie Zhou\n",
    "        \"sapna\",              # Sapna Maheshwari\n",
    "        \"VRLalchand\",         # Vidhi Lalchand\n",
    "        \"svpino\",             # Santiago L Valdarrama\n",
    "        \"ceobillionaire\",     # Vincent Boucher\n",
    "        \"ykilcher\",           # Yannic Kilcher\n",
    "        \"BornsteinMatt\",      # Matt Bornstein\n",
    "        \"lachygroom\",         # Lachy Groom\n",
    "        \"goodside\",           # Riley Goodside\n",
    "        \"amasad\",             # Amjad Masad\n",
    "        \"polynoamial\",        # Noam Brown\n",
    "        \"sytelus\",            # Shital Shah\n",
    "    ]\n",
    "    \n",
    "    # Calculate date range for the last 3 days\n",
    "    end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    start_date = (datetime.now() - timedelta(days=3)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    print(f\"Scraping tweets from {start_date} to {end_date}\")\n",
    "    \n",
    "    # Create directory for results if it doesn't exist\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    \n",
    "    # Initialize results storage\n",
    "    all_tweets = []\n",
    "    \n",
    "    # CSV for structured data\n",
    "    with open(\"results/ai_influencer_tweets.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        # Write header\n",
    "        csv_writer.writerow([\"Author\", \"Twitter Handle\", \"Tweet Content\", \"Date\", \"Replies\", \"Likes\", \"Retweets\", \"Link\"])\n",
    "        \n",
    "        # Iterate through each Twitter handle\n",
    "        for i, handle in enumerate(twitter_handles):\n",
    "            print(f\"[{i+1}/{len(twitter_handles)}] Scraping tweets for @{handle}...\")\n",
    "            \n",
    "            # Prepare the Actor input\n",
    "            run_input = {\n",
    "                \"maxItems\": 100,  # Limit to 100 tweets per author\n",
    "                \"sort\": \"Latest\",\n",
    "                \"author\": handle,\n",
    "                \"start\": start_date,\n",
    "                \"end\": end_date,\n",
    "            }\n",
    "            \n",
    "            # Implement retry logic\n",
    "            max_retries = 3\n",
    "            retries = 0\n",
    "            success = False\n",
    "            \n",
    "            while retries < max_retries and not success:\n",
    "                try:\n",
    "                    # Run the Actor and wait for it to finish\n",
    "                    run = client.actor(\"61RPP7dywgiy0JPD0\").call(run_input=run_input)\n",
    "                    \n",
    "                    # Track tweets for this author\n",
    "                    author_tweets = []\n",
    "                    \n",
    "                    # Fetch tweets from the dataset\n",
    "                    for tweet in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "                        # Extract relevant data\n",
    "                        author_name = tweet.get(\"authorName\", \"\")\n",
    "                        content = tweet.get(\"content\", \"\")\n",
    "                        date = tweet.get(\"date\", \"\")\n",
    "                        replies = tweet.get(\"repliesCount\", 0)\n",
    "                        likes = tweet.get(\"likesCount\", 0)\n",
    "                        retweets = tweet.get(\"retweetsCount\", 0)\n",
    "                        url = tweet.get(\"url\", \"\")\n",
    "                        \n",
    "                        # Create tweet object\n",
    "                        tweet_data = {\n",
    "                            \"author\": author_name,\n",
    "                            \"handle\": handle,\n",
    "                            \"content\": content,\n",
    "                            \"date\": date,\n",
    "                            \"replies\": replies,\n",
    "                            \"likes\": likes,\n",
    "                            \"retweets\": retweets,\n",
    "                            \"url\": url\n",
    "                        }\n",
    "                        \n",
    "                        # Add to collections\n",
    "                        author_tweets.append(tweet_data)\n",
    "                        all_tweets.append(tweet_data)\n",
    "                        \n",
    "                        # Write to CSV\n",
    "                        csv_writer.writerow([author_name, handle, content, date, replies, likes, retweets, url])\n",
    "                    \n",
    "                    # Save individual author results to JSON\n",
    "                    if author_tweets:\n",
    "                        with open(f\"results/{handle}_tweets.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                            json.dump(author_tweets, f, indent=4)\n",
    "                    \n",
    "                    print(f\"  Scraped {len(author_tweets)} tweets for @{handle}\")\n",
    "                    success = True\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    retries += 1\n",
    "                    print(f\"  Error (attempt {retries}/{max_retries}): {e}\")\n",
    "                    if retries < max_retries:\n",
    "                        print(f\"  Retrying in 10 seconds...\")\n",
    "                        time.sleep(10)\n",
    "                    else:\n",
    "                        print(f\"  Failed to scrape tweets for @{handle} after {max_retries} attempts.\")\n",
    "            \n",
    "            # Save cumulative results after each author (in case of interruption)\n",
    "            with open(\"results/all_tweets.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(all_tweets, f, indent=4)\n",
    "            \n",
    "            # Add a delay between authors to respect rate limits\n",
    "            if i < len(twitter_handles) - 1:  # Don't delay after the last author\n",
    "                print(f\"  Waiting 5 seconds before next author...\")\n",
    "                time.sleep(5)\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\nScraped a total of {len(all_tweets)} tweets from {len(twitter_handles)} AI influencers\")\n",
    "    print(f\"Results saved to:\")\n",
    "    print(f\"- CSV: results/ai_influencer_tweets.csv\")\n",
    "    print(f\"- JSON: results/all_tweets.json\")\n",
    "    print(f\"- Individual JSON files for each author in the 'results' directory\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_ai_influencer_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping tweets from 2025-03-01 to 2025-03-03\n",
      "[1/2] Scraping tweets for @sama...\n",
      "  Scraped 2 tweets for @sama\n",
      "  Waiting 5 seconds before next author...\n",
      "[2/2] Scraping tweets for @ylecun...\n",
      "  Scraped 1 tweets for @ylecun\n",
      "\n",
      "Scraped a total of 3 tweets from 2 AI influencers\n",
      "Results saved to:\n",
      "- CSV: results/ai_influencer_tweets.csv\n",
      "- JSON: results/all_tweets.json\n",
      "- Individual JSON files for each author in the 'results' directory\n"
     ]
    }
   ],
   "source": [
    "from apify_client import ApifyClient\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "def scrape_ai_influencer_tweets():\n",
    "    # Initialize the ApifyClient with your API token\n",
    "    client = ApifyClient(\"apify_api_kbKxE4fYwbZOMBA30gS7DkbjinZqy91SEHb9\")\n",
    "    \n",
    "    # Twitter handles of AI researchers and professionals\n",
    "    twitter_handles = [\n",
    "        \"sama\",               # Sam Altman\n",
    "        \"ylecun\",             # Yann LeCun\n",
    "        \"AndrewYNg\",          # Andrew Ng\n",
    "        \"fchollet\",           # François Chollet\n",
    "        \"_KarenHao\",          # Karen Hao\n",
    "        \"karpathy\",           # Andrej Karpathy\n",
    "        \"SchmidhuberAI\",      # Jürgen Schmidhuber\n",
    "        \"sarahookr\",          # Sara Hooker\n",
    "        \"demishassabis\",      # Demis Hassabis\n",
    "        \"saranormous\",        # Sarah Guo\n",
    "        \"hardmaru\",           # David Hardmaru\n",
    "        \"lilianweng\",         # Lilian Weng\n",
    "        \"OriolVinyalsML\",     # Oriol Vinyals\n",
    "        \"Michael_J_Black\",    # Michael Black\n",
    "        \"JeffDean\",           # Jeff Dean\n",
    "        \"goodfellow_ian\",     # Ian Goodfellow\n",
    "        \"achowdhery\",         # Aakanksha Chowdhery\n",
    "        \"PeterDiamandis\",     # Peter H. Diamandis\n",
    "        \"GaryMarcus\",         # Gary Marcus\n",
    "        \"giffmana\",           # Lucas Beyer\n",
    "        \"rasbt\",              # Sebastian Raschka\n",
    "        \"quaesita\",           # Cassie Kozyrkov\n",
    "        \"KateKayeReports\",    # Kate Kaye\n",
    "        \"EMostaque\",          # Emad\n",
    "        \"drfeifei\",           # Fei-Fei Li\n",
    "        \"DrJimFan\",           # Jim Fan\n",
    "        \"omarsar0\",           # Elvis Saravia\n",
    "        \"conniechan\",         # Connie Chan\n",
    "        \"hugo_larochelle\",    # Hugo Larochelle\n",
    "        \"benjedwards\",        # Benj Edwards\n",
    "        \"rebecca_szkutak\",    # Becca Szkutak\n",
    "        \"svlevine\",           # Sergey Levine\n",
    "        \"ericschmidt\",        # Eric Schmidt\n",
    "        \"ilyasut\",            # Ilya Sutskever\n",
    "        \"patrickmineault\",    # Patrick Mineault\n",
    "        \"natashajaques\",      # Natasha Jaques\n",
    "        \"pabbeel\",            # Pieter Abbeel\n",
    "        \"ESYudkowsky\",        # Eliezer Yudkowsky\n",
    "        \"geoffreyhinton\",     # Geoffrey Hinton\n",
    "        \"wintonARK\",          # Brett Winton\n",
    "        \"jeffclune\",          # Jeff Clune\n",
    "        \"RamaswmySridhar\",    # Sridhar Ramaswamy\n",
    "        \"bentossell\",         # Ben Tossell\n",
    "        \"johnschulman2\",      # John Schulman\n",
    "        \"_akhaliq\",           # Ahsen Khaliq\n",
    "        \"quocleix\",           # Quoc Le\n",
    "        \"jackclarkSF\",        # Jack Clark\n",
    "        \"mervenoyann\",        # merve\n",
    "        \"DavidSHolz\",         # David\n",
    "        \"natolambert\",        # Nathan Lambert\n",
    "        \"RichardSocher\",      # Richard Socher\n",
    "        \"mustafasuleymn\",     # Mustafa Suleyman\n",
    "        \"ZoubinGhahrama1\",    # Zoubin Ghahramani\n",
    "        \"nathanbenaich\",      # Nathan Benaich\n",
    "        \"johnvmcdonnell\",     # John McDonnell\n",
    "        \"tunguz\",             # Bojan Tunguz\n",
    "        \"bengoertzel\",        # Ben Goertzel\n",
    "        \"ch402\",              # Chris Olah\n",
    "        \"Kseniase_\",          # Ksenia Se\n",
    "        \"paulg\",              # Paul Graham\n",
    "        \"rsalakhu\",           # Russ Salakhutdinov\n",
    "        \"gdb\",                # Greg Brockman\n",
    "        \"vivnat\",             # Vivek Natarajan\n",
    "        \"bxchen\",             # Brian X. Chen\n",
    "        \"AnimaAnandkumar\",    # Anima Anandkumar\n",
    "        \"JeffreyTowson\",      # Jeffrey Towson\n",
    "        \"Thom_Wolf\",          # Thomas Wolf\n",
    "        \"johnplattml\",        # John Platt\n",
    "        \"SamanyouGarg\",       # Samanyou Garg\n",
    "        \"KirkDBorne\",         # Kirk Bourne\n",
    "        \"Alber_RomGar\",       # Alberto Romero\n",
    "        \"SilverJacket\",       # Matthew Hutson\n",
    "        \"ecsquendor\",         # Tim Scarfe\n",
    "        \"jordnb\",             # Jordan Burgess\n",
    "        \"jluan\",              # David Luan\n",
    "        \"NPCollapse\",         # Connor Leahy\n",
    "        \"NaveenGRao\",         # Naveen Rao\n",
    "        \"azeem\",              # Azeem Azhar\n",
    "        \"Suhail\",             # Suhail Doshi\n",
    "        \"maxjaderberg\",       # Max Jaderberg\n",
    "        \"Kyle_L_Wiggers\",     # Kyle Wiggers\n",
    "        \"cocoweixu\",          # Wei Xu\n",
    "        \"aidangomezzz\",       # Aidan Gomez\n",
    "        \"alexandr_wang\",      # Alexandr Wang\n",
    "        \"CaimingXiong\",       # Caiming Xiong\n",
    "        \"YiMaTweets\",         # Yi Ma\n",
    "        \"notmisha\",           # Misha Denil\n",
    "        \"peteratmsr\",         # Peter Lee\n",
    "        \"shivon\",             # Shivon Zilis\n",
    "        \"jackyliang42\",       # Jacky Liang\n",
    "        \"v_vashishta\",        # Vin Vashishta\n",
    "        \"xdh\",                # Xuedong Huang\n",
    "        \"FryRsquared\",        # Hannah Fry\n",
    "        \"ravi_lsvp\",          # Ravi Mhatre\n",
    "        \"ClementDelangue\",    # clem\n",
    "        \"oh_that_hat\",        # Hattie Zhou\n",
    "        \"sapna\",              # Sapna Maheshwari\n",
    "        \"VRLalchand\",         # Vidhi Lalchand\n",
    "        \"svpino\",             # Santiago L Valdarrama\n",
    "        \"ceobillionaire\",     # Vincent Boucher\n",
    "        \"ykilcher\",           # Yannic Kilcher\n",
    "        \"BornsteinMatt\",      # Matt Bornstein\n",
    "        \"lachygroom\",         # Lachy Groom\n",
    "        \"goodside\",           # Riley Goodside\n",
    "        \"amasad\",             # Amjad Masad\n",
    "        \"polynoamial\",        # Noam Brown\n",
    "        \"sytelus\",            # Shital Shah\n",
    "    ]\n",
    "    \n",
    "    # Calculate date range for the last 2 days\n",
    "    end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    start_date = (datetime.now() - timedelta(days=2)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    print(f\"Scraping tweets from {start_date} to {end_date}\")\n",
    "    \n",
    "    # Create directory for results if it doesn't exist\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    \n",
    "    # Initialize results storage\n",
    "    all_tweets = []\n",
    "    \n",
    "    # CSV for structured data\n",
    "    with open(\"results/ai_influencer_tweets.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        # Write header\n",
    "        csv_writer.writerow([\"Author\", \"Twitter Handle\", \"text\", \"createdAt\", \"replyCount\", \"likeCount\", \"retweetCount\", \"url\"])\n",
    "        \n",
    "        # Iterate through each Twitter handle\n",
    "        for i, handle in enumerate(twitter_handles):\n",
    "            print(f\"[{i+1}/{len(twitter_handles)}] Scraping tweets for @{handle}...\")\n",
    "            \n",
    "            # Prepare the Actor input\n",
    "            run_input = {\n",
    "                \"maxItems\": 100,  # Limit to 100 tweets per author\n",
    "                \"sort\": \"Latest\",\n",
    "                \"author\": handle,\n",
    "                \"start\": start_date,\n",
    "                \"end\": end_date,\n",
    "            }\n",
    "            \n",
    "            # Implement retry logic\n",
    "            max_retries = 3\n",
    "            retries = 0\n",
    "            success = False\n",
    "            \n",
    "            while retries < max_retries and not success:\n",
    "                try:\n",
    "                    # Run the Actor and wait for it to finish\n",
    "                    run = client.actor(\"61RPP7dywgiy0JPD0\").call(run_input=run_input)\n",
    "                    \n",
    "                    # Track tweets for this author\n",
    "                    author_tweets = []\n",
    "                    \n",
    "                    # Fetch tweets from the dataset\n",
    "                    for tweet in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "                        # Extract relevant data\n",
    "                        author_name = tweet.get(\"authorName\", \"\")\n",
    "                        text = tweet.get(\"text\", \"\")\n",
    "                        date = tweet.get(\"createdAt\", \"\")\n",
    "                        replies = tweet.get(\"replyCount\", 0)\n",
    "                        likes = tweet.get(\"likeCount\", 0)\n",
    "                        retweets = tweet.get(\"retweetCount\", 0)\n",
    "                        url = tweet.get(\"url\", \"\")\n",
    "                        \n",
    "                        # Create tweet object\n",
    "                        tweet_data = {\n",
    "                            \"author\": author_name,\n",
    "                            \"handle\": handle,\n",
    "                            \"text\": text,\n",
    "                            \"date\": date,\n",
    "                            \"replies\": replies,\n",
    "                            \"likes\": likes,\n",
    "                            \"retweets\": retweets,\n",
    "                            \"url\": url\n",
    "                        }\n",
    "                        \n",
    "                        # Add to collections\n",
    "                        author_tweets.append(tweet_data)\n",
    "                        all_tweets.append(tweet_data)\n",
    "                        \n",
    "                        # Write to CSV\n",
    "                        csv_writer.writerow([author_name, handle, content, date, replies, likes, retweets, url])\n",
    "                    \n",
    "                    # Save individual author results to JSON\n",
    "                    if author_tweets:\n",
    "                        with open(f\"results/{handle}_tweets.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                            json.dump(author_tweets, f, indent=4)\n",
    "                    \n",
    "                    print(f\"  Scraped {len(author_tweets)} tweets for @{handle}\")\n",
    "                    success = True\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    retries += 1\n",
    "                    print(f\"  Error (attempt {retries}/{max_retries}): {e}\")\n",
    "                    if retries < max_retries:\n",
    "                        print(f\"  Retrying in 10 seconds...\")\n",
    "                        time.sleep(10)\n",
    "                    else:\n",
    "                        print(f\"  Failed to scrape tweets for @{handle} after {max_retries} attempts.\")\n",
    "            \n",
    "            # Save cumulative results after each author (in case of interruption)\n",
    "            with open(\"results/all_tweets.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(all_tweets, f, indent=4)\n",
    "            \n",
    "            # Add a delay between authors to respect rate limits\n",
    "            if i < len(twitter_handles) - 1:  # Don't delay after the last author\n",
    "                print(f\"  Waiting 5 seconds before next author...\")\n",
    "                time.sleep(3)\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\nScraped a total of {len(all_tweets)} tweets from {len(twitter_handles)} AI influencers\")\n",
    "    print(f\"Results saved to:\")\n",
    "    print(f\"- CSV: results/ai_influencer_tweets.csv\")\n",
    "    print(f\"- JSON: results/all_tweets.json\")\n",
    "    print(f\"- Individual JSON files for each author in the 'results' directory\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_ai_influencer_tweets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
